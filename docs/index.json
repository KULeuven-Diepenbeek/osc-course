[
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/intro/",
	"title": "1.1: The C programming language",
	"tags": [],
	"description": "",
	"content": " The C language is a programming language developed in the \u0026lsquo;70s to make it easier to interface with hardware. C was/is in essence nothing more than a few abstraction layers on top of assembly itself.\nA quick intro to C C is an imperative programming language. You will notice this when writing some code yourself: we write instructions as statements. These rules or statements are structured in function and struct types. There is little declarative to it, compared to other higher level languages. C\u0026rsquo;s footprint is quite small, the syntax is concise and easy to learn. Statements always express how to do things, instead of what it is doing. Increasing readability is of course important. We could for instance use #define to give meaning to a few symbols, or write clear function blocks.\nC is primarily being used in embedded system development because it is so closely related to the hardware itself. The UNIX, Windows and OSX kernels are fully written in C. The operating system of your cellphone, smartwatch or handheld all build on top of C. A huge amount of languages such as Java (JVM), Python, Ruby and PHP are first and foremost implemented in C.\nComparison with Java import java.io.IOException; import java.nio.*; class FileReader { @Override public String read(String file) throws IOException { return new String(Files.readAllBytes(Paths.get(file))); } } class Main { public static void main(String[] args) { System.out.println(\u0026#34;reading file: \u0026#34;); System.out.println(new FileReader().read(\u0026#34;sup.txt\u0026#34;)); } } How would one go around doing something like that in C? That will become difficult as C does not have a class system! A lower level implementation could look like this:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; char* read(char* file) { FILE *filePointer = fopen(file, \u0026#34;r\u0026#34;); char *buffer = malloc(255); fgets(buffer, 255, (FILE*) filePointer); fclose(filePointer); return buffer; } int main() { printf(\u0026#34;reading file: \\n\u0026#34;); printf(\u0026#34;%s\\n\u0026#34;, read(\u0026#34;sup.txt\u0026#34;)); return 0; } Compile the above with gcc -o fileio fileio.c.  Save some text in a file called \u0026ldquo;sup.txt\u0026rdquo;, and execute the program with ./fileio.  Congratulations on your first compiled C program!\n  There are a lot of problems with this implementation: the buffer length is hardcoded and the memory has not been released. A FileReader Java class that does everything for you simply cannot be created. As you can see it\u0026rsquo;s a lot more low-level work than Java\u0026rsquo;s one-liners like Files.readAllBytes! C does not even have the keyword new. Ouch.\nKey differences between C and a higher level language such as C#:\n C\u0026rsquo;s syntax footprint is small: no private/protected/class/interface/inheritance/bool/string\u0026hellip; No \u0026ldquo;standard\u0026rdquo; libraries. C does NOT have exceptions! It works with interrupts and error codes (return 0). C does NOT have garbage collection: you manage the memory yourself. C does NOT have a virtual machine (JVM, CLR) but gets compiled to native machine code. C code usually is full of pointer variables to manipulate memory directly. C allows for combination-integer-types (unsigned short int) C works with headers (.h) and source (.c) files. An executable file requires two steps: compiling and linking. Linking allows for mixing with assembly.  The following figure represents the Java Virtual Machine you have been using in the INF1 course (source):\nNote that there is only one source code and byte code block. Any .class file can be executed on any Linux/Windows/MacOS machine, provided you installed the correct JVM on top of the OS, as also pictured. When you compile .c C files, they do not translate into byte code but into OS-specific binaries! That means you cannot simply share your Windows binaries with a friend that runs Linux on his or her machine.\nWhy should you learn C? Good question. A few answers:\n Take a look at the TIOBE index and guess which programming language is the single most used throughout the world. Since this course is part of an engineering curriculum, it needs to stay close to its engineering roots: the hardware (embedded systems). Controlling hardware components on a PCB chip can only be done with low level machine instructions, that are coming from a low-level programming language such as C. Of course, writing in Assembly is even more precise, but hurts just a tad bit more. This is more of an operating systems course than it is a programming course. However, to learn concepts of an OS, you\u0026rsquo;ll need a firm grasp of the basics in C.\n  Basic C Hello World #include \u0026lt;stdio.h\u0026gt; int main() { int nr = 42; printf(\u0026#34;sup? %d\u0026#34;, nr); return 0; } The main() function returns a number that determines whether or not your program was executed successfully (0), else some kind of error code will be returned. printf is a function in the default IO header that we need to include, just like Java\u0026rsquo;s import.\nThe \u0026ldquo;f\u0026rdquo; of printf stands for \u0026ldquo;formatting\u0026rdquo; as you can see in the example. See Formatted output.\nWrite a program that outputs the following:  \u0026ldquo;pi is\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;3.1415\u0026rdquo; Based on the floating-point variable pi with a value of 3.1415. The output should end with a new line and contain a tab.\n  Use functions to structure code Done with function. Blocks such as if, for, while, do are familiar and work just like in other languages:\n#include \u0026lt;stdio.h\u0026gt; void say_something_if_positive(int number) { if(number \u0026gt; 5) { printf(\u0026#34;wow, positive or what? \\n\u0026#34;); for(int i = 1; i \u0026lt;= number; i++) { printf(\u0026#34;%d \u0026#34;, i); } printf(\u0026#34;\\n\u0026#34;); } } int main() { say_something_if_positive(5); return 0; } You cannot overload functions in C, unlike in C++ and Java. That means each function name is unique:\nint yay() { return 1; } int yay() { return 0; } int main() { return yay(); // ?? } Does, depending on the compiler, not compile:\n test.c:5:5: error: redefinition of 'yay' int yay() { ^ test.c:1:5: note: previous definition is here int yay() { ^ 1 error generated.  Primitives and combinational types The C language provides the four basic arithmetic type specifiers:\n char (1 byte) int (4 bytes) float (4 bytes) double (8 bytes)  Together with the modifiers:\n signed (minus and plus signs supported) unsigned (only plus signs, greater range) short (/2 bytes) long (x2 bytes)  The table on Wikipedia lists the permissible combinations to specify a large set of storage size-specific declarations. char is the smallest addressable unit, and long double is the biggest. For instance, unsigned char gives you a range of 0 to 255, while signed char works from -127 to 127.\nActual byte sizes are dependent on the target platform - the amount of bytes given above is usually correct for a 64-BIT machine. This can be retrieved using sizeof(type).   Try this out for yourself! See the combinational types demo C file in osc-exercises/ch1_c/combinational.c\n Strings? What do you mean? Forget it: char[] or a char* pointer is the only possibility. And no, it is not as easy as in Java to handle arrays due to the way they are defined.\n#include \u0026lt;stdio.h\u0026gt;#define SIZE 10  int main() { int arr[SIZE]; for(int i = 0; i \u0026lt; SIZE; i++) { arr[i] = i * 10; } for(int j = 0; j \u0026lt; SIZE; j++) { printf(\u0026#34;array index %d has value %d \\n\u0026#34;, j, arr[j]); } char string[] = \u0026#34;hi there\u0026#34;; printf(\u0026#34;%s\u0026#34;, string); return 0; } C reserves the right amount of memory with string literals you know from Java. The string[] char array does contain 9 characters and not 8! That is because the end of the array is determined by a magical NULL terminator, \\0. That makes it easier to loop through all characters and print them - or just let printf do that for you by formatting using %s.\nIn C, strings are just concatenations of characters, terminated with \\0. For instance, string[] msg = \u0026quot;hey\u0026quot;; looks like this in memory1:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR H[h] E[e] Y[y] T{/0} H -- E E -- Y Y -- T  An elaborated example that illustrates how character sequences work, and how they are terminated with \\0, can be found at http://www.cplusplus.com/doc/oldtutorial/ntcs/.\nHandy string utility functions reside in the header file \u0026lt;string.h\u0026gt; (copying, concatenating, asking for the length, \u0026hellip;) See GNU C: String utils.\nWhat is the result of strcmp(\u0026quot;hello\u0026quot;, \u0026quot;Hello\u0026quot;)?  And of strncmp(\u0026quot;hello, world\u0026quot;, \u0026quot;hello, amazing world!!\u0026quot;, 5)?\n  Hint: man strcmp!\nOnline C compilers:\n https://rextester.com/ https://godbolt.org/ https://repl.it/ (requires logging in)  The C Language - a quick reference Actually, the ANSI C syntax fits in just two index cards - that\u0026rsquo;s how small it really is. Download the quick reference cards here. Compared to Java, C# or C++, C is very much falls in the category \u0026ldquo;easy to learn, hard to master\u0026rdquo;. See the handbook comparison of C++ and C and you\u0026rsquo;ll know what we mean.\n The backslash (\\) is depicted as a forward slash (/) in the Figure. [return]   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/",
	"title": "1: Introduction in C",
	"tags": [],
	"description": "",
	"content": " Chapter 1 Introduction to C Chapter 1 handles the following subjects:\n Intro to C: primitives, differences between C/Java Structures, char* as strings, arrays Building C source files C Ecosystems Compiling \u0026amp; cross-compiling Building header/source files Makefiles  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-interrupts/theory1_arduino/",
	"title": "2.1 Arduino UNO",
	"tags": [],
	"description": "",
	"content": "  \u0026raquo;\u0026nbsp;[Naar de labo opgave](#oef) -- The Arduino UNO is an open-source microcontroller board. It is powered by an off-the-shelf ATMega microcontroller from Microchip  . The datasheet for this family of microcontrollers can be found here  .\n  An Arduino UNO. Source. arduino.cc   This datasheet contains a block diagram of the AVR architecture. Nonetheless, most of these components could be found in many other microcontrollers as well. The image should be familiar to students that took up the Elektronische systemen course at KU Leuven. üòÑ\n  Block diagram of the AVR Architecture. Source. podkalicki.com    The upper right component that is attached to the 8-bit data bus is the Interrupt Unit. This component is the topic of this chapter. Before diving into the details, another topic is explained first: Timer The ATMega microcontroller on the Arduino has, amonst other components, a dedicated Timer. This timer block consists out of 3 timers: two 8-bit timers and a single 16-bit timer. Through the use of the 16-bit timer, the concept of interrupts is illustrated.   Low Level programming Programming the Arduino UNO can be easily done with the Arduino IDE. This user friendly environment is a very nice entry point for new users of microcontrollers. Future engineers, however, should be able to understand what is going on behind the curtain. After all, magic doesn\u0026rsquo;t exist.\n  The Arduino IDE   Bit manipulations When bare metal programming a microcontroller it is often required to start poking specific bits in a register. A quick background refresh is given first.\n  Endianness and approaches in hardware and software    Binary Everybody agrees that the hexadecimal number 0x41 is binary written as 01000001. This is interpreted as 1 x 26 + 1 x 20 = 64 + 1 = 65. The least-significant bit is in the term: 1 x 20. Endianness When this byte is serialised (eg. in an array or in communcation) the question of \"Which bit do we write first ?\" arises. This is defined as the endianness. Big Endian writes the MSB on the right, while Little Endian writes the LSB on the right. Hardware vs. Software In contrast with the software world, hardware engineers typically start counting from the right (instead of the left). The C code for the assignement above would be: int myvalue[] = {1, 0, 0, 0, 0, 0, 1, 0}; where in VHDL this assignment would be: myvalue Now both 'worlds' see myvalue[0] as '1'.   Let\u0026rsquo;s explore the bit manipulations. First we assign the value 1 to an 8-bit type variable.\nunsigned char x; x = 1; With this line, x holds the number \u0026ldquo;00000001\u0026rdquo;. This means the bytes are represented in Little Endian.\nSetting the sixth bit, might be a bit cumbersome. First the result of 25 has to be calculated. After googling the result, the programmer can write:\nunsigned char x; x = 32; More seasoned C programmers might be inclined to use the shift operator  : \u0026lt;\u0026lt;. This litteraly takes te value 1 (remember: this is represented as 00000001) and shifts this value 5 positions to the left, while inserting 0\u0026rsquo;s on the right ( 00000001 + 00000 =\u0026gt; 00000 + 00100000 =\u0026gt; 00100000).\nunsigned char x; x = (1 \u0026lt;\u0026lt; 5); Next to the shift operator, bitwise operators  are also heavily used for setting and/or clearing certain bits. The logical functions AND (\u0026amp;), OR(|) and NOT(~) can be used as efficient tools for bit fiddling.\n‚ùó ‚ùó üêâ Beware of the dragons üêâ ‚ùó ‚ùó.\nunsigned char x=0, y=0; ... x = (1 \u0026lt;\u0026lt; 5); y = y | (1 \u0026lt;\u0026lt; 5); ... x = (1 \u0026lt;\u0026lt; 5); y = (1 \u0026lt;\u0026lt; 4); if(x \u0026amp;\u0026amp; y) { printf(\u0026#34;YES\\n\u0026#34;); } else { printf(\u0026#34;NO\\n\u0026#34;); } if(x \u0026amp; y) { printf(\u0026#34;YES\\n\u0026#34;); } else { printf(\u0026#34;NO\\n\u0026#34;); } if(x | y) { printf(\u0026#34;YES\\n\u0026#34;); } else { printf(\u0026#34;NO\\n\u0026#34;); }  What will be the printed output of the C-code above ?  Write a line of C-code that:  sets the 3rd bit to \u0026lsquo;1\u0026rsquo; and all others to \u0026lsquo;0\u0026rsquo; (resulting in \u0026ldquo;00000100\u0026rdquo;) sets the 4th and 5th bit to \u0026lsquo;1\u0026rsquo; and all others to \u0026lsquo;0(resulting in \u0026ldquo;00011000\u0026rdquo;)\u0026rsquo;     Hello hardware, this is software speaking A frequently used way of communication between hardware and software is through memory-mapped registers. Such a register can be read or written by software at a certain address. This register serves as driver (in the hardware sense) for inputs of a specific hardware component.\nWhen a register is written by software and read by (or driving) hardware, such a register is sometimes called a command register (CR). When a register is written (or driven) by hardware and read by the software, such a register is sometimes referred to with a status register (SR).\nGoing through the datasheet  of the microcontroller on the Arduino many CRs can be found. Section 36 summarises these registers on more than 3 pages. An example of a register that serves both as a CR and as a SR for controlling of the Analog-to-Digital Converter is shown below. As can be learned from the datasheet, the MSB of this register is used to enable or disable the hardware ADC through software instructions.\n  Example of a CR in the ATMega microcontroller. Source: arduino.cc   The register above can be accessed from software on address register 0x7A. When the software writes to this register ALL 8 bits are written. This is important to remember to avoid one of the dragons described above.\n #define HEARTBEAT_LED 7 int main(void) { int i, j; /* setup */ DDRD |= (1 The C-file above can be found in the Virtual Machine as example1.c\n A number of things should be pointed out. 1. Addresses and bit positions have to be known up front. these are defined in the **io.h** header file. If this file would not exist the addresses can be found in the datasheet. 2. The \"Setup and Loop\"-function approach can be mimicked, but this is not required. 3. The **delay(x)** function is missing, but this will be addressed later. -- "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-debugging/debugging-ide/",
	"title": "4.1: The Easy Way: IDEs",
	"tags": [],
	"description": "",
	"content": " 1. The easy way: Debugging using an IDE Arguably one of the better integrated C/C++ IDEs out there is CLion, a toolkit from Jetbrains based on the IDEA platform you all know from IntelliJ. It has exactly the same tools and capabilities but is fully geared towards C and C++. Cross-compiling and toolchain setup is also very easy using CLion. The Figure below is a screen capture of CLion showcasing it\u0026rsquo;s integrated unit testing capabilities which we will expand upon in the coming sections.\nA quick glance at the screenshot reveals the following buttons:\n Play: Compile and Run Debug Attach to process Run tests (step through, \u0026hellip;) File management window Gutter with line numbers and possibility to add breakpoints \u0026hellip;  A short live demo of CLion\u0026rsquo;s debugging capabilities is in order here.\n CLion is not free but a 30-day trail is, and as a student you can apply for a one-year license for free using your student e-mail address. Bigger development environments like this are typically used when developing large applications with a lot of source and header files. In this course, we will not be needing that. That is why the usage of a tool like this is not needed for now.\nInstead of relying on visual debug tools like CLion, another \u0026lsquo;hard-core\u0026rsquo; commandline alternative exists for Linux: gdb (The GNU debug tool).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-introos/intro-os/",
	"title": "5.1: Intro to OS using Linux",
	"tags": [],
	"description": "",
	"content": " In Chapter 2 the concept of interrupts was introduced. By using these interrupts it becomes easier to execute multiple independent tasks on a single processor. However, can you imagine developing a complete office-suite using interrupts?\nWhen more tasks come into play, interrupts will not be the answer to our question. But guess what the answer could be: Operating systems !!! (OSes)\nThe image below shows the classic picture when introducing OSes. The user never talks directly to hardware (or the OS), but always to the software.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A[User] B[Software] C[Operating System] D[Hardware] A -- B B -- A B -- C C -- B C -- D D -- C  Giving a single, clear definition of what an operating system is, is no simple job. The OS is basicaly a layer of software that is responsible for a number of things. Mainly, but certainly not exclusively, these tasks include:\n Allowing separate independent programs to use the hardware (ideally at the same time); Directly accessing the hardware; Hiding most of the complexity of the computer for the user; Guaranteeing that different tasks are isolated and protected (from each other); \u0026hellip;  One could say an OS is an abstraction layer that makes it easier to write software that interfaces with different types of hardware and that ensures a measure of robustness and security.\nDifferent OSes exist for different computing platforms.  Laptops, desktops, and serversOn laptops, desktops, and servers, the most well known operating systems are used. These include: Microsoft\u0026rsquo;s Windows, Linux, and MAC OS. It goes without saying that there are many more operating systems for these platforms, but some/many of them are fairly unknown to the wider public. These might include: DOS, BeOS, BSD, Unix, Solaris, SunOS, \u0026hellip;   Source: imimg.com       Source: fridaymagazine.ae    Embedded systemsEmbedded systems come in many flavours, colours and sizes. Typically, these devices are smaller and have fewer features than the laptops and co do. It goes without saying that the OSes that run on embedded systems are different to, or at least ported from, the other OSes. A number of OSes for embedded systems are: Android, FreeRTOS, Symbian, mbedOS, and brickOS. \nIn recent years, the distinction between these types of OSes has started to fade, with systems like Android and iOS becoming more full-featured, and even low cost hardware systems like Raspberry PIs being able to run a full Linux OS.\nWhich operating system is the best ?\n  Types of operating systems The image above showed that the OS places itself between general software and hardware. In the most inner core of an OS resides the kernel, the heart of the OS. Depending on the type of the kernel, a typical classification of OSes can be made:\n  source: Wikipedia. (IPC: Inter-Process communication. VFS: Virtual Filesystem Switch)    A Monotlithic kernel is a kernel that runs the complete OS in kernel space. Linux is a example of an OS that uses a Monolithic kernel.\n A Microkernel is a kernel that runs the bare minimum of an OS in kernel space. FreeRTOS is a example of an OS that uses a Microkernel.\n A Hybrid kernel is a type of kernel that is a combination of the two types above. No surprise there, right? Windows 10 is an example of an OS that uses a Hybrid kernel\n  In the definition the Microkernel it states that it runs the bare minimum of software. Generally this contains the following mechanisms:\n Task management: the ability to run multiple programs/tasks on the same hardware at (apparently) the same time. Inter-Process Communication (IPC): the ability for two different programs to communicate with each other. Address space management: manages how the available (RAM) memory is divided between programs.  These three mechanisms form the core of an OS and will be elaborated on in the remainder of this course.\nSystem calls The area marked in \u0026nbsp;yellow\u0026nbsp; in the figure above is called the user mode. The area marked in \u0026nbsp; red \u0026nbsp; in the figure above is called the kernel mode or the privileged mode. A program in user mode has no direct access to the hardware or to the entire memory space: it has to go through the kernel first.\nWhen the user, or the software on the user\u0026rsquo;s behalf, needs something from the more privileged world, the border between the user mode and kernel mode needs to be crossed. This is done using special functions that facilitate this, called System Calls. A list of all the system calls in a Linux operating system can be found here. These include methods to start a new program, send a message to a different program, read from a file, allocate memory, send a message over the network, etc.\nThese System Calls form the API (Application Programming Interface) between the higher-level software/applications in user mode (what you will typically write) and the lower-level features and hardware in kernel mode.\nLinux Since there are many OSes, we can of course not discuss them all. Most are however very similar in concept and in the basic systems they provide. As such, we will mainly focus on explaining these basic mechanisms in this course (see above).\nStill, to be able to get some hands-on experience with these systems, we need to choose one OS: the Linux OS. We do this because it is a very advanced and stable OS that is used extensively worldwide, and because it is open source (meaning that, unlike Windows and MacOS, we can view all the code in the kernel and even change it).\nThe Linux kernel almost never comes on its own but is packaged in a distribution (a.k.a. a \u0026ldquo;distro\u0026rdquo;). Such a distro is the combination of various pieces of software and the linux kernel. A distro can for example include a Graphical User Interface (GUI) layer, Web browsers, file management programs etc. As such, there is a large number of Linux distributions available, which mainly differ in the additional software they provide on top of the Linux kernel (which is pretty much the same across distros). Which one to pick was (and probably is) the start of multiple programmer wars, as everyone has their own preference. Our recommendation is to take into consideration what you want to use it for. For example when using Linux on a Web server, you probably don\u0026rsquo;t need a full GUI or a Web browser, which is different from when you want to use it as your main OS on your laptop.\n  source: Wikimedia   If the figure above doesn\u0026rsquo;t contain a distribution to your liking you can always Do It Yourself: Linux from scratch. Happy compiling !!\nOne of the main aspects in which these distro\u0026rsquo;s differ is the packaging system with which it\u0026rsquo;s distributed. These packaging systems allow you to install/uninstall/\u0026hellip; your software (a bit like the Windows Store or the mobile App Stores, but with more control over individual software libraries). The people that are making distro\u0026rsquo;s take source code from main software packets and compile them using the distro\u0026rsquo;s dependencies. This is then packaged and made available for package managers to install from. Typical examples of package mangers are:\n   Name Extension Typical distro\u0026rsquo;s     dpkg .deb Debian and Ubuntu a.o.   RPM .rpm Red Hat, Fedora, and CentOS a.o.   packman .pkg.tar.xz Arch Linux, FeLi Linux a.o.    If you search for Bodhi in the image above, you\u0026rsquo;ll learn that Bodhi is based on Ubuntu, which is based on Debian. Therefore APT (Advanced Package Tool) is used. This is why we used apt install SOFTWARE_NAME when setting up our Virtual Machine.\n  Use apt to see a list of the installed packages in the VM Update the list of packages in the VM Upgrade all packages to the most recently available version Install frozen-bubble in your VM  (tip: adding -h or \u0026ndash;help to a Linux command typically shows you the main options. Try apt -h.)\n  Got Root ? Most operating systems allow for multiple users to share one system and provide ways to clearly separate those users (and what they can access) from each other. Like in most OSes, Linux also has an administrative user, or super-user: the root user, who can access -everything- on the system. This highly privileged user is typically not used when doing day-to-day work, as it can be dangerous (for example, the root user can remove all files on the hard drive with a single command).\nA better way of approaching your day-to-day work on a Linux system is to use a standard user. Whenever you need a higher privilege-level, you can use sudo (Super User DO). This is a simple tool that allows a regular user to execute only certain commands as the root-user (only when needed). In your VM, your main user that you use to login with is normally also the root user, but you still need to use \u0026ldquo;sudo\u0026rdquo; to execute sensitive commands.\n  source: xkcd.com   Every user that has a login on a Linux system also automatically belongs to a group. Depending on the distribution, this group can have multiple names. On Bodhi for example, a new group is created for every user that bares the same name. Linux allows you to assign certain privileges and access rights to entire groups at a time, instead of only to individual users.\nFind the single command that can remove all files on the hard drive online (but please don\u0026rsquo;t try it!)\n  On files and such On Linux, almost everything is a file. A file is a file. A directory is a file. An entire harddrive is a file. A UART port is a file. Even a network connection is a file! Every file is owned by one user. All other users\u0026rsquo; privileges for that file (read, write, execute) are based on the access permissions.\n  Access permissions in an example folder   The image above shows the content of a folder example this folder contains: 2 files, 2 directories, a hidden directory (.secretfolder), and a hidden file (.ssshhhtt.txt). With hidden is meant that these files are not seen with a normal ls. To see these files the -a options has to be present. In Linux, all files starting with a \u0026ldquo;dot\u0026rdquo; character (.) are hidden and are often called \u0026ldquo;dotfiles\u0026rdquo;. They are often used to store configuration metadata; for example, git repositories have a .gitignore file with a list of files/directories that should not be included in the version control system.\nYou will also notice two special/strange entries at the top of the ls -al output, named \u0026ldquo;.\u0026rdquo; and \u0026ldquo;..\u0026rdquo;. These are not real files on the disk, but rather virtual files that help navigation in the filesystem and the execution of commands. Firstly, the \u0026ldquo;..\u0026rdquo; always means \u0026ldquo;the parent of this directory\u0026rdquo;. So if you are at the path \u0026ldquo;/home/user/test\u0026rdquo; and you do cd .., you will automatically go to \u0026ldquo;/home/user\u0026rdquo;. Doing cd ../.. will go to \u0026ldquo;/home\u0026rdquo;. Secondly, the single dot \u0026ldquo;.\u0026rdquo; means \u0026ldquo;the current directory\u0026rdquo;. This comes in handy if you want to search for something in files in the current directory or copy something to where you are at that moment without having to type the entire path.\nWith the -l parameter for ls the long listing format is shown (as is seen in the image above). With this, the access permissions are shown in the first 10 characters of each line:\n  source: drawings.jvns.ca   The first character indicates whether the entry is a directory (d) or a file (-).\nOut of fuel ? Take a Shell When a user logs in on a Linux computer, typically one of the following approaches is used:\n a login through a Graphical User Interface (GUI) a login through a Command Line Interface (CLI)  For a desktop/laptop that is running Linux, the GUI approach is typically used. An example is the Ubuntu/Bodhi Virtual Machine you use for the labs. On those systems there are terminal emulators which emulate the CLI. Many flavours of these terminals are available: gnome-terminal, xterm, konsole, \u0026hellip;\n  Example of a Terminal emulator. Source: linuxcommand.org   For embedded systems or Linux running on servers, the CLI is more appropriate. Running the Graphical User Interface requires CPU time and storage. Both are scarce on an embedded system. Since everything can be done through the command line, removing the GUI is a win-win. Additionally, it is useful to learn the CLI commands even if you normally use the GUI, because they play a large part in writing automation scripts for the OS: automation scripts typically do not indicate commands like \u0026ldquo;click button at location x,y\u0026rdquo; but rather execute the necessary CLI commands directly.\nWhen a CLI is used in Linux (or an emulator), a shell is started. The shell is a small program that translates commands, given by the user, and gives these translations to the OS. As with anything, there are many flavours of shells: sh, bash, ksh, zsh, \u0026hellip; Most shells provide the same basic commands, but others allow additional functionality and even full programming environments. You can write so-called \u0026ldquo;shell scripts\u0026rdquo; (typically have the file extension \u0026ldquo;.sh\u0026rdquo;), which are mostly lists of CLI commands, that are used extensively to automate operations on Linux systems.\nOnce the shell is running and the user asks to create new processes, all of these newly create processes will have the shell as a parent process (this will become important later on).\nAs a side note: Linux is not the only OS that uses a CLI. Windows for example has multiple, including the older \u0026ldquo;Command Prompt\u0026rdquo; and the more recent \u0026ldquo;Powershell\u0026rdquo;.\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/processes/",
	"title": "6.1: Processes",
	"tags": [],
	"description": "",
	"content": " One of the three core tasks of an OS is the management of tasks. These different tasks are all running on the same processor. To make this magic happen some form of management is required.\nWhat\u0026rsquo;s in a process ? By definition, a process is an instance of a program. As was dicussed in the \u0026ldquo;C-portion\u0026rdquo; of this course, a basic program can be divided in multiple segments. When source code is compiled into a binary, these segments are fixed.\n text/code: is the machine code in assembly. This section is compiled for one (type of) processor(s). data: is the segment that contains initialised variables. These variables could be global, static, or external. A further distinction can be made between normal initialised variables and constants. This distinction is often referred to with read-only (ro-data) and read-write (rw-data), with the former the part where the constants reside and the latter that of the normal variables. bss: (Block Starting Symbol) this segment contains the global and static variables that are not initialised. stack: dynamic part of memory used to store typically short-lived data. see Chapter 8 heap: dynamic part of memory used to store typically longer-lived data. see Chapter 8    source: Wikipedia   Let\u0026rsquo;s put these definitions to the test. A very simple program could be written as follows:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;hello world !\\n\u0026#34;); return 0; } As you know by now, this program can be compiled with the command below:\njvliegen@localhost:~/$ gcc -o hello hello.c jvliegen@localhost:~/$  On the CLI there is a command size that can be used to examine these segments.\njvliegen@localhost:~/$ size -B hello text data bss dec hex filename 1516 600 8 2124 84c hello jvliegen@localhost:~/$  This command shows the sizes of the sections that are discussed above. The text, data and bss sections are 1516, 600, and 8 bytes. In total this sums up to 2124 bytes, which can be written hexadecimally as 0x84C.\nOK, this makes sense :)\nRun process ! By the time you\u0026rsquo;re reading this part of the course, running the binary that was compiled above should be a walk in the park.\njvliegen@localhost:~/$ ./hello hello world ! jvliegen@localhost:~/$  When a program is started, an instance of the program is created, which we now call a process. Many of such processes can be active at the same time. Starting a process results (among many other things) in an allocation of the program in the memory. This means that the program is mapped somewhere in the memory space. How this mapping in the memory space is achieved will be discussed in chapter 8.\nSo now the program was loaded into the memory. But to actually run it, we need to keep additional state for each instance. This additional space is kept in the Process Control Block (PCB), which is created in the kernel upon mapping an instance of the program in memory space.\nProcess Control Block The Process Control Block (PCB) is a representation of the process in the OS. It is created as a task_struct object. Such a PCB is made for every process and the definition of its struct is in the kernel source.\nHOLD ON Linux is an open source OS, so does that mean we should be able to find this struct in the source code ?\nWELL YES Inspecting the sched.h file verifies:\n that we can read the source code that the task_struct is indeed there, that the PCB has a lot of parameters. The definition of the struct starts at line 649 and ends at line 1389 (at the moment of writing, March 15th 2021). As might be clear from this code (or even from thinking about the number of lines of codes), this struct is rather large. A (very) small subset of the fields in the struct represent:  the process state the process identifier (PID) the process priority CPU scheduling info list of open files memory limits the CPU registers the program counter (PC)   Especially these last two parts are crucial: they store the actual execution state of the program at a given time. The PC points to the current instruction the CPU will execute. As such, you can already see how it becomes easy to for example pauze a process for a while: simply stop updating the PC.\nThe PCBs of all the processes are contained in a doubly-linked list which is headed by the mother of all processes. This process is called init and has PID 0. The doubly-linked list is commonly known as the process table.\nLet\u0026rsquo;s verify all of the above. To do that, we need to keep a process running for long enough to determine its PID. For this, we\u0026rsquo;ll adapt the C program so it takes much more time to run.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #define DURATION_IN_MINUTES 2  int main(void) { int i; for(i=0;i\u0026lt;(DURATION_IN_MINUTES*60);i++) { printf(\u0026#34;hello world !\\n\u0026#34;); sleep(1); } return 0; } After starting the program, the PID of the ./longhello process can now be looked for using the ps command (\u0026ldquo;Process Status\u0026rdquo;).\njvliegen@localhost:~/$ ps -u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND jvliegen 14339 0.0 0.0 30408 5500 pts/3 Ss 09:57 0:00 bash jvliegen 20162 0.0 0.0 4504 804 pts/1 S+ 11:49 0:00 ./longhello jvliegen 20244 3.0 0.0 47232 3616 pts/3 R+ 11:51 0:00 ps -u jvliegen 27675 0.0 0.0 30544 5556 pts/0 Ss+ 08:55 0:00 bash jvliegen 27700 0.2 0.5 207400 96300 pts/0 Sl 08:55 0:30 /usr/local/bin/ jvliegen 29414 0.0 0.0 30412 5504 pts/1 Ss 09:13 0:00 bash ... In the example above the PID is 20162. Now we want to get some information on this process. Remember how we said that in Linux, almost everything is a file? Well, while this process is running, Linux magically creates a folder in the /proc map that has the same name as the PID. There is a lot of metadata on the process that can be consulted here. For example, certain fields in the struct_task are reflected in this folder.\njvliegen@localhost:~/$ ls /proc/20162 attr exe mounts projid_map status autogroup fd mountstats root syscall auxv fdinfo net sched task cgroup gid_map ns schedstat timers clear_refs io numa_maps sessionid timerslack_ns cmdline limits oom_adj setgroups uid_map comm loginuid oom_score smaps wchan coredump_filter map_files oom_score_adj smaps_rollup cpuset maps pagemap stack cwd mem patch_state stat environ mountinfo personality statm Let\u0026rsquo;s try to execute some commands here and see what they do: 1. ls -al /proc/20162/cwd 2. cat /proc/20162/sched 3. cat /proc/20162/status 4. cat /proc/20162/limits 4. ls -al /proc/20162/net\n(tip: the cat command (short for concatenate) can be used to read a file and display its contents.)\n  Process state One of the fields in the PCB is the process state. This value can be set to any of these values:\n New The process is being created Ready The process is waiting to be assigned to a processor Running Instructions are being executed Waiting The process is waiting for some event to occur (such as an I/O completion or reception of a signal) Terminated The process has finished execution    The different process states and their transitions    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nOpen files list Every process that is created has a list of open files. Even programs that do not actually work with files have this list, and it is never empty. Typically three files are open by default:\n standard input (stdin) standard output (stdout) standard error (stderr)  As you might have understood by now, these are not -actual- files on your hard disk. This is gain some Linux magic, where input/output logic is represented as files, to make them easy to read/write from.\nFor example, by default, stdin is mapped to the keyboard and both stdout and stderr are mapped to the command line. These mappings can be altered and redirected, however. For example, we might want to send errors (stderr) to a file instead. Redirecting can be done and will be briefly touched in section 6.5.\nCreating processes In Linux there are two typical ways for users to create processes: using fork or exec. There is a third option using a system() function, but because it is less secure and less efficient then the other two, it is not discussed here.\nBoth fork and exec create a new process, but in very different ways. The fork function copies the PCB and the entire memory space of its current process to a new PCB and memory location. The process that calls the fork() is referred to as the parent process, while the new process is its child. The child process will continue operations on the same line as the parent process (because of the copying of the PCB, the program counter is also copied!). This method is useful if you want to for example process a lot of data on multiple processors: you first get everything ready, then fork() new child processes that each deal with a part of the data. This is easier than manually starting new processes from scratch.\nThe exec function in contrast doesn\u0026rsquo;t really spawn a new process, but instead replaces itself entirely with a new program. While it initially starts by copying its own PCB, it then replaces its internal \u0026lsquo;program\u0026rsquo; (read the text, data, and bss) with the new program to be executed. It then clears all its memory and starts executing the new program from the first instruction. Note that the new program can be a copy of the current one (essentially \u0026ldquo;starting over\u0026rdquo;), but more typically it\u0026rsquo;s an entirely new program. The main way in which Linux starts new processes is to first fork() the current process, and then to call exec() directly inside of the new fork\u0026rsquo;ed clone, replacing it with the intended new program.\nIt is mentioned earlier that the first process that is started is the init process. When the complete OS starts, the init process spawns a lot of other processes. There are processes that handle connecting to the network, processes that do logging, and so on. Using the pstree command we can see the processes in a tree, and have a visual representation of which child-parent relations exist between processes.\n  An example of the pstree command. The left image shows the result of Linux on an embedded system. The right image shows the result of Linux running on a laptop   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/algorithms/",
	"title": "7.1: Scheduling algorithms",
	"tags": [],
	"description": "",
	"content": " In the previous chapter on Tasks, we\u0026rsquo;ve discussed one of the main responsibilities of an operating system: task management. Well to be fair, we have only been creating tasks and stopping or killing tasks. The necessary component that allows tasks to be run on one or multiple processors, the scheduler, is discussed in this chapter. Note that \u0026ldquo;tasks\u0026rdquo; or \u0026ldquo;jobs\u0026rdquo; can refer to either processes and/or threads.\nThe scheduler has two main responsibilities:\n Choose the next task that is allowed to run on a given processor Dispatching: switching tasks that are running on a given processor  Remember the image below? The first responsibility of the scheduler is the transition between the \u0026ldquo;ready\u0026rdquo; and \u0026ldquo;running\u0026rdquo; states by interrupting (pausing) and dispatching (starting/resuming) individual tasks:\n  The different process states and their transitions    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nWhile the image above is mainly for processes, similar logic of course exists for Threads as well, as they go through similar conceptual lifecycle phases as processes.\n For the sake of simplicity, in this chapter we assume a system which has only a single processor with a single CPU core. However, the concepts introduced here also (largely) hold for multi-core systems.\n(Don\u0026rsquo;t) Interrupt me !! When working with a single core, only a single task can be active at the same time. Say that the scheduler starts with the execution of the first task. It then has two options to determine when the next job is dispatched:\n non-preemptive / cooperative scheduling: Let the current task run until it is finished or until it (explicitly) yields (to give up/away, to release) its time on the processor. One way of yielding is for example by using the sleep(), but also pthread_join() or sem_wait() can indicate a task can be paused for the time being. preemptive scheduling: Interrupt the current task at any time. This is possible because the PCB and equivalent TCB\u0026rsquo;s track fine-grained per-task state.  While the cooperative scheduling approach is the simplest, it also has some severe downsides. If a given task takes a long time to complete or doesn\u0026rsquo;t properly yield at appropriate intervals, it can end up hogging the CPU for a long time, causing other tasks to stall. As such, most modern OSes will employ a form of preemptive scheduling.\nIf the scheduler needs to preempt jobs after a certain amount of time (or execution ticks), it requires hardware assistance. The timer, discussed in chapter 2, is a fundamental piece of hardware for preemptive scheduling.\n Scheduler algorithms Independent of whether cooperative or preemptive scheduling is used, there exist many algorithms the scheduler may use to determine which job is to be scheduled next. A (very select) number of algorithms are given here.\nTo be able to reason about different scheduling algorithms, there is a need of some sort of metric to determine which approach is best. When studying schedulers, the following metrics are typically used:\n Average Throughput: the number of tasks that are executed in one second. Average Job Wait Time (AJWT): the average time that a job needs to wait before it gets scheduled for the first time (first time - creation time). Average Job Completion Time (AJCT): the average time that a job needs to wait before it has fully finished (last time - creation time). CPU efficiency (\u0026#0951;CPU): the percentage of time that the processor performs useful work. Remember that every time the CPU switches between tasks, there is a certain amount of overhead due to context switching. As such, the more transitions between tasks there are, the less efficient the CPU is being used.  FCFS A simple algorithm that a scheduler can follow is: First Come, First served (FCFS). The order in which the jobs arrive (are started) is the same as the order on which the jobs are allowed on the processor.\nThe image below shows three tasks that arrive very close to each other. The result of the cooperative scheduler\u0026rsquo;s job is shown in the image:\n  FCFS with cooperative scheduling.   Applying the first three metrics on the example above gives the following results:\nAverage Throughput:\n 3 jobs over 12 seconds =\u0026gt; 0.25 jobs / s  AJWT:\n Task 1 arrives at 0 sec and can immediately start running =\u0026gt; wait time: 0s Task 2 arrives at 0.1 sec and can start after 1s =\u0026gt; wait time: 0.9s =\u0026gt; 1s (rounded) Task 3 arrives at 0.2 sec and can start after 11s =\u0026gt; wait time: 10.8s =\u0026gt; 11s (rounded) Average Job Wait Time = (0 s + 1 s + 11s)/3 = 12s / 3 = 4s  AJCT:\n Task 1 arrives at 0 sec, waits 0s and takes 1s =\u0026gt; duration: 1s Task 2 arrives at 0.1 sec, waits 0.9s and takes 10s =\u0026gt; duration: 11s (rounded) Task 3 arrives at 0.2 sec, waits 10.8s and takes 1s =\u0026gt; duration: 12s Average Job Completion Time = (1 s + 11 s + 12s)/3 = 24s / 3 = 8s  For these examples, the decimal portion can be rounded away. It is only used to make a distinction in the order of arrival.\n SJF By looking at the FCFS metrics, we can immediately see an easy way to improve the AJWT and AJCT metrics: schedule Task 3 before Task 2!\nOne algorithm that would allow such an optimization is called Shortest Job First (SJF). With this algorithm the scheduler looks at the tasks that are in the ready state. The shortest job within this queue is allowed first on the processor.\nIf the scheduler applies the SJF algorithm on the same example, the occupation of the processor looks like shown below.\n  SJF with cooperative scheduling.   Calculate the three metrics for the result of the SJF example, above: Throughput, AJWT, and AJCT.   Answer:  Throughput = 3 taken / 12 s = 0.25 jobs/s  AJWT = ( 0 s + 1 s + 2 s ) / 3 = 1s  AJCT = ( 1 s + 2 s + 12 s ) / 3 = 5s  We can see that the AJWT and AJCT metrics are indeed improved considerably for this example using SJF! \n  Preemptive scheduling Both of the examples for FCFS and SJF have so far been for non-preemptive/cooperative scheduling. Tasks have been allowed to run to their full completion. Let\u0026rsquo;s now compare this to preemptive scheduling, where the scheduler can pause a task running on the processor. Note that for the practical example we\u0026rsquo;ve been using, nothing much would change with preemptive scheduling: the selected next job would always be the same (either the first one started that hasn\u0026rsquo;t finished yet, or the shortest one remaining).\nAs such, let\u0026rsquo;s use a slightly more advanced example:\n  Preemptive scheduling.   For preemptive scheduling, there are again several options to determine when to preempt a running task, as here we\u0026rsquo;re no longer waiting for a task to end/yield. You could for example switch tasks each x milliseconds/x processor ticks. In our example, the scheduler preempts only when a new job comes in: it stops the currently running job and starts the most recently added job.\nIn the example above the following actions are taken:\n @ 0s, there is only one job: T2 =\u0026gt; schedule T2 @ 2s, there are two new jobs (T1 and T3) and one old (T2) =\u0026gt; schedule T1 (or T3) @ 3s, T1 (or T3) is done; there is one old job (T3 (or T1)) and one even older (T2) =\u0026gt; schedule T3 (or T1) @ 4s, T3 (or T1) has finished; there is one older (T2) =\u0026gt; schedule T2 @ 12s, T2 has finished; there are no more jobs  As such, this example demonstrates a sort-of Last-Come-First-Served (LCFS) approach.\nCalculate the three metrics for the result of the preemption example, above: Throughput, AJWT, and AJCT.   Answer:  Throughput = 3 taken / 12 s = 0.25 taken/s  AJWT = ( 0 s + 0 s + 1 s ) / 3 = 0.33 s  AJCT = ( 1 s + 12 s + 2 s ) / 3 = 5 s \n  Apply cooperative FCFS and SJF scheduling to the new example tasks and calculate the necessary metrics. Compare the results to the preemptive LCFS.\n  Priority-based scheduling At this point we could try a SJF approach with preemption (which here would be called shortest-remaining-time-first). Although this a perfectly fine exercise (wink), in practice estimating the duration of a job is not an easy task, as even the program itself typically doesn\u0026rsquo;t know how long it will run for! The OS could base itself on earlier runs of the program (or similar programs), or on the length of the program, but it remains guesswork. As such, SJF is rarely used in practice. In our example, it also wouldn\u0026rsquo;t be the perfect approach, since both T1 and T3 have equal (estimated) durations, and it wouldn\u0026rsquo;t help the OS to decide which should be run first. Put differently, the scheduler wouldn\u0026rsquo;t be deterministic.\nA more practical approach is priority-based scheduling. In this setup, you can assign a given priority to each task, and have jobs with higher priority run before those of lower priority. This still leaves some uncertainty/non-determinism for processes with the same priority, but it\u0026rsquo;s a good first approach.\nLet\u0026rsquo;s assume the priorities as mentioned in the image below. Try to complete the graph with the correct scheduler decision.   Preemptive scheduling. \n  As can be seen from the example above, this approach might hold a potential risk: starvation. Some jobs with lower priority (in our case T1) might not get any processor time until all other processes are done: they starve. One solution for starvation is priority ageing. This mechanism allows the priority of a job to increase over time in case of starvation, leading to the job eventually being scheduled. The actual priority thus becomes a function of the original priority and the age of the task. Again, as you can imagine, there are several ways to do this priority ageing (for example at which time intervals to update the priority and by how much, or by how/if you change the priority after the task has been scheduled for its first time slot). We will later see how this is practically approached in Linux.\nRound-Robin scheduling As you can see, scheduling algorithms can get quite complex and it\u0026rsquo;s not always clear which approach will give the best results for any given job load. As such, it might be easier to just do the simplest preemptive scheduling we can think of: switch between tasks at fixed time intervals in a fixed order (for example ordered by descending Task start time). This is called Round-Robin scheduling (RR).\nAs such, RR allows multiple tasks to effectively time-share the processor. The smallest amount of time that a job can stay on the processor is called a time quantum or time slice. Typically the duration of a time slice in a modern OS is between 10 and 100 ms. All jobs in the ready queue get assigned a time slice in a circular fashion. An (unrealistic) example with a time slice of 1s is shown below:\n  Round Robin scheduling.   Calculate the three metrics for the result of the preemption example, above: Throughput, AJWT, and AJCT.   Answer:  Throughput = 3 taken / 12 s = 0.25 taken/s  AJWT = ( 0 s + 1 s + 2 s ) / 3 = 1 s  AJCT = ( 10 s + 11 s + 12 s ) / 3 = 11 s \n  As can be seen from the example above, RR also has some downsides. While each task gets some CPU time very early on (low AJWT), the average completion time (AJCT) is of course very high, as all tasks are interrupted several times. If we were to compute CPU efficiency, this would also be lowest here, due to the high amount of context switches between tasks.\nA preemptive scheduler does not wait until a task yields the CPU, but interrupts its execution after a single time slice (or, in previous examples, when a new task arrives). This does NOT mean however that a task cannot yield the CPU !!!   Another way of putting it is: a job can either run until the time slice has run out (this is when the scheduler interrupts the job) or until the job itself yields the processor. In practice, both of course happen often during normal executing of tasks in an OS.\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/stackvsheap/",
	"title": "8.1: The Stack &amp; the Heap",
	"tags": [],
	"description": "",
	"content": " The Stack and the Heap Program Memory  Compiled computer programs are divided into different sections, each with their own specific needs and properties. Together, they form the program memory. The following image represents these sections, from bottom to top:   text Read-only, fixed size. Contains executable instructions.   data Can be modified. Contains global or static variables that are initialized, such as static int i = 5;. Global variables are variables that live outside of any function scope, and are accessible everywhere, such as int i = 5; int main() {¬†printf(\"%d\", i); }.   bss Can be modified. Contains uninitialized data, such as static int i;.   heap Dynamically growing. Contains data maintained by malloc() and free(), meaning most pointer variables. The heap is shared by all threads, shared librarys, and dynamically loaded modules in a process. Can be modified while the process is running.   stack Set size. Contains automatic variables: variables created when (automatically) entered a function, such as int main() { int i = 5; }. Can be modified manually using the command ulimit - but this cannot be modified once the process is running.      \u0026nbsp;  The Stack Besides (automatic) variables, a few more important things also live in the stack section of the program. These are the stack pointer (SP) and the \u0026lsquo;program stack\u0026rsquo; itself.\nContrary to initialized pointers, arrays within functions are also bound to the stack, such as char line[512];.\nThe Heap Contrary to arrays, initialized pointers are bound to the heap, such as char* line = malloc(sizeof(char) * 10) - except for pointer values that are being assigned directly with a string constant such as char* line = \u0026quot;hello\u0026quot;;. Freeing the last line would result in the error munmap_chunk(): invalid pointer.\nThe usage of malloc() and such is required if you want to reserve space on the heap. memcpy() from \u0026lt;string.h\u0026gt; makes it possible to copy values from the stack to the heap, without having to reassign every single property. Make sure to reserve space first!\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt; typedef struct Data { int one; int two; } Data; Data* from_stack() { Data data = { 1, 2 }; Data *heap_data = malloc(sizeof(Data)); memcpy(heap_data, \u0026amp;data, sizeof(Data)); return heap_data; } int main() { Data* heap = from_stack(); printf(\u0026#34;one: %d, two: %d\\n\u0026#34;, heap-\u0026gt;one, heap-\u0026gt;two); } This is called creating a deep copy, while a shallow copy creates a copy of a pointer, still pointing to the same value in memory space.\nWhat happens when you omit malloc() and simply write Data *heap_data = memcpy(heap_data, \u0026amp;data, sizeof(Data));?\n  As another side node, it is possible to resize variables on the heap, using realloc(). This is simply not possible on the stack: they cannot be resized. Also, using calloc instead of malloc initializes the allocated memory to zero instead of \u0026ldquo;nothing\u0026rdquo;. So now you know how to use malloc, calloc, realloc, and free.\nInspecting program memory in the OS Unix-like operating systems implement procfs, a special filesystem mapped to /proc, that makes it easy for us to inspect program running program state. You will need the process ID (PID) as it is the subdir name. Interesting files to inspect are:\n /proc/PID/maps, a text file containing information about mapped files and blocks (like heap and stack). /proc/PID/mem, a binary image representing the process\u0026rsquo;s virtual memory, can only be accessed by a ptrace\u0026rsquo;ing process.  We will take a closer look at these during the labs.\nMac OSX Does not have procfs. Instead, you will have to rely on commandline tools such as sysctl to query process information.\nShould I use the stack or the heap? Good question. The answer is obviously both. Use the stack when:\n You do not want to de-allocate variables yourself. You need speed (space is managed efficiently by the CPU). Variable size is static.  Use the heap when:\n You require a large amount of space (virtually no memory limit). You don\u0026rsquo;t mind a bit slower access (fragmentation problems can occur). You want to pass on (global) objects between functions. You like managing things yourself. Variable size could be dynamic.  What piece of code could be dynamic in size? Data structures, such as linked lists from chapter 3: pointers and arrays, are a good candidate for this: arrays, sets, maps, and any other form of collection can grow and shrink in size, therefore need dynamic memory mapped. Using [] will, in most cases, not suffice in the C programming language, unless you are doing something very simple.\nMemory management Freeing up space In order to create an instance of a structure and return it, you have to allocate memory using malloc() from \u0026lt;stdlib.h\u0026gt; - we now that already. In contrast with higher level languages such as Java, C requires programmes to clean up the allocaed memory themselves! This means calling free() to free up the space for future use. For instance:\nstruct Stuff { int number; }; typedef struct Stuff Stuff; void do_nasty_things() { // ...  Stuff* ugly = malloc(sizeof(Stuff)); ugly-\u0026gt;number = 10; // ... } int main() { do_nasty_things(); // other things } As soon as the method do_nasty_things() ends, ugly is not accessible anymore because it was not returned and there are no other references to it. However, after the function, memory is still reserved for it. To counter memory leaks such as these, you can do a few things:\n Keep things local, by keeping things on the stack. The Stack, for that function, will be cleared after calling it. Change Stuff* to Stuff. Free the pointer space at the end of the function by calling free(ugly).  Since this is a small program that ends after main() statements are executed, it does not matter much. However, programs with a main loop that require a lot of work can also contain memory leaks. If that is the case, leak upon leak will cause the program to take op too much memory and simply freeze.\nDo not make the mistake to free up stack memory, such as in this nice example, from the \u0026lsquo;Top 20 C pointer mistakes\u0026lsquo;:\nint main() { int* p1; int m = 100; // stack var  p1 = \u0026amp;m; // pointer to stack var  free(p1); // BOOM headshot!  return 0; }  a.out(83471,0x7fff7e136000) malloc: *** error for object 0x7fff5a24046c: pointer being freed was not allocated *** set a breakpoint in malloc_error_break to debug Abort trap: 6  Dangling pointers A second mistake could be that things are indeed being freed, but pointers still refer to the freed up space, which is now being rendered invalid. This is called a dangling pointer, and can happen both on the heap (while dereferencing an invalid pointer after freeing up space):\nint *p, *q, *r; p = malloc(8); // ... q = p; // ... free(p); r = malloc(8); // ... something = *q; // aha!  , and on the stack (while dereferencing an invalid pointer after returning an address to a local variable that gets cleaned up because it resides on the stack):\nint *q; void foo() { int a; q = \u0026amp;a; } int main() { foo(); something = *q; // aha! } Garbage Collection - not happening in C\u0026hellip; The above mistakes are easily made if you are used to Java:\nvoid foo() { Animal cow = new Animal(); cow.eat(); // ... } public static void main(String[] args) { foo(); // cow instances are cleaned up for you... } This cleaning process, that automatically frees up space in multiple parts of the allocated memory space, is called garbage collecting.  And it is completely absent in C, so beware!\nWhat happens when the stack and heap collide? That is platform-dependent and will hopefully crash instead of cause all forms of pain. There are a few possibilities:\n Stack \u0026ndash;\u0026gt; heap. The C compiler will silently overwrite the heap datastructure! On modern OS systems, there are guard pages that prevent the stack from growing, resulting in a segmentation fault. Also, modern compilers throw exceptions such as stack overflow if you attempt to go outside the reserved space (= segfault). Heap \u0026ndash;\u0026gt; Stack. The malloc() implementation will notice this and return NULL. It is up to you to do something with that result.  Write a program with an infinite loop that puts stuff on the stack. What is the program output? Do the same with infinite malloc()\u0026rsquo;s. What happens now?\n  What\u0026rsquo;s a stack overflow? The stack is a limited, but fast piece of program memory, made available for your program by the OS. The keyword here is limited. Unlike the heap, it will not dynamically grow, and it is typically hard-wired in the OS. Simply keeping on adding stuff to the stack, such as calling methods within methods without a stop condition (infinite recursion), will cause a stack overflow exception, signaling that the OS prevented your program from taking over everything:\n// forward definition void flow(); void flow() { // on the stack  int x = 5; // on the stack  flow(); // keep on going } int main() { flow(); } This causes a segmentation fault on my OSX machine, signaling that it was killed by the OS. Add printf() statements to your liking.\nHow do I know how big the stack can be on my OS? Use ulimit -a to find out:\n outers-MacBook-Air:ch8-stack wgroeneveld$ ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited file size (blocks, -f) unlimited max locked memory (kbytes, -l) unlimited max memory size (kbytes, -m) unlimited open files (-n) 4864 pipe size (512 bytes, -p) 1 stack size (kbytes, -s) 8192 **BINGO** cpu time (seconds, -t) unlimited max user processes (-u) 709 virtual memory (kbytes, -v) unlimited  So it\u0026rsquo;s 8.19 MB.\nOptimizing C code Compiler flags Depending on your compiler and your target platform, the C compiler will try to optimize code by rearranging declarations and possibly even removing lines such as completely unused variables. The GNU and LLVM gcc compilers offer multiple levels of optimization that can be enabled by passing along -O1, -O2, and -O3 flags (O = Optimize). Consider the following code:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;void stuff() { char dingdong[] = \u0026#34;hello? who\u0026#39;s there?\u0026#34;; printf(\u0026#34;doing things\\n\u0026#34;); } int main() { stuff(); } stuff is not doging anything with the char array. Compile with gcc -g -O3 test.c to enable debug output and optimize. When disassembling using lldb (LLVM) or gdb (GNU), we see something like this:\n (lldb) disassemble --name stuff a.out`stuff at test.c:3: a.out[0x100000f30]: pushq %rbp a.out[0x100000f31]: movq %rsp, %rbp a.out[0x100000f34]: leaq 0x4b(%rip), %rdi ; \"doing things\" a.out[0x100000f3b]: popq %rbp a.out[0x100000f3c]: jmp 0x100000f64 ; symbol stub for: puts a.out`main + 4 [inlined] stuff at test.c:12 a.out`main + 4 at test.c:12: a.out[0x100000f54]: leaq 0x2b(%rip), %rdi ; \"doing things\" a.out[0x100000f5b]: callq 0x100000f64 ; symbol stub for: puts a.out[0x100000f60]: xorl %eax, %eax  Where is dingdong? The compiler saw it was not used and removed it. Without the -O3 flag:\n (lldb) disassemble --name stuff a.out`stuff at test.c:3: a.out[0x100000e90]: pushq %rbp a.out[0x100000e91]: movq %rsp, %rbp a.out[0x100000e94]: subq $0x30, %rsp a.out[0x100000e98]: movq 0x171(%rip), %rax ; (void *)0x0000000000000000 a.out[0x100000e9f]: movq (%rax), %rax a.out[0x100000ea2]: movq %rax, -0x8(%rbp) a.out[0x100000ea6]: movq 0xc3(%rip), %rax ; \"hello? who's there?\" a.out[0x100000ead]: movq %rax, -0x20(%rbp) a.out[0x100000eb1]: movq 0xc0(%rip), %rax ; \"ho's there?\" a.out[0x100000eb8]: movq %rax, -0x18(%rbp) a.out[0x100000ebc]: movl 0xbe(%rip), %ecx ; \"re?\" a.out[0x100000ec2]: movl %ecx, -0x10(%rbp) a.out[0x100000ec5]: movl $0x0, -0x24(%rbp) a.out[0x100000ecc]: cmpl $0xa, -0x24(%rbp) a.out[0x100000ed3]: jge 0x100000ef2 ; stuff + 98 at test.c:5 a.out[0x100000ed9]: movslq -0x24(%rbp), %rax a.out[0x100000edd]: movb $0x63, -0x20(%rbp,%rax) a.out[0x100000ee2]: movl -0x24(%rbp), %eax a.out[0x100000ee5]: addl $0x1, %eax a.out[0x100000eea]: movl %eax, -0x24(%rbp) a.out[0x100000eed]: jmp 0x100000ecc ; stuff + 60 at test.c:5 a.out[0x100000ef2]: leaq 0x8b(%rip), %rdi ; \"doing things\\n\" a.out[0x100000ef9]: movb $0x0, %al a.out[0x100000efb]: callq 0x100000f46 ; symbol stub for: printf a.out[0x100000f00]: movq 0x109(%rip), %rdi ; (void *)0x0000000000000000 a.out[0x100000f07]: movq (%rdi), %rdi a.out[0x100000f0a]: cmpq -0x8(%rbp), %rdi a.out[0x100000f0e]: movl %eax, -0x28(%rbp) a.out[0x100000f11]: jne 0x100000f1d ; stuff + 141 at test.c:9 a.out[0x100000f17]: addq $0x30, %rsp a.out[0x100000f1b]: popq %rbp a.out[0x100000f1c]: retq a.out[0x100000f1d]: callq 0x100000f40 ; symbol stub for: __stack_chk_fail  You can fiddle with options and such yourself in godbolt.org.\nInstead of bootstrapping the debugger to inspect disassembly, you can also simply dump the object contents using objdump -D (GNU) or otool -tV (OSX).\n volatile When heavily optimizing, sometimes you do not want the compiler to leave things out. This is especially important on embedded devices with raw pointer access to certain memory mapped spaces. In that case, use the volatile keyword on a variable to tell the compiler to \u0026ldquo;leave this variable alone\u0026rdquo; - do not move it\u0026rsquo;s declaration and do not leave it out. For instance:\nint array[1024]; int main (void) { int x; for (int i = 0; i \u0026lt; 1024; i++) { x = array[i]; } } Does pretty much nothing. Compiling with -O3 results in 2 assembly instructions:\n main: xor eax, eax ret  However, if you want x to be left alone, use volatile int x; and recompile:\n main: mov eax, OFFSET FLAT:array .L2: mov edx, DWORD PTR [rax] add rax, 4 mov DWORD PTR [rsp-4], edx cmp rax, OFFSET FLAT:array+4096 jne .L2 xor eax, eax ret  That\u0026rsquo;s a big difference.\nFunction call order Another part of optimizing code is the determination of function call order. For instance, consider the following statement: x = f() + g() * h(). Which function gets called first?\nThe answer is we do not know. Do not rely on function order to calculate something! Each function should be completely independant. There should not be a global variable manipulated in f() which will then be needed in g() or h(). You can inspect disassembled code for different compilers on https://godbolt.org/. It will differ from platform to platform, and from compiler to compiler (and even from option flag to flag).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/memory/",
	"title": "9.1: Memory management",
	"tags": [],
	"description": "",
	"content": " At multiple times we referred to the registers in processor. These 16 (or at least in that order of magnitude) registers don\u0026rsquo;t suffice to run the complex tasks that are running on our devices. More space is needed !!!\nWhat good is additional space is you can\u0026rsquo;t access it ? As we do in real life we give every house an address. This provides an unambiguous way of addressing üòâ every single house.\nIn modern computers and embedded systems there is a large amount of memory available. All memory locations are uniquely addressed.\nHow many addresses are there on a modern computer/laptop ? How does this relate to a \u0026ldquo;64-bit\u0026rdquo; processor ?   Answer:  The processor of a modern computer is often referred to as a 64-bit processor. While this number reflects the internal bus size, typically it also implies that the address bus is 64 bits wide. The number of available addresses hence is 264. If we assume a binary order of magnitude, that comes down to 224+10+10+10+10 addresses, or 224 terabyte or 16 exabyte.   For the sake of correctness, this should be 16 exbibyte. Similarly, 1024 bits is officially a kibibyte, where we typically call this a kilobyte. More information on this can be found on Wikipedia. \n  The flat earth of Arduino On small systems-on-chip (SOCs) that do not run an operating system, the world of memory addresses looks flat.\n  Flat earth view on memory.      Image source: Getty     If an 8-bit processor, like the ATMega 328p on the Arduino Uno, is assumed the complete memory space covers 28 = 256 addresses. On every address an 8-bit value is stored.\nLet\u0026rsquo;s share The purpose of an OS is to run multiple, different tasks on a one piece of hardware. With the flat earth memory approach this becomes cumbersome. A simple solution would be to share the complete memory in multiple chunks. One chunk for each task.   It is the responsibility of the OS to make sure that each process stays within it own share. Often this is referred to with memory protection.   Meltdown and Spectre - Two attacks that got access to other process\u0026#39;s memory.    Some computer hackers try the most insane techniques to get access to another process's memory. Exactly this has recently been shown in the Meltdown and Spectre attacks.    The complete memory space, divided to one chunk per process.     To keep an eye on that everybody stays within its context the following approach is used. Each task gets a base address. This address gives the start of that process\u0026rsquo;s context. The second parameter is the limit which gives the size of the context. When a process is scheduled the OS loads the base register and the limit register with the correct values. The check that the OS has to monitor is that the address of the accessed memory falls within the interval of the base address PLUS the limit. If this condition does not hold, there might (un-)intentional error. The OS will handle this error.\n  Base address register and limit register in logical addressing.   The two registers that store the base and limit addresses can only be written by the OS. Moreover, these registers can only be written by processes from the kernel-space because the operation is done through a special privileged instruction. The context that is assigned to a specific task is called a logical address space.\nAddress Binding When software is written, it is done from a certain point of abstraction with respect to the hardware. The C language has a relatively low level of abstraction (close to the hardware) whereas Java has a higher level of abstraction (further from the hardware). Luckily, in C we don't have to bother ourselves with physical addresses.\nVariables are declared in C with statements like: int my_score; The compiler and the linker take care of the rest. Due to the underlying OS the required number of bytes can be reserved.\n#include \u0026lt;stdio.h\u0026gt; int main(void) { short i; short my_array[4]; long a_long_value; ... }  Apart from registers that need to be mapped, there are also functions that require mapping. There typically are three moments in time when this mapping can be done:  at compile time at load time at execution time       When the mapping is done during compile time, it should be possible to know in which register a certain value is stored. Also the puzzle with functions has to be made. Therefore, all functions should be known which typically is not the case.\nWhen the mapping is done at load time, relocatable code is generated by the compiler. The addressing is done relatively. In the example above it could be stated that the first element of my_array is at the memory address of i, incremented with the size of a short (2 bytes). When such a program is run, the binding to memory address is done when the program loads. When the start address becomes known, all other address can be resolved.\nFinally, when (part of) the mapping is done while executing the program, the binding is done when it is required. This requires an additional hardware component: memory-management unit. Nevertheless, this option of binding at execution time is used by most general-purpose operating systems.\nThe memory-management unit The address that is used by the CPU is the logical address or virtual address, the address that is actually used in hardware is the physical address. The mapping between logical and physical addresses is typically done with the memory-management unit (MMU).\nThe first approach the MMU can use is the one similar to the base and limit registers, as seen above.\n  The MMU    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nThe CPU \u0026lsquo;sends\u0026rsquo; is logical address to the MMU. The MMU then adds a value to \u0026ldquo;relocate\u0026rdquo; the address. The address that comes out of the MMU is a physical address that can be used by the hardware to find the memory location. Because things not need to get more complicate, the register in the MMU is called the relocation register.\nThere are more approaches the MMU can use. They will be discussed later.\nSegmentation Most programmers, ourselves included, think of memory like this long-stretched, linear addressable chunk of highway that is solely here for us (if only it were so) üòÑ. We have seen that all the memory is split into multiple segments that might vary in size. Don\u0026rsquo;t make the mistake of thinking that your program gets a single nicely demarcated field of memory. Our (simple) program also gets segmented.\n   Continuous image. Source: G.I.       Segmented image  \n \nThe different segments are made, based on the sections that were discussed earlier: text, data, bss, stack, \u0026hellip; So instead of seeing a program as a single piece of the memory-puzzle, it actually breaks down into multiple pieces. All of those puzzle pieces should be mapped , but the will be spread out all over the memory space.\nAs mentioned before, the puzzling is done by the development tools and the OS. For us, the programmer, it still feels like our very own private highway. With the segments that we are introducing, the job of the puzzler becomes even harder. Luckily there is dedicated hardware to assist. All the segments are collected in the segment table. Similarly as above, each segment has as base and a limit.\n  Segmentation hardware    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nThe address that the programmer sees secretly consists of two parts: a segment number (s) and an offset (d). The segment number (s) will be used as an address in the segment table. Through this table the base address of the segment can be found. The offset number (d) will be used to define an offset within this segment and is simply added to the base address.\nIf the sum of the base address and the offset falls within the segment limit, everything is fine. Otherwise the operating system will catch this error.\nWhat, oh what, could be the name of the error that the OS produces if the sum of the base address and the offset does not fall within the segment limit ?\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/filesystems/",
	"title": "X.1: File systems",
	"tags": [],
	"description": "",
	"content": " Introduction in Unix filesystems Files in Unix Working daily with files in Unix operating systems will no doubt increase your awareness of the specific ins and outs of the Unix filesystem. As you know from previous chapters, everything is a file in Unix: we have used this to our advantage to inspect the inner workings of a process in the /proc directory.\nA file is the smallest possible unit of storage in a unix system. However, a filesystem is more than mere data (the file itself): it also contains relations to other files, and attributes or metadata of the file itself, such as:\n name size timestamp owner, protection parameters  A directory is a special kind of file, in which other files can be stored. It simply references to other files it contains and does not duplicate the data itself.\n  src: http://www.rhyshaden.com/unix.htm   The image above gives an overview of common Unix directory structures, starting from the root / (forward slash). Depending on the Unix flavor, there will be slight variations. For example, Linux usually has /home as user root, while OSX has /Users (capital).\nThere are also special device files, mapped at /dev, that form the bridge between the virtual OS system and the physical machine system. These device files are char or block systems that enable the OS to read data from an external device. For instance, your USB devices, listed through lsusb, can be read directly using cat /dev/ttyXX.\nHard drives are mapped using the same technique, in /dev/sdx, listed through cat /etc/fstab (mount). lsblk (list block devices) allows you to see which block devices are currently linked through files to the OS. On OSX, my mount command outputs:\n Wouters-MacBook-Air:~ wgroeneveld$ mount /dev/disk1 on / (hfs, local, journaled) devfs on /dev (devfs, local, nobrowse) map -hosts on /net (autofs, nosuid, automounted, nobrowse) map auto_home on /home (autofs, automounted, nobrowse)  Depending on the Unix flavor, the location of the device files will be different.\nFile attributes When you list all files using ls -la, you see something like this:\n  src: https://homepages.uc.edu/~thomam/   These are attributes of each file. As you can see in the above image, the permission mode also indicates whether it\u0026rsquo;s a directory file or not, denoted by d at the beginning. The rest of the permissiom modes indicate whether or not the file is writable.\nThere are five different permissiom mode characters displayed:\n r (readable), w (writable), x (executable). - means everything is disabled, and d means it\u0026rsquo;s a directory file.  The default file permissions of new files can be set using umask, while the the permission mode of existing files can be set using the chmod command. For example:\n chmod +x adds a flag: execute. chmod -x removes a flag chmod 640 changes all flags to the octal number-coded version (0-7).  File security happens on three different levels in Unix:\n Owner mode. (mumber 1) Group mode. (mumber 2) Everyone else. (mumber 3)  Changing the owner of the file can be done using chown user:group [file]. To set read permission for any grouping (i.e. user, group or other) you add the value of 4 to the respective i, j, or k value, to set write permission, you add 2 and to set execute permission, you add 1. The values of 4, 2, and 1 are derived from the first three powers of two, i.e. 22, 21, 20 respectively. Thus to set the user permissions to rwx, you set i to 7 (4 for read + 2 for write + 1 for execute), to set the group and other permissions to r-x, you set j to 5 (4 + 1) and likewise for k.\nSome examples:\n permission 640 means the user can read and write, the group can read, and the rest can\u0026rsquo;t access the file. permission 777 means everyone can do everything. Bad idea.  Of course the root user can still write to files with permission 400.\nFile system (FS) flavors EXT (Extended File System) The EXT filesystem is originally developed for Unix-like Operating Systems. Its first variant came into market in 1992. Variant by variant this has overcome the limitations like size of single file, size of volume, number of files in a folder or directory. Journaling was introduced in ext3, and extended features in ext4, such as persistent pre-allocation, delayed allocation, an unlimited number of subdirectories, checksums, transparent encryption, online defrag.\nExt4 is backwards compatible with Ext2, meaning you can safely mount an older disk formatted with the older file system using ext4 drivers. Using Ext filesystems in Windows usually requires some driver that enables mounting of these file systems (Ext2Fsd for example).\nFAT (File Allocation Table) FAT stands for File Allocation Table and this is called so because it allocates different file and folders using lookup tables. This was originally designed to handle small file systems and disks. This system majorly has three variant FAT12, FAT16 and FAT32 which were introduced in 1980, 1984 and 1996 respectively.\n  Remember this? src: http://www.youtube.com/watch?v=EHRc-QMoUE4   (Magnetic) Floppy disks that could store up to 1.2 MB and later on 1.44 MB, were ideally for this FS. FAT32 is still mostly used in pen drive and micro SDs. It can store or copy a file with a max size of 4GB (size of a single file to be stored). If the size of file exceeds 4GB it won‚Äôt copy on storage media, but its partition size was up to 8TB (size of partition on which FAT could be applied).\nNTFS (New Tech. File System) Microsoft too has moved on from FAT to NTFS with the introduction of Windows NT (nice intro video for the curious student) (New Technology - what an original name!) in 1993. This was an enhanced and more advanced version of FAT systems. All modenr Windows installations are done on NTFS. NTFS has no file size limit and no partition or volume limit.\nModern features of this brand New Technology:\n Journaling Transactions File Compression Security  Since NTFS is a proprietary technology, is has long been difficult to mount NTFS partitions using Unix systems. Read-only is no problem, but writing is another matter. Even on OSX, it still requires the use of third-party software such as Mounty.\nNFS (Network File System) File systems usually enable the storage of files on local disks. The NFS FS, Network File System, does enable us to manage files in a distributed manner, where multiple client machines can connect to one or a few servers. To be able to do that, there are two different File Systems:\n A client-side one, connected to a network layer A server-side one, connected to a network layer - with underlying effective FS    src: http://bigdata-guide.blogspot.com/2014/02/network-file-system-nfs.html   Client system calls such as read(), write() and others pass through these. That means the Virtual FS is completely transparent: no special API is required. A network protocol (TCP/UDP) through a network layer enables communication between both client and server. The server is also called the \u0026lsquo;file server\u0026rsquo;. NFS was originally developed by Sun Microsystems, and it contains much needed server crash recovery modules.\nNFS version 2 migrated from a stateful to a stateless network protocol. Statelessness means the server is unaware of what the client is doing (what blocks are being cached, for instance) - it simply delivers al information that is required to service a client request. If a server crash happens, the client would simply have to retry the request.\nCommon File System techniques Fragmentation A hard disk drive has a number of sectors on it, each of which can contain a small piece of data. Files, particularly large ones, must be stored across a number of different sectors (in file fragments). As files grow in size (for example, by altering them), these chunks drift apart, depending on the file system. in the old FAT FS, file parts are simply saved as close to the physical start of the disk as possible, and so on, becoming fragmented over time.\n  src: https://7datarecovery.com/best-registry-cleaners/   NTFS is a bit smarter, allocating free buffer space around the files on the drive. However, after a set amount of time, NTFS systems still become fragmented. The Defragmentation process then is the process of rearranging file fragments from different sectors closer to each other, to minimize disk IO, especially without solid state.\nThe EXT file system handles sectors differently. The FS scatters different files all over the disk, leaving a large amount of free space between them. When a file is edited and needs to grow, there‚Äôs usually plenty of free space for the file to grow into. This means fragmentation will only occur if the disk space is used up from 80% and onwards.\nTransactions When programs need to make multiple changes at FS level, if one or two changes fail for some reason, no change at all should occur. That is the main premise of a transactional system, in which rollback behavior will occur when something goes wrong.\nIt\u0026rsquo;s the same principle applied to most database transactions: a transaction should be an atomic operation, meaning no change or influence can occur in-between different changes. This usually involves some kind of locking mechanism.\nWindows Vista introduced Transactions in NTFS, but the use is now discouraged. The transaction system has largely been superseded by the Journaling system.\nJournaling A definition from Wikipedia:\n A journaling file system is a file system that keeps track of changes not yet committed to the file system\u0026rsquo;s main part by recording the intentions of such changes in a data structure known as a \u0026ldquo;journal\u0026rdquo;, which is usually a circular log. In the event of a system crash or power failure, such file systems can be brought back online more quickly with a lower likelihood of becoming corrupted.\n Journaling minimizes the loss of data when hard disks crash by recording additional metadata in a circular log. EXT4 and NTFS both implement this technique.\nCompressing Some file systems, such as the open-source Btrfs, introduce transparent file compression. In NTFS, it is not completely transparent: you can enable NTFS compression by changing advanced attributes of directories in Windows Explorer. It does come at a cost: CPU power. Therefore, it should never be applied to OS-specific and frequently-modified files.\nExt4 does not have transparent file compression, but it can be enabled in conjunction with ZFS. The APFS (Apple File System), a proprietary system by Apple for High Sierra (10.13) and later, does come with auto-compression systems based on Zlib and LZVN. HFS+, Apple\u0026rsquo;s older FS, comes without it.\nMore Resources  https://kerneltalks.com/disk-management/difference-between-ext2-ext3-and-ext4/ https://www.freebsd.org/doc/handbook/network-nfs.html https://en.wikipedia.org/wiki/Ext4 https://en.wikipedia.org/wiki/Comparison_of_file_systems https://www.linux-magazine.com/Online/Features/Filesystems-Benchmarked https://selvamvasu.wordpress.com/2014/08/01/inode-vs-ext4/  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/c-integrated-ex/",
	"title": "1. Integrated C exercise",
	"tags": [],
	"description": "",
	"content": " Download the start project here: c-integrated-start.c.\nThe program represents a book, consisting out of multiple pages, of which each page contains (random) text, represented as a char[].\nThe following functions are given:\n printBook() prints all pages of the book (no need to modify this) clearBook() clears pages and frees memory (no need to modify this) createRandomBook() creates a new book consisting out of amount of pages createRandompage() creates a new page with random text main() bootstraps everything.  1. Commandline compiling Create a simplified Makefile which does the following when executing the command make:\n Clean up any left object files Compile the exercise Run the exercise  This file should also be submitted.\n2. Create two \u0026ldquo;books\u0026rdquo; This program makes a book of 10 pages, instances of the page struct, represented as a linked list. Each object has a random bit of text as it\u0026rsquo;s text value, stolen from the Lorem Ipsum generator (thank you, Cicero). In this exercise, you have to extend the program such that the book will be split up into two books: one with text on pages that start with a vowel, and another with text that starts with a consonant. The pages have to continue to exist, but have to be relinked. It is of paramount importance not to make copies of objects! (Remember pointers?)\nSo, output without modifications:\nprinting the list: faucibus quam. eget, elit. dui ipsum faucibus neque dui, velit. end of the list the list is empty Output with modifications:\nprinting the list: faucibus quam. dui faucibus neque dui, velit. end of the list printing the list: eget, elit. ipsum end of the list 3. Refer to previous objects Extend the structure in such a way that not only a pointer to the next page is available, but also to the previous one. Currently, the structure looks like this:\nstruct page { char text[100]; struct page* next; }; A third variable should be added called previous. Think about which of the above functions you need to modify in order to set the correct values. This is essentially creating a double-linked list instead of a single-linked one.\nChange the main() and printBook() functions such that they keep track of and print the TAIL of the list instead of the HEAD, using your newly created variable.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/intro-labs/",
	"title": "1.2: String Manipulation",
	"tags": [],
	"description": "",
	"content": " We are at War! We\u0026rsquo;re at war! The orcs are attacking and are looking very hungry! Look at them!\n  Orcs attacking! source: G.I.   1. String manipulation However, instead of simply killing you, these not so friendly looking beasts target vowels instead of bowels. So when speaking to, they munch and munch, stripping your carefully chosen words of all vowels. How rude. Implement a function called char* munch(char* sentence) that obscures all vowels with an \u0026lsquo;X\u0026rsquo;, and then prints the results. You will also need a int main() function.\nAssume a maximum character length of 100 for the input sentence.\nTips:\n Re-read chapter 1. How do you start writing a program in C? Create one file, create a main function, print something and compile/run to test if it works. Then expand. Will you be using scanf() or fgets() for user input? What is the difference? Look up how to use either functions. You can safely ignore the *. A char array gets converted to a pointer if returned or given as an argument. Remember, in Java, the function signature would simply be char[] munch(char[] sentence) Go through the GNU Coding standards. Methods in C are snake-cased: my_nice_method instead of Java\u0026rsquo;s camelcasing myNiceMethod.   INPUT: 'hello friendly green guys' OUTPUT: 'hXllX frXXndly grXXn gXys'  Start from this blueprint:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; char* munch(char* sentence) { char* response = malloc(sizeof(char) * 100); // TODO eat those vowels!  return response; } int main() { char sentence[100]; // TODO read input  printf(\u0026#34;INPUT: %s\\n\u0026#34;, sentence); printf(\u0026#34;OUTPUT: %s\\n\u0026#34;, munch(sentence)); } The correct use of malloc() will be explained in the coming labs.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/structs/",
	"title": "1.3: Creating order: Structures",
	"tags": [],
	"description": "",
	"content": " Structs The only way to structure data in C is using the struct keyword:\nstruct Person { int age; int gender; // no bool, remember?  char name[100]; // do not forget to add a size } We can use this structure to assign values like this:\nstruct Person jaak; // do not forget \u0026#34;struct\u0026#34; // jaak.name = \u0026#34;Jaak Trekhaak\u0026#34;; - this is too easy - won\u0026#39;t work strcpy(jaak.name, \u0026#34;Jaak Trekhaak\u0026#34;); // include \u0026lt;string.h\u0026gt; for this jaak.age = 80; jaak.gender = 1; Another way of assigning values is defining the values inline using the {}brackets:\nstruct Person jaak = { \u0026#34;Jaak Trekhaak\u0026#34;, // need to be in order of property definition  80, 1 }; The next question is, can we also define functions in a struct? Yes and no. A function pointer makes this possible, but it is not the same such as a member variable of a class in Java. C function pointers, however, can be very usefully used as callback methods:\n#include \u0026lt;stdio.h\u0026gt; struct Person { int age; int (*is_old)(); }; int is_old(struct Person this) { printf(\u0026#34;checking age of person: %d\\n\u0026#34;, this.age); return this.age \u0026gt; 60; } int main() { struct Person jaak; jaak.age = 40; jaak.is_old = \u0026amp;is_old; printf(\u0026#34;is jaak old? %d\\n\u0026#34;, jaak.is_old(jaak)); } It looks a bit weird because there is no such thing as a magical variable named this - that\u0026rsquo;s one argument you have to provide yourself. You can emulate functions as members of a data structure, but as you can see it\u0026rsquo;s going to cost you. The function pointer, or callback method, will be further explained in chapter 2.\nCompile and execute the above code. what happens when you comment out jaak.is_old = \u0026amp;is_old;? Implement another function with signature int scoff_at(struct Person p) that calls people old when they are called \u0026ldquo;Jaak\u0026rdquo;. Look at the previous example on how to expand the struct to add a name property.\n  Extra definitions Creating a person looks awkward: struct Person jaak; - why can\u0026rsquo;t we simply use Person jaak;? That is possible if you define your own types using the keyword typedef. It\u0026rsquo;s also useful to emulate your own string implementation:\ntypedef struct Person Person; typedef char* string; Magic numbers are usually defined on top, in header files, using #define. With some tricks we can emulate booleans in C:\n#define TRUE 1 #define FALSE 0  typedef unsigned short int bool; bool male = TRUE; These #define statements are preprocessor flags. These can be as simple as this example or as complex as switching on different CPU architectures and executing another set of rules depending on the outcome. Macros are expanded just before expanding, see the \u0026ldquo;compiling\u0026rdquo; section below. Intricate examples are visible at Wikipedia.\nTypical C code that you may encounter due to lack of a bool: if (result) {...} where result is an int. This is in no case the same as JavaScripts Truthy / Falsey construction! The number 0 is false. EOF,NULL or \\ 0 all evaluate to a number to use this.\nC\u0026rsquo;s By-Value VS Java\u0026rsquo;s By-Ref In C, everything you pass to functions is passed by value, meaning a copy of the value is created to pass to the called function. This is very important to grap because mistakes are easily made. For instance, by passing the Person struct, we copy it. Any changes made to the struct in the function are done ON THAT COPY:\nvoid happy_birthday(Person person) { person.age++; } int main() { Person jaak; jaak.age = 13; happy_birthday(jaak); printf(\u0026#34;%d\\n\u0026#34;, jaak.age); // HUH? Still 13? } The copy of jaak gets to celebrate, but jaak himself stays 13.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; main{main} happy{happy_birthday} jaak[Person jaak] copy[copy of jaak] jaak -.-|create copy| copy main --|push to main stack| jaak happy --|push to local fn stack| copy  To fix this, we need the use of pointers, as explained in chapter three. In Java, every object is passed by reference, meaning it points to the same value and changes will be persistent. As expected, In Java (and in pretty much any other programming langauge) this is not the case for primitives:\npublic static void increase(int i) { i++; } public static void main(String[] args) { int i = 5; increase(i); System.out.println(\u0026#34;i is \u0026#34; + i); // still 5 } Use of header Files The #include statements ensure the correct inclusion of functions in your program. Large programs consist of multiple C (source) and H (header) files that are glued together with compiling and linking. A header file contains function definitions, the declarations are in the source files:\n// person.h  struct Person { int age; } int is_old(struct Person p); With the following source file:\n// person.c  #include \u0026lt;stdio.h\u0026gt;#include \u0026#34;person.h\u0026#34; int is_old(struct Person p) { return p.age \u0026gt; 60; } int main() { struct Person jaak; jaak.age = 10; return 0; } The main function works as a bootstrapper and is never placed in a header file. Note the difference between brackets \u0026lt;\u0026gt; and brackets \u0026quot;\u0026quot; at include: that is the difference between system includes and own includes (use relative paths!).\nThe reason for splitting this up is that other source files also provide access to is_old() and Person and thus the ability to reuse things.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{person.h} --|source| B[person.c] B -- E[definition is_old] A{person.h} --|source| C[facebook.c] C -- F[use struct] A{person.h} --|source| D[twitter.c] D -- G[use struct]  If you create a separate header file and include them into the source files, there is no need to compile or link it separately. That is, gcc code.c still suffices. Only when you split up source code into separate source files, multiple output files will need to be compiled - and linked together (with one main() function present somewhere).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/c-debugging-testing/",
	"title": "2. Testing in C(++)",
	"tags": [],
	"description": "",
	"content": " Download the start project files here: cpp-testing.cpp, cpp-testing-main.cpp. Remember that in order to run this as a Google Test Suite, you need to compile it using g++ and bootstrap GTest in a separate .cpp file where the main() function resides. Go back to the lab notes if you\u0026rsquo;re not sure how to pull this off.\nThe following code is given:\n date and period structs to represent a date and a period of dates in code. isInPeriod() which needs to be completed, and takes a pointer of a period struct and a date, to check whether or not the date is inside the period.  The C++-specific code is kept to a bare minimum, but remember to compile using g++ because Google Test asks us to do so.\n1. Commandline compiling Create a simplified Makefile which does the following when executing the command make:\n Clean up any left object files Compile the exercise, using C++ and linking against GTest Run the exercise  The result of the executed start project is the following:\n Wouters-Air:exercises wgroeneveld$ ./a.out [==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from PeriodTest [ RUN ] PeriodTest.GoodLuckWithThat cpp-testing.cpp:23: Failure Expected: 0 To be equal to: 1 Add as many TEST cases as you can think of and complete isInPeriod()! [ FAILED ] PeriodTest.GoodLuckWithThat (0 ms) [----------] 1 test from PeriodTest (0 ms total) [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (0 ms total) [ PASSED ] 0 tests. [ FAILED ] 1 test, listed below: [ FAILED ] PeriodTest.GoodLuckWithThat 1 FAILED TEST  2. Create Test cases Replace the single test case called \u0026ldquo;GoodLuckWithThat\u0026rdquo; with as many test cases as you can think of to cover all possible cases. What if a date is outside of the period, being in the past, or in the future, or\u0026hellip; We expect at least 6 different scenarios - and of course they have to be unique. A test case which tests the same but with different data is not enough.\n3. Implement the isInPeriod method Only after creating test cases, you can move on to the completion of this method. It will return:\n 0 if the given date is not inside the period. 1 if the given date is inside the period.  Rerun your tests and make sure everything is passing (not by changing values in your test cases!).\n4. Add new functionality: merging two periods Add a new method:\nperiod* mergePeriods(period* one, period* two) { period* result = malloc(sizeof(period)); return result; } Write all possible test cases for this method. Some will fail because it simply returns an empty period instance, but let\u0026rsquo;s assume we\u0026rsquo;ve got working code here. You do not need to implement this method, only to think about Google Test cases for this one. Write as many TEST() cases as you can think of. And yes, you can ignore the memory leaks.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-interrupts/lab1_arduino-multfreq/",
	"title": "2.2: Arduino multi frequencies",
	"tags": [],
	"description": "",
	"content": " Time to make some harmonics.\n  image source: wikipedia.org   Three\u0026rsquo;s a charm The example below has to be completed. Three LEDs are placed and defined. If you run the code, the left-most LED starts blinking at 1 Hz. You can choose how you want approach this. Either you search for you very own Arduino and rebuild the design. Or, otherwise, you can use a (webbased) simulator like for example Tinkercad.\n   #define LED_1Hz 7 #define LED_2Hz 6 #define LED_3Hz 5  void setup() { pinMode(LED_1Hz, OUTPUT); pinMode(LED_2Hz, OUTPUT); pinMode(LED_3Hz, OUTPUT); } void loop() { digitalWrite(LED_1Hz, HIGH); delay(500); digitalWrite(LED_1Hz, LOW); delay(500); }    Complete the example so the three LEDs blink at 1 Hz, 2 Hz, and 3 Hz, respectively.\n  Something to point out is the preciseness of the LEDs. If you\u0026rsquo;d hold a high-speed camera on the setup and do some precise timing, it becomes clear that the frequencies are not very precise. Take the example of 1 Hz. Executing the digitalWrite() function is not instantaneous. As you\u0026rsquo;ll learn in this course, executing a function involves some steps which are overhead. Nonetheless, if lives were at stake, this could be measured very precisely. The additional overhead time could be reduced from the 500 ms, to achieve a better precision.\nMaking a second \u0026lsquo;application\u0026rsquo; A second application that can be run on the Arduino is a Serial port echo service. You know this is a killer-app and could make you millions ;-)\nMake a program that can read in a character from the Serial port. The Arduino should simply echo the incoming byte in HEX.\n  Merge the two applications The third exercise is simple, at least on paper screen it is.\nMerge both programs.\n  What is effect of the additional application on the preciseness of the blinking LEDs\u0026rsquo; frequency ?\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-interrupts/",
	"title": "2: Interrupts",
	"tags": [],
	"description": "",
	"content": " Chapter 2 Interrupts, the next step towards multitasking 2.1: Arduino Uno\n Intro Low level programming Bit manipulations Hello hardware, this is software speaking Two beating hearts  2.2 (lab): Bare metal Arduino\n2.3: The timer\n 16-bit Timer Timer frequency Polling vs Interrupt Configuring the timer with interrupts  2.4: (lab) Launch that rocket !!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-debugging/debugging-gdb/",
	"title": "4.2: The Hard Way: GDB",
	"tags": [],
	"description": "",
	"content": " 2. The hard way: Command-line debugging using GDB In order to fluently debug binary programs, they have to be compiled with the debug flag, gcc -g. This will add metadata to the binary file that gdb uses when disassembling and setting breakpoints. IDEs automatically add metadata like this when you press the \u0026ldquo;Debug\u0026rdquo; button on them, but since this is a command-line application, we need to do everything ourselves.\n2.1 With debug flags Let\u0026rsquo;s start with a heap-based application we would like to inspect:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;string.h\u0026gt; char password[] = \u0026#34;250382\u0026#34;; int main(int argc, char *argv[]) { int stackvar = 5; char *buf = (char *)malloc(100); char *secret = (char *)malloc(100); strcpy(secret, password); printf(\u0026#34;Crackme! \\n\u0026#34;); printf(\u0026#34;Password? \u0026#34;); scanf(\u0026#34;%s\u0026#34;, buf); if (!strcmp(buf, secret)) { printf(\u0026#34;Password OK :)\\n\u0026#34;); } else { printf(\u0026#34;Invalid Password! %s\\n\u0026#34;, buf); } return 0; } If the source code would not have been supplied, making an estimated guess against the password would take a (very) long time. (We naively assume here that the program has been compiled with debug information enabled).\nCompile using gcc -g hackme.c. Take a look at the filesize - without flag:\n wouter@wouter-Latitude-7490:~/Development$ gcc hackme.c -o hackme.bin \u0026\u0026 ls -la | grep hackme.bin -rwxr-xr-x 1 wouter wouter 8568 Jan 7 19:59 hackme.bin  With flag:\n wouter@wouter-Latitude-7490:~/Development$ gcc hackme.c -g -o hackme.bin \u0026\u0026 ls -la | grep hackme.bin -rwxr-xr-x 1 wouter wouter 11352 Jan 7 19:59 hackme.bin  Star the gdb debugger using gdb [binary]. It will enter the interactive gdb shell, where you can set breakpoints, step through code, and have a chance at inspecting the heap, where we might attempt to figure out what\u0026rsquo;s hidden in there.\nThings you need to know from the GDB debugger:\n r: running the program (main() method execution) c: continue after a breakpoint i: inspect (i r [regname]: inspect register name) start and next: start stepping through the application. b *[addr]: set breakpoint at certain function/line/*address (see manual). Conditionals are possible, for instance: break func if arg == 3. delete: deletes all breakpoints disassemble [fn]: disassembles functionname (after running) x/[length][format] [address expr]: inspect dynamic memory block (see manual) print x: print var, or \u0026amp;var address (Enalbe printing of addresses: show print address) info address/line (fn) or source  Bootstrap gdb and step through the whole application. As soon as the stackvar has been evaluated, try to inspect the memory value using x/d. The address expression could be hexadecimal, or \u0026amp;stackvar.  How could you evaluate a heap variable using the x command? If you have the address, how do you pry out the value on the heap?\n  More useful commands:\n Don\u0026rsquo;t remember which breakpoints you\u0026rsquo;ve set? info b. (info breakpoints) Don\u0026rsquo;t remember where you\u0026rsquo;re at now? Inspect the stack: bt (backtrace), optionally appended with full that includes local variables.  Do not forget that the expression that is printed out is the one to be evaluated after you enter the \u0026lsquo;next\u0026rsquo; command. You can already inspect the stack variable address but it will contain junk:\n (gdb) start Temporary breakpoint 1 at 0x7d9: file hackme.c, line 11. Starting program: /home/wouter/Development/hackme.bin Temporary breakpoint 1, main (argc=1, argv=0x7fffffffdd68) at hackme.c:11 11 int stack = 5; (gdb) x/d \u0026stack 0x7fffffffdc6c: 21845 (gdb) next 12 char *buf = (char *)malloc(100); (gdb) x/d \u0026stack 0x7fffffffdc6c: 5  Address 0x7fffffffdc6c first contains 21845 - a coincidence that might have another value on your machine.\nBootstrap gdb, disassemble the main function, and set breakpoints after each malloc() call using b *[address]. You can check the return value, stored at the register eax, with i r eax.\n  How come something interesting is hidden in eax after calling malloc()?\n Because eax is the return value register, or the accumulator. You should be familiar with it due to other Hardware-oriented courses. Because malloc returns a void pointer - read the man pages carefully!  Interested in more useful registers to fiddle with? Check out info registers in the gdb console (i r is an abbreviation).\n2.2 Without debug flags Now try to \u0026lsquo;hack\u0026rsquo; the password using gdb without the -g compiler flag. Imagine someone has put up a binary file on the internet and you managed to download it. No source code available, and no debug information compiled in. The gdb tool still works, disassembling still works, but method information is withheld. That means calling start and next will not reveal much-needed information about each statement, and we will have to figure it out ourselves by looking at the disassembly information.\nTry to disassemble again and look at the heap value of our secret. Notice that you will not be able to use something like x [varname] because of the lack of debug information! We will have to rely on breakpoints of address values from the disassembly.\n  Remember to always run the program first before disassembling - otherwise address values will be way too low, and thus incorrect. bt does noet help us either here: No symbol table info available.\nWhen inspecting the return value of eax, gdb returns a relative address for our current program (8 BITS), while we need an absolute one (16 BITS) when using the x command to inspect the heap. Look at the disassembly info to prepend the right bits:\n ---Type  to continue, or q  to quit--- 0x0000555555554844 : mov -0x8(%rbp),%rdx 0x0000555555554848 : mov -0x10(%rbp),%rax 0x000055555555484c : mov %rdx,%rsi ... (gdb) b *0x00005555555547ea Breakpoint 1 at 0x5555555547ea (gdb) r Starting program: /home/wouter/Development/osc-labs/solutions/debugging/a.out Breakpoint 1, 0x00005555555547ea in main () (gdb) i r eax eax 0x55756260 1433756256 (gdb) x 0x55756260 0x55756260: Cannot access memory at address 0x55756260 (gdb) x 0x0000555555756260 0x555555756260: 0x00000000  As you can see, 0x55756260 is an invalid memory address, but based on the disassembly info, we can deduce it is actually 0x0000555555756260 we need to look at.\nThe (still) hard way: DDD, a UI on top of GDB Instead of invoking gdb, one can also employ ddd. This is a crude UI on top of the gdb debugger, with multiple windows where the same commands can be entered as you have learned so far. However, ddd also allows you to visualize heap/stack variables while stepping through the application. The Figure below shows a screen-shot of a debug session of our hackme app using ddd.\nThings to try out:\n Display the Source Window via the View menu. This window lets you set breakpoints and interact with the source code. Display the Machine Code Window via the View menu. This window is the equivalent of bt (backtrace) in gdb. Right-click on a line in source (compile with -g again!) -\u0026gt; Add breakpoint Start/step using the buttons or the commands in the cmdline window. Right-click in the main window -\u0026gt; \u0026ldquo;New Display\u0026rdquo; to add variables by name to watch (for instance buf and password, as shown). You can also watch references to functions - any valid gdb-style expression will do.  Take a moment to fiddle with ddd. Try to inspect the same heap variable as the previous exercises, but this time visualize them in the main window. It should be (slightly) easier to accomplish.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-introos/lab1_wetfeet/",
	"title": "5.2: Getting your CLI-feet wet",
	"tags": [],
	"description": "",
	"content": "  Source: Neo\nGetting your feet wet This \u0026lsquo;lab\u0026rsquo; consists of a list of small tasks that introduce you to the command-line interface. None of these tasks should require exhaustive manual labour. Most can be accomplished by using the commands you already know (and that are listed at the cheat sheet), and for others a short Google session should give you the answer.\nNavigation  figure out in which folder you are at the moment navigate to the root folder of the OS navigate to your home folder (/home/username), without actually typing that path make an alias (man alias) that navigates to your home folder from everywhere within the system  File manipulation  make a folder \u0026ldquo;myVeryOwnFolder\u0026rdquo; navigate into that folder create a file: hello.txt that contains the text hello world. Do this in a single step (hint: you can \u0026ldquo;pipe\u0026rdquo; commands together) move up on folder and remove the entire directory myVeryOwnFolder, including the .txt file  Access permissions  create a new file with any content make this file read-only for everyone remove the file make a folder ToBeDeleted remove the execute rights from this folder for ALL users remove this folder  Various  navigate to osc-exercises/ch5_os (from your cloned git repository)  all these files contain random text one file contains the word \u0026ldquo;bamboozle\u0026rdquo; find out which file contains this word find out the line number on which the word occurs  search the man page for the meaning of \u0026ldquo;-x\u0026rdquo; in the command ls display the current date and time (is this correct? Why wouldn\u0026rsquo;t it be?) execute three commands, using the enter key only once  display the current date and time sleep for 10 seconds display the current date and time   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab1_processmgmt/",
	"title": "6.2: Processes (lab)",
	"tags": [],
	"description": "",
	"content": "    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nCreating and inspecting processes TIP: the unistd.h header has some useful functions. You may also want to check out the wait() function in sys/wait.h and exit() in stdlib.h.\n Write a C-program that prints its own PID on the screen.    An example output    Write a C-program that spawns another process using fork(). Both parent and child processes announce their existence (through a printf) and their PIDs.    An example output    Write a C-program that creates 4 child processes using fork()  each of the childeren checks which numbers below 10000 are prime every child reports only numbers that are prime (using printf), together with its own PID before exiting, a child must announce how many prime numbers it has found in total TIP: don\u0026rsquo;t worry too much about the efficienty of calculating whether a number is prime or not. The calculation is meant to be time-consuming. Answer this question: how are the processes scheduled? Do they all execute one after the other or at the same time? How can you tell?     An example output   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/lab1_algorithms/",
	"title": "7.2: Scheduling algorithms (lab)",
	"tags": [],
	"description": "",
	"content": "   image source: unsplash.com   Let\u0026rsquo;s compare ! We have discussed a number of algorithms the scheduler can use to do it\u0026rsquo;s job. Let\u0026rsquo;s compare them. We assume the following tasks:\n T1: arrives @ 0s, takes 10s, and has priority low T2: arrives @ 1s, takes 2s, and has priority high T3: arrives @ 4s, takes 5s, and has priority high T4: arrives @ 7s, takes 1s, and has priority medium  Compare Average Throughput, AJWT, and AJCT of the 4 algorithms we\u0026rsquo;ve seen up until:\n cooperative (non preemptive) FCFS, cooperative (non preemptive) SJF, preemptive priority based, and preemptive round robin  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/inspection-labs/",
	"title": "8.2: Inspecting memory regions",
	"tags": [],
	"description": "",
	"content": " Accompanying Screencast:\n  1. No malloc, no heap Let\u0026rsquo;s look at memory regions of a process that does not call malloc(). This means we will not use the heap just yet. Compile the following code:\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main() { getchar(); // breaks process until char input  return 0; } To explore the above example, we will introduce getchar() before ending the main() loop, so the program pauses and gives us a change to look under the hood.\nInspect the memory regions of the above program while running it. Look up the process ID using ps aux and browse through the files using cat in /proc/[procid].\n  Locate the following:\n 7f8122192000-7f8122193000 rw-p 00000000 00:00 0 7fffc566a000-7fffc568b000 rw-p 00000000 00:00 0 [stack] 7fffc577d000-7fffc5780000 r--p 00000000 00:00 0 [vvar] 7fffc5780000-7fffc5782000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]  No [heap] region allocated yet. Let\u0026rsquo;s do the same, but use malloc() to allocate a random block of memory. The return value of the method is a void* that can be printed to show the address of the heap region. Use printf(\u0026quot;%p\u0026quot;, p).\n2. malloc: use heap Extend the above example and allocate dynamic memory. What happens in the /proc files? Can you see how the returned address is inside the heap region?\n  Let us use the program strace to figure out what malloc() exactly does. It should be a system call that asks the Operating System to allocate a certain amount of dynamic memory. But we have yet to figure out which system calls exactly are called. The program strace is used to track system calls and signals. Don\u0026rsquo;t be surprised by the amount of calls a simple program like ours makes! Let us try to find the calls for memory allocation.\nIt is a good idea to printf(\u0026quot;before malloc! \u0026quot;) to make sense of the strace output.\nuse strace ./program. It will output a lot of text, ending with read(0, (that is our getchar() break). Can you find the syscalls we are after?\n   munmap(0x7f88a38dc000, 114791) = 0 brk(NULL) = 0x5644fb34d000 brk(0x5644fb36e000) = 0x5644fb36e000 fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0 write(1, \"0x5644fb34d260\\n\", 150x5644fb34d260 ) = 15 fstat(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0 read(0,  Aha, brk! What is that? Use man brk to find out more.\nThe data segment gets expanded by moving the program break pointer up, so the heap is actually an extension of the data segment of the program.\n3. multiple mallocs What happens when we call malloc() multiple times in a row?\nExtend the program by allocating 1024 bytes four times in a row. Inspect the program again using strace. What do you notice?\n  Memory allocation is optimized by avoiding a system call each time. It firsts allocates more than needed that will (hopefully) suffice.\nInstead of calling malloc four times, let\u0026rsquo;s see what happens when we use a loop to see how many times brk is called. More importantly, we will also see that he heap grows \u0026lsquo;upward\u0026rsquo;!\nExtend the program by allocating 1024 bytes in a for loop (Use getchar() before and after the loop) that counts to a random high number. Print \u0026ldquo;loop\u0026rdquo; and \u0026ldquo;end\u0026rdquo; before and after the loop. Inspect again using the trace tool, and also take a look at the maps file in the process folder. After entering a key you can re-inspect everything.\n   Before loop: 556a1a947000-556a1a968000 rw-p 00000000 00:00 0 [heap] After loop: 556a1a947000-556a1aa4f000 rw-p 00000000 00:00 0 [heap]  Converted to decimal: 93914201911296 - 93914200965120 = 946176 bytes used.\nIndeed, the Figure from chapter 7.1 and our findings confirm that the heap grows upwards.\n4. Free Until now the above examples have never taken into consideration the fact that one has to get rid unused space using free(). We will leave it up to you to inspect what happens in /proc.\nExtra: Using Valgrind to inspect the memory heap Valgrind is a useful commandline tool that makes it easy for C programmers to inspect how much dynamic (heap) memory a program actually consumed, and how much of it was freed. It is a lot easier to use than dabbling in different disassemble commands, but it does NOT come with the GNU toolchain. Use apt-get install valgrind to install it onto your virtual machine.\nLet\u0026rsquo;s assume a simple program that reserves some memory, fills in the blanks, and then frees up some space: (also available in the osc-exercises repository)\n#include \u0026lt;stdlib.h\u0026gt; int main() { int* ptr; ptr = malloc(sizeof(int) * 1000); // we allocated 4000 bytes (since an int is usually 4 bytes)  free(ptr); return 0; } After compiling this, we can let the tool figure out how much space we took up, how many leaks there were, and much more:\n Wouters-MacBook-Air:ch8-stack wgroeneveld$ valgrind ./a.out ==87742== Memcheck, a memory error detector ==87742== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al. ==87742== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info ==87742== Command: ./a.out ==87742== --87742-- run: /usr/bin/dsymutil \"./a.out\" warning: no debug symbols in executable (-arch x86_64) ==87742== ==87742== HEAP SUMMARY: ==87742== in use at exit: 22,529 bytes in 188 blocks ==87742== total heap usage: 268 allocs, 80 frees, 32,649 bytes allocated ==87742== ==87742== LEAK SUMMARY: ==87742== definitely lost: 3,472 bytes in 55 blocks ==87742== indirectly lost: 2,832 bytes in 9 blocks ==87742== possibly lost: 0 bytes in 0 blocks ==87742== still reachable: 0 bytes in 0 blocks ==87742== suppressed: 16,225 bytes in 124 blocks ==87742== Rerun with --leak-check=full to see details of leaked memory ==87742== ==87742== For lists of detected and suppressed errors, rerun with: -s ==87742== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)  Try to let the above program produce a few memory leaks. Does valgrind notice you did not clean up your mess? The definitely lost amount should skyrocket after a few uncleaned malloc() calls. Are amount of reported bytes correct? Recalculate this manually.\n  Further Reading  Hack the Virtual Memory: malloc, the heap \u0026amp; the program break Valgrind quickstart  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/lab1_segfault/",
	"title": "9.2: Deliberate Segfaulting",
	"tags": [],
	"description": "",
	"content": "   source: xkcd.com   X A to the Z  Run the program below and verify that it runs without errors\n#include \u0026lt;stdio.h\u0026gt; #define WRITE_SIZE 26 #define READ_SIZE 27  int main(void) { int i; unsigned char alphabet[26]; for(i=0;i\u0026lt;WRITE_SIZE;i++) { alphabet[i] = 65 + i; } for(i=0;i\u0026lt;READ_SIZE;i++) { printf(\u0026#34;%2d -\u0026gt; %c (%02x)\\n\u0026#34;, i, alphabet[i], alphabet[i]); } return 0; } Recompile the code above, but verify that you have the -0s option added. This compiler flag sets the optimisation towards size. What do you learn ?\n Increase the WRITE_SIZE to 27. Compile and run again. Any errors ? If so, what type of error could this be ?\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/rtos/",
	"title": "X.2: Real-time Operating Systems",
	"tags": [],
	"description": "",
	"content": " Operating Systems In the previous chapters, many aspects are handled that form the Operating System. The image below, linked from Wikipedia, shows the most important components in the Linux kernel.\n  source: wikipedia.com   The image above shows:\n Task management Memory management the IO subsystem    The desired situation.      The real situation.     The main goal of the OS is still the same: allow multiple programs and/or users use the same hardware. On the left-hand side of the image above a visual representation is given of what we want to experience. Everything is running truly in parallel. We have touched on a number of reasons why reality is more like is shown on the right-hand side of the image. The OS will do its best to achieve the desired situation as good as possible. However, sometimes doing your best is not good enough.\nTry to imagine these situations (and the possible explanations from the developers):\n A pacemaker missed an arrhythmia  Sorry, at that moment the tiny OS was switching tasks.  A surveillance drone crashed against a construction crane  Sorry, at that moment the drone should have detected the crane, the flight computer was in the middle of a TCP time-out while sending telemetry.  The motor of a high-end sports car burned out due to a faulty-timed gear shift  Sorry, at the moment the timer ended, the microcontroller was handling a switch in radio stations.   Sometimes doing your best is not good enough\n Real-time Operating Systems While a general OS tries its best to meet all constraints, a sporadic failure is not a vital problem. If there is a dip in network handling resulting in a short lag of a youtube-clip the computer will not crash-and-burn. Operating systems like this are called soft real-time operating systems and examples are: Linux, Windows, iOS, \u0026hellip;\nLike illustrated above, some situations have tasks which have to meet their constraints. Failing to do so results in a system failure. For these applications Real-Time Operating Systems (RTOS) can be used. These operating systems are called hard real-time systems. They have very specific deterministic constraints and ALL of them should be MET, at ALL TIME. Examples of RTOSes are:\n FreeRTOS mbedOS Contiki Xenomai (there are many more)  For the sake of completeness it is pointed out that tweaks are available to turn the Linux kernel into a real-time kernel.\nFreeRTOS   The FreeRTOS logo   FreeRTOS is an open source real-time OS that is tailored for embedded systems. You can run this on the Arduino we touched upon in chapter 2. Again \u0026hellip; you can run this OS on the little microcontroller üòÑ\nThis OS essentially consists out 5 files:\n tasks.c: handles task management queue.c: handles queues \u0026amp; synchronisation list.c: handles lists port.c: details for porting to a specific processor heap_x.c: handles the heap  FreeRTOS is a trade-off between bare-metal programming and the luxury of an OS. With everything you\u0026rsquo;ve seen in this course you should be able, after some studying perhaps, to understand how a task is described.\nvoid runClock(void* pvParameters) { short i, j, k, l; for/*ever*/(;;) { for(i=0;i\u0026lt;24;i++){ for(j=0;j\u0026lt;60;j++){ d[0]= j % 10; d[1]= (j-d[0]) / 10; d[2]= i % 10; d[3]= (i-d[2]) / 10; for(k=0;k\u0026lt;10;k++) { for(l=0;l\u0026lt;4;l++) { _delay_ms(5); }\t} }} } /* end of for/*ever*/ // no return statement } The function above is the program/tasks that simulates time. The time is written in shared memory that is accessible by other programs/tasks.\nThe main function could like this:\nint main(void) { /* Perform any hardware setup necessary. */ // define the outputs to be the 4 digit-selectors and the 7 segement-selectors  DDRB |= 0b00111111; DDRD |= 0b11111000; /* APPLICATION TASKS CAN BE CREATED HERE * eg. xTaskCreate(TaskBlinkGreenLED, (const portCHAR*) \u0026#34;GreenLED\u0026#34;, 256, NULL, 3, NULL); * with 1st argument: name of the function * 2nd argument: human readable name (only for debugging purposes) * 3rd argument: stacksize (in \u0026#34;words\u0026#34; (words*stackwidth = memory)) * 4th argument: function parameters * 5th argument: priority (0 ... (configMAX_PRIORITIES ‚Äì 1)) * 6th argument: pxCreatedTask can be used to pass out a handle to the task being created. This handle can then * be used to reference the task in API calls that, for example, change the task priority or * delete the task. **/ xTaskCreate(runClock, (const portCHAR*) \u0026#34;runclock\u0026#34;, 256, NULL, 3, NULL); xTaskCreate(showOnDisplay, (const portCHAR*) \u0026#34;displayer\u0026#34;, 256, NULL, 3, NULL); /* start the scheduler */ vTaskStartScheduler(); /* Execution will only reach here if there was insufficient heap to start the scheduler. */ for/*ever*/ (;;); return 0; } Outdated ? One could state that RTOS-es are outdated. Processors and OS-es, anno 2020, are so powerful that no additional measures should be taken to guarantee specific constraints. While that might be true, there is also scalability.\nIt might be feasible to write all the desired software (networking stacks, logging, sensor reading, \u0026hellip;) in such a way all constraints are met, BUT it requires a state-of-the-art system. For example: a processor with 2 GB of RAM memory and 160 GB of solid state storage. Installing this in every gearbox of a car, for example, simply is too expensive. If the same constraints and performance requirements can be met for a fraction of the price, industry dictates the latter option should be chosen.\nThat\u0026rsquo;s good news for whoever is studying this course, as skill-full programmers and engineers are a necessity to make this work üòÑ\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-interrupts/theory2_timer/",
	"title": "2.3 The timer",
	"tags": [],
	"description": "",
	"content": " With our sleeves rolled up and dirt all the way up to our elbows, it\u0026rsquo;s time to grab a coffee and do so reading/studying again. In this section interrupts will be introduced through the use of a timer.\nA lot of applications that are built using an Arduino need some sense of time. As mentioned earlier the ATMega has 3 dedicated timers on chip: two 8-bit timers and one 16-bit timer. Through these timers the concept of time can be implemented.\n16-bit Timer One component that is in almost every microcontroller is a Timer/counter. This timer/counter can be used for many things: stopwatch, wave generation, timing, \u0026hellip; The block diagram of the 16-bit timer/counter in the Arduino is shown here. Next to this 16-bit timer/counter there are 2 8-bit timer/counters available. The documentation reports registers like TCNTn and OCnB. The letter n is a placeholder for the number of the timer. In the case of the micro controller on the Arduino, the 16-bit counter results in n = 1.\n  Block Diagram of the 16-bit counter in the Arduino microcontroller. Source: arduino.cc   During normal operation the counter simply counts from 0x0 to its maximum value.. The left-side image below shows what happens to the 16-bit value TCNT1. The counter can also be used in a different mode: Clear-timer-on-compare (CTC). In this mode the timer starts again at 0x0000, but only counts to a certain value. When this value is reached, the counter resets and starts over. The right-side image below shows the CTC mode with the cap at 0x7FFF and at 0x3FFF.\n  Value of TCNT during normal operation of the 16-bit counter.      Value of TCNT during clear-timer-on-compare (CTC) operation of the 16-bit counter.     The value to which the timer counts in CTC mode can be set through the register OCR1A: the Output Compare Register of timer 1 named A. As can be seen in the block diagram, there is a comparator in the Timer block that compares TCNT1 and OCR1A. The result of this comparator can be evaluated in the control logic. By configuring the timer this control logic can be altered.\nWhat is the frequency at which the counter reaches its maximum value in normal mode ?\n  Timer frequency To determine the frequency at which the counter operates a quick visit to the datasheet holds the answer. The clkT1 can be generated from an external or internal clock source: clkI/O. This incoming clock goes through a prescaler which can be set by the Clock Select bits (CS12, CS11, and CS10).\n  Clock Distribution (source: Datasheet)      Clock Select bits for prescaler cconfiguration (source: Datasheet)     To introduce a clock, a crystal oscillator  is used. The one on the Arduino UNO board runs at a frequency of 16\u0026rsquo;000\u0026rsquo;000 Hz or 16 MHz. This is the frequency at which the CPU is working, often referred to as the clock speed. Given that frequency, and the value of the pre-scaler, the clock frequency at which the counter operates can be calculated.\n\\( f_{clk_{T1}} = { f_{clk_{I/O}} \\over prescaler_{T1}} = { 16e6 \\over 2^{10}} = 15'625 Hz \\) Because the counter needs 216 clock ticks to reach its maximum, this maximum will be reached every \\( 2^{16} \\over 15625 \\) = 4.19 s. If a LED were to be toggled every time the counter reaches its maximum, the frequency of the toggling LED would be (2 * 4.19 s)-1 = 0.119 Hz.\n  Value of TCNT during clear-timer-on-compare (CTC) operation of the 16-bit counter.   Polling vs Interrupt Before you put down your coffee to get back to work let\u0026rsquo;s discuss polling first. A nice illustration of polling is shown below.\n   \u0026nbsp;\nOur toddler is polling her father. Another example of polling is shown below. This clip, however, ends with an interrupt.\n   \u0026nbsp;\nWould it not be nice that the CPU could just continue working on something else until a certain event occurs eg. the timer reaching its maximum value ? In the second example (the one with the cartoon), the co-pilot interrupts what the processor was doing.\nAn interrupt is a signal that goes to the processor signalling a certain event. There are two sources for this interrupt: hardware and software. The timer that reaches his maximum count and signals this to the processor is an example of a hardware interrupt. An example for a software interrupt could be an attempt of a division by zero.\nConfiguring the timer with interrupts As can be seen from the block diagram on the top of this page, 4 registers are available for interaction with the software:\n TCNTn: Timer/counter value for counter n OCRnA/OCRn: Output Compare Register A for counter n (also available for B) ICRn: Input Capture Register TCCRnA: Timer/Counter Control Register for counter n  The first three registers (TCNT, OCRA/OCRB, and ICR) are all 16-bit registers. Their functionality is self-explanatory. The Control Register needs more explanation, though. Luckily for us, there is \u0026hellip; the datasheet.\n  Timer/Counter Configuration Register 1 (TCCR1)   The following configuration are given. For other configuration the reader is invited to consult the datasheet for him/herself.\n CS12 | CS11 | CS10 : Clock Select as explained above. Dividing the incoming clock with a 1024-bit prescaler gives configuration: 101. WGM13 | WGM12 | WGM11 | WGM10 : Waveform Generation Mode. This is the formal name for the normal mode or the CTC mode that were discussed above. Normal mode of operation is achieved by setting these bits to: 0000 IC.. : Input Capture (Noise Cancellation and Edge Select) are irrelevant for this mode of operation and can be both set to: 0 COM1.. : What should happen when OCRA or OCRB can be chosen from 4 different options (hence the 2 bits). Both these settings are irrelevant for this mode of operation and can be both set to: 00  In summary, TCCR1A should be set to 0b00000000 (or 0x00) and TCCR1B should be set to 0b00000101 (or 0x05).\nThe settings above configure TIMER/COUNTER 1 to operate like requested, but the interrupts still have to be set. To set this, two different flags need to be enabled. In the final section of this chapter these two flags are elaborated on. For now, it suffices to know there is a general Interrupt Enable and a \u0026lsquo;maskable\u0026rsquo; Interrupt Enable. The former can be set by calling the function sie() the latter should be set through a register: Timer Interrupt Mask Register (TIMSK1).\n  Timer Interrupt Mask Register (TIMSK1)   As can be seen from the image above, there are other Interrupts that can be set. For this example only the final one (Timer Overflow Interrupt Enable) should be set to one. This results in setting TIMSK1 to 0b00000001 (or 0x01).\nFinally, when the interrupt is triggered, something should happen. Otherwise this would be quite useless. When an interrupt occurs, the processor halts whatever it was doing and executes a function. In stead of \u0026ldquo;calling\u0026rdquo; the function ourselves, this function call is done automatically. To be able to distinguish between different interrupt sources, different prefixed function names are available. The next section of this chapter will elaborate on this as well. The function-name that is linked to the TIMER/COUNTER 1 overflowing is: TIMER1_OVF_vect.\nThe example code below puts everything together.\n#define LED_1Hz 7  int led0_status = 0; ISR(TIMER1_OVF_vect) { if(led0_status == 1) { led0_status = 0; digitalWrite(LED_1Hz, LOW); } else { led0_status = 1; digitalWrite(LED_1Hz, HIGH); } } void setup() { pinMode(LED_1Hz, OUTPUT); /* configure TIMER/COUNTER 1 */ TCCR1A = 0x00; TCCR1B = 0x05; /* enable interrupt mask */ TIMSK1 = 0x01; /* enable the interrupts */ sei(); } void loop() { } The C-file above can be found in the Virtual Machine as /home/osc/osc-exercises/ch2_interrupts/example2.c\n -- Compile and run the code above. What is the frequency at which the LED toggles ?\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/tasks_inter_thread_communication/",
	"title": "3. Inter-thread communication",
	"tags": [],
	"description": "",
	"content": " Analyse exercise 6_4_3.c (shown below) and write a report (2-3 pages):  describe what the program does (in general, not line-by-line) make a line chart that plots \u0026lsquo;the elapsed real time\u0026rsquo; the program consumes (y-axis) in function of \u0026lsquo;the number of threads that are contributing\u0026rsquo; (x-axis)  for this, you change the NUMBER_OF_THREADS in the program to different values and re-run the program to observe the time it takes to complete Note: the scale on the X-axis goes from 2 to 300 at least 8 data points should be present in the chart you can use any charting program for this (for example excel or google sheets) TIP use the command time  interpret the chart and draw conclusions explain what you think would happen if you would create more than one producer thread (with MAX_COUNT divided between the threads) actually extend the program so it runs more than one producer thread  Note: sem_trywait can be used to good effect here  compare the actual outcome with your hypothesis from step 4 and explain why this happens   hand in the report and the extended source code via toledo\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;semaphore.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;signal.h\u0026gt; #define NUMBER_OF_THREADS 5 #define MAX_COUNT 500000  static sem_t obj_produced; static sem_t obj_consumed; static int shelf; void * producer() { int i; for(i=2; i\u0026lt; MAX_COUNT; i++) { shelf = i; sem_post(\u0026amp;obj_produced); sem_wait(\u0026amp;obj_consumed); } return NULL; } void * consumer() { unsigned char isPrime; int i; int VUT; while(1) { sem_wait(\u0026amp;obj_produced); VUT = shelf; sem_post(\u0026amp;obj_consumed); printf(\u0026#34;[CONSUMER] %d\\n\u0026#34;, VUT); isPrime = 1; for (i=2;i\u0026lt;VUT; i++) { if (VUT % i ==0) { isPrime = 0; } } if(isPrime==1) { printf(\u0026#34; thread #x announces that %d is prime.\\n\u0026#34;, i); } } } int main(void) { int i = 0, err; pthread_t tid[NUMBER_OF_THREADS]; // create semaphores  err = sem_init(\u0026amp;obj_produced, 0, 0); if(err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create semaphore: obj_produced [%s]\u0026#34;, strerror(err)); return 1; } err = sem_init(\u0026amp;obj_consumed, 0, 0); if(err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create semaphore: obj_produced [%s]\u0026#34;, strerror(err)); return 1; } // create producer thread  err = pthread_create(\u0026amp;(tid[i]), NULL, \u0026amp;producer, NULL); if (err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create producer thread: [%s]\u0026#34;, strerror(err)); return 1; } printf(\u0026#34;Producer thread created\\n\u0026#34;); // create consumer threads  for(i=1;i\u0026lt;NUMBER_OF_THREADS;i++) { err = pthread_create(\u0026amp;(tid[i]), NULL, \u0026amp;consumer, NULL); if (err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create consumer thread %d: [%s]\u0026#34;, i, strerror(err)); } printf(\u0026#34;Consumer thread %d created\\n\u0026#34;, i); } // wait for producer thread  pthread_join(tid[0], NULL); // kill consumer threads  for(i=1;i\u0026lt;NUMBER_OF_THREADS;i++) { pthread_kill(tid[i], 9); } // delete the semaphores  sem_destroy(\u0026amp;obj_produced); sem_destroy(\u0026amp;obj_consumed); return 0; }"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-pointers/pointers/",
	"title": "3.1: Pointers and arrays",
	"tags": [],
	"description": "",
	"content": " In the schematic examples below, the following concepts can be explicitly distinguished:\n The name of a variable The value of a variable  And the following implicitly:\n The type of a variable The adres of a variable  The type of a variable determines the amount of memory that is freed up to be able to save the value. The value is either a current value or an address that refers to a different value. Each variable has a unique address. Variables can therefore refer to each other.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; A[varnamevalue] B{varnameaddress}  This example visualizes the instruction int a = 5:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; A[a5]  Where the type, int, reserves a few bytes depending on the target platform (sizeof(int)), on a specific address. We can manipulate the address itself - this is essential when working with arrays.\n\u0026ldquo;Compound\u0026rdquo; types C uses a pass-by-value system to pass variables to functions. This means that the value is copied, and that function cannot make changes to the original value. That is something positive: separation of concerns.\nWhen we think of our person example of chapter 1, that struct is therefore always copied. That can be very inefficient, depending on the size of the data! To avoid this, we use a \u0026ldquo;pointer\u0026rdquo;: a reference to the current data. Objects are passed by-reference by default in Java - so in C we have to do something extra for this.\nInstead of is_old(struct Person person) the signature becomes is_old(struct Person * person) (note the added asterik *). We have two options for reading a value here:\n \u0026ldquo;dereferencing\u0026rdquo; the pointer: asking for the real value, behind the reference. Following the arrow where it points towards, so to speak. Ask for members of the pointer using \u0026ldquo;.\u0026rdquo;.  Because in C, the . operator takes precedence over *, we have to add brackets to combine both: (*person).age. It is annoying to constantly have to use brackets, so the creators came up with an alternative, the -\u0026gt; operator: person-\u0026gt;age.\n(*pointervariable).property equals to pointervariable-\u0026gt;property.\n In Java properties are accessed using the dot operator ..\nPointer types A pointer is a \u0026ldquo;changeable\u0026rdquo; reference to a variable. Pointers have their own memory address on the stack and can refer to something else at any time: they are not constant. They are recognizable by * after variable type.\n#include \u0026lt;stdio.h\u0026gt; int main() { int young = 10; int old = 80; int *age = \u0026amp;young; age = \u0026amp;old; printf(\u0026#34;%d\\n\u0026#34;, *age); printf(\u0026#34;%d\\n\u0026#34;, age); } What will be printed in the above example? The first line should be obvious, but the second one\u0026hellip;\n  mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{*age} --|after first assignment| B[young10] A -.- |after second assignment| C[old80]  Notice the use of the \u0026amp; operator, it is the address-of operator to fetch the address of a variable. A pointer points to an address, not to a value (of a variable).\nLook at it this way: I live in streetname, city. When I give you my card, you have a reference to my address. I can hand out cards to more people. The card does not represent my house, but points towards it. If you wish to do so, you can write a different address on the card, eliminating my previous address. From that point on, your card points to a different address, while other cards I dealt out still point to my original address.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; C{my card} --|contains directions to| A[my house] B[your house]  If I want to get the address of your house, I\u0026rsquo;d have to use the address-of operator: \u0026amp;your_house. If I want to get the house itself (physically impossible\u0026hellip;), I\u0026rsquo;d use the dereference operator on the card: *card. This simply follows the arrow where card currently points to.\nSo, what was the output of printf(\u0026quot;%d\\n\u0026quot;, age);? 1389434244! Huh? We are printing the address of the pointer, not the actual value (by following where it points to). Remember, to do that, you have to use the dereference * operator: printf(\u0026quot;%d\u0026quot;, *age);. The compiler hints at this with the following warning:\n warning: format specifies type \u0026lsquo;int\u0026rsquo; but the argument has type \u0026lsquo;int *\u0026rsquo; [-Wformat]\n C\u0026rsquo;s By-Value VS Java\u0026rsquo;s By-Ref - redux Pointers can point to pointers which can point to pointers which can \u0026hellip; Add enough * symbols!\nint val = 10; int *ptr = \u0026amp;val; int **ptr_to_ptr = ptr; int **ptr_to_ptr = \u0026amp;ptr; Why does int **ptr_to_ptr = ptr; generate a compiler error?\n  mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{\"**ptr_to_ptr\"} --|ref| B{\"*ptr\"} B -- |ref| C[val10]  Practical use of the double ** notation would be to relink a pointer to another location. As you know from chapter one, variables in C are passed along by value: even pointer values. This means a copy of a pointer is created whenever calling a function with a pointer. Chaining the actual value is possible by following the address using the dereference operator. But chaining the address itself is only possible with double pointers:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;void increase(int* nr) { (*nr)++; } void reassign(int** nr, int* dest) { *nr = dest; } int main() { int* nr = malloc(sizeof(int)), *nr2 = malloc(sizeof(int)); *nr = 10, *nr2 = 5; increase(nr); printf(\u0026#34;%d\\n\u0026#34;, *nr); // prints 11  reassign(\u0026amp;nr, nr2); printf(\u0026#34;%d\\n\u0026#34;, *nr); // prints 5 } Pointer instantiation Where does a new pointer point to that is not yet instantiated?:\nint *ptr; printf(\u0026#34;%d\u0026#34;, *ptr); // prints -12599900072 Whoops. Always assign \u0026lsquo;nothing\u0026rsquo; to a pointer, using int *ptr = NULL. Note that depending on the C implementation (such as VC++, clang, GNU C), an uninitialized pointer might contain the value 0.\nNULL is a platform dependent (!!) macro that in C refers to zero (0), usually in the form of a void pointer. A void * pointer can refer to any type and is usually used to address low-level memory, as we will see using embedded hardware equipment.\nWhat gets printed in the above example if we assign NULL to *ptr?\n  The definition of a pointer does not prescribe the exact location of the *: int* age is the same asint *age (notice the placement of the stars). Be careful with things like \u0026lsquo;int *age, old_age\u0026rsquo;! The last variable here is an ordinary int, and not a pointer!\nFunction pointers Now things are getting interesting. A pointer can also point to a function. (Remember the datastructure from chapter one?). You will need the same signature definition to do that:\n#include \u0026lt;stdio.h\u0026gt; int increase(int nr) { return nr + 1; } int doublenr(int nr) { return nr * 2; } int main() { int (*op)(int) = \u0026amp;increase; printf(\u0026#34;increase 5: %d\\n\u0026#34;, op(5)); op = \u0026amp;doublenr; printf(\u0026#34;double 5: %d\\n\u0026#34;, op(5)); return 0; } The definition of the op pointer looks a bit strange, but the signature predicts that we will return an int (far left), and that one parameter is needed, also in the form of anint (in brackets). If you fail to do so (for instance, by creating a double doublenr(int nr) function), weird things happen, but the program does not crash:\n Wouters-Air:development jefklak$ gcc test.c \u0026\u0026 ./a.out test.c:15:8: warning: incompatible pointer types assigning to 'int (*)(int)' from 'double (*)(int)' [-Wincompatible-pointer-types] op = \u0026doublenr; ^ ~~~~~~~~~ 1 warning generated. increase 5: 6 double 5: 14  Function pointers can also be given as a parameter, for example with void exec (int (* op) (int)) {. A function can return a function (pointer), for example with int (* choose_op (int mod)) (int) {. The function \u0026ldquo;choose_op\u0026rdquo; expects 1 int parameter and returns a function pointer that refers to a function with 1 int parameter and return value int. To simplify that mess, typedef is usually used:\n#include \u0026lt;stdio.h\u0026gt; typedef int(*func_type)(int); int increase(int nr) { return nr + 1; } int doublenr(int nr) { return nr * 2; } func_type choose_op(int mod) { return mod == 0 ? \u0026amp;increase : \u0026amp;doublenr; } void exec(int (*op)(int)) { printf(\u0026#34;exec: %d\\n\u0026#34;, op(5)); } int main() { exec(choose_op(0)); // print 6  exec(choose_op(1)); // print 10  return 0; } Now you understand how we used the \u0026lsquo;callback function\u0026rsquo; is_old() in the Person struct in chapter 1.\nWatch out for syntax! Remember that symbols such as * en \u0026amp; have different meanings.\n int *p; - * after a type: it\u0026rsquo;s a pointer. p = \u0026amp;i - \u0026amp; used in an expression: address-of operation *p = i - * used in an expression: dereference operation  Ponder on this  What is the difference between char msg[] = \u0026quot;heykes\u0026quot; and char *msg = \u0026quot;heykes\u0026quot;? Clarify your answer with a drawing. Wat is the difference between int a[10][20] and int *b[10]? Can you also say something about memory usage? In which case would you definitely use pointers in C, and in which case would you not? Explain your choice. What happens when I get the address of a stack variable, like \u0026amp;x in section \u0026lsquo;changing values around\u0026rsquo;, but the stack got cleared because the method call was finished?  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-pointers/",
	"title": "3: Pointers &amp; Arrays",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Pointers and Arrays Chapter 3 handles the following subjects:\n Arrays in C Pointers in C: arrays don\u0026rsquo;t exist! By-reference, By-value malloc(), free() Function pointers Practical examples of pointers:  Aritmetics Linked Lists Swapping Values   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-debugging/testing/",
	"title": "4.3: The Middle Way: TDD",
	"tags": [],
	"description": "",
	"content": " Test-Driven Development: Google Test A concept you learned to love in the Software Engineering Skills course.\nIt\u0026rsquo;s concepts and definitions will not be repeated here, but we will introduce Google Test, a unit testing framework for C/C++ that enables us to write tests to track down bugs and reduce the amount of time needed dabbling in gdb. That is one of the major advantages of using automated test frameworks.\nGoogle Test is a C++ (11) framework, not a C framework! We will be using g++ instead of gcc to compile everything. C++ files are suffixed with .cpp instead of .c. Major differences between both languages exist but will not be needed to know all about in order to write a few simple tests. Since g++ is not installed on the image by default, use apt install g++ to download and install the toolchain.\n A. Installation Most open source libraries require you to download the source code and compile it yourself. For Google Test, we will do exactly that, since we are learning how to work with compiling and making things anyway. We want to only compile googletest, and not googlemock - both are part of the same repository.\n Clone the github repository: https://github.com/google/googletest/. We want to build branch v1.10.x - the master branch is too unstable. Remember how to switch to that branch? Use git branch -a to see all branches, and git checkout -b [name] remotes/origin/[name] to check it out locally. Verify with git branch. cd googletest Create a builddir and navigate into it: mkdir build, cd build Build Makefiles using cmake: cmake ./../ Build binaries using make: make.  If all goes according to plan, two libraries will have been created:\n libgtest.a ligbtest_main.a  In the subfolder googletest/googletest/build/lib.\nB. Usage Using the library is a matter of doing two things:\n1. Adding include folders You will need a main() function to bootstrap the framework:\n// main.cpp #include \u0026#34;gtest/gtest.h\u0026#34; int main(int argc, char *argv[]) { ::testing::InitGoogleTest(\u0026amp;argc, argv); return RUN_ALL_TESTS(); } And another file where our tests reside:\n// test.cpp #include \u0026#34;gtest/gtest.h\u0026#34; int add(int one, int two) { return one + two; } TEST(AddTest, ShouldAddOneAndTo) { EXPECT_EQ(add(1, 2), 5); } TEST(AddTest, ShouldAlsoBeAbleToAddNegativeValues) { EXPECT_EQ(add(-1, -1), -2); } What\u0026rsquo;s important here is the include that refers to a gtest/gtest.h file. The gtest directory resides in the include folder of your google test installation directory. That means somehow we have to educate the compiler on where to look for the includes!\nThe -I[directory] (I = include) flag is used to tell g++ where to look for includes.\n 2. Linking with the compiled libraries When running the binary main() method, Google Test will output a report of which test passed and which test failed:\n Wouters-MacBook-Air:unittest wgroenev$ ./cmake-build-debug/unittest [==========] Running 2 tests from 2 test cases. [----------] Global test environment set-up. [----------] 1 test from SuiteName [ RUN ] SuiteName.TrueIsTrue [ OK ] SuiteName.TrueIsTrue (0 ms) [----------] 1 test from SuiteName (0 ms total) [----------] 1 test from AddTest [ RUN ] AddTest.ShouldAddOneAndTo /Users/wgroenev/CLionProjects/unittest/test.cpp:18: Failure Expected: add(1, 2) Which is: 3 To be equal to: 5 [ FAILED ] AddTest.ShouldAddOneAndTo (0 ms) [----------] 1 test from AddTest (0 ms total) [----------] Global test environment tear-down [==========] 2 tests from 2 test cases ran. (0 ms total) [ PASSED ] 1 test. [ FAILED ] 1 test, listed below: [ FAILED ] AddTest.ShouldAddOneAndTo 1 FAILED TEST  However, before being able to run everything, InitGoogleTest() is implemented somewhere in the libraries we just compiled. That means we need to tell the compiler to link the Google Test libraries to our own application.\nAdd libraries as arguments to the compiler while linking. Remember to first use the -c flag, and afterwards link everything together.\n Bringing everything together:\n Wouters-MacBook-Air:debugging wgroeneveld$ g++ -I$GTEST_DIR/include -c gtest-main.cpp Wouters-MacBook-Air:debugging wgroeneveld$ g++ -I$GTEST_DIR/include -c gtest-tests.cpp Wouters-MacBook-Air:debugging wgroeneveld$ g++ gtest-main.o gtest-tests.o $GTEST_DIR/build/lib/libgtest.a $GTEST_DIR/build/lib/libgtest_main.a -lpthread Wouters-MacBook-Air:debugging wgroeneveld$ ./a.out [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from AddTest [ RUN ] AddTest.ShouldAddOneAndTo  As you can see, it can be handy to create a shell variable $GTEST_DIR that points to your own Google Test directory. To do that, edit the .bashrc file in your ~ (home) folder. Remember that files starting with a dot are hidden by default, so use the -a flag of the ls command. Add the line:\nexport GTEST_DIR=/home/[user]/googletest/googletest\nAnd reopen all terminals. Verify the above using echo $GTEST_DIR, it should print out the path.\nThe -lpthread linking flag tells the compiler to link the standard threading libraries along with anything else, that are needed by GTest internally. We will get back on these in chapter 6.  Without this flag, you will get the following errors: \u0026ldquo;ld returned 1 exit status, undefined reference to pthread_[fn]\u0026rdquo;\n C. \u0026lsquo;Debugging\u0026rsquo; with GTest Going back to the crackme implementation, a simplified method that verifies input is the following:\nint verify(char* pwd) { // return 1 if verified against a pre-determined password, 0 otherwise. } Write a set of tests for the above method - BEFORE implementing it yourself! Time to hone your TDD skills acquired from the course \u0026lsquo;Software Engineering Skills\u0026rsquo;. Simply copy it into the test file, or include it from somewhere else. You should at least have the following edge cases:\n right password entered wrong password entered empty password (what about NULL or \u0026quot;\u0026quot;?)  Use the GTest macro EXPECT_TRUE and EXPECT_FALSE. These correspond to JUnit\u0026rsquo;s AssertTrue() and AssertFalse().\nAgain, watch out with the order in which parameters should be passed (expected/actual)! See Google Test Primer.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/interprocess/",
	"title": "6.3: Inter Process communication",
	"tags": [],
	"description": "",
	"content": " Having multiple processes running is all good-and-well. Hey, it is one of the main reasons why the concept of an OS was introduced, remember ? Right, good job !!\nIt would make sense, though, if different processes were able to communicate with each other. That\u0026rsquo;s what this Section is about.\nThere are two main techniques to facilitate communication between multiple processes. These two techniques are shown in image below.\n Shared memory Message passing    The two main techniques for inter process communication    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nShared memory Shared memory is \u0026hellip; memory that is shared. Normally multiple processes are not allowed to read/write to each other\u0026rsquo;s memory space. This is enforced by the operating system. Errors, similar to the one in the example below, are generated by the OS if a processes try to access areas that it is not allowed to access.\n#include \u0026lt;stdio.h\u0026gt; int main(void) { int i, my_array[8]; for(i=0;i\u0026lt;=8;i++) { my_array[i] = i+1; } for(i=0;i\u0026lt;8;i++) { printf(\u0026#34;%d\\n\u0026#34;, my_array[i]); } return 0; }   An example of memory protection that given by the OS   The code above exceeds the allowed stack space. Try to find out why this happens.\n  The technique of using shared memory allows other processes to gain access certain regions of the address space. Both processes have to be aware that the memory is not protected by the OS. A programming API for using shared memory is provided by POSIX (Portable Operating System Interface). The example below shows a producer on the left (a process which puts data inside of the shared memory) and a consumer on the right (which uses the data it gets from the producer).\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/shm.h\u0026gt;#include \u0026lt;sys/mman.h\u0026gt; int main() { const int SIZE = 4096; /* buffersize (bytes) */ const char *name = \u0026#34;OS\u0026#34;; /* shared memory object name */ const char *data_0 = \u0026#34;Hello\u0026#34;; const char *data_1 = \u0026#34;World!\u0026#34;; int shm_fd; /* shared memory file descriptor */ void *ptr; /* create the shared memory file descriptor */ shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666); /* configure the size of the shared memory file */ ftruncate(shm_fd, SIZE); /* memory map the shared memory file */ ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0); /* write to the shared memory file */ sprintf(ptr,\u0026#34;%s\u0026#34;,data_0); ptr += strlen(data_0); sprintf(ptr,\u0026#34;%s\u0026#34;,data_1); ptr += strlen(data_1); return 0; }   #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/shm.h\u0026gt;#include \u0026lt;sys/mman.h\u0026gt; int main() { const int SIZE = 4096; /* buffersize (bytes) */ const char *name = \u0026#34;OS\u0026#34;; /* shared memory object name */ int shm_fd; /* file descriptor */ void *ptr; /* open the shared memory file */ shm_fd = shm_open(name, O_RDONLY, 0666); /* memory map the shared memory file */ ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0); /* read from the shared memory file */ printf(\u0026#34;%s\u0026#34;,(char *)ptr); /* remove the shared memory file */ shm_unlink(name); return 0; }     p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nNote: to compile these, you need to pass the -lrt flag to gcc (\u0026ldquo;link with library rt\u0026rdquo;) like so: gcc -o producer producer.c -lrt.\nThis very simple example uses shared memory. Try to find answers to the questions below:\n Try to find out what these programs do What is the size of the memory that is shared ? Can a producer read from the shared memory ? Can a consumer write to the shared memory ? How do both processes know which data is shared ? In other words, how does the consumer decide which memory it connects to? Do both processes have to be active at the same time for the memory sharing to work? Why (not)?    Message passing The second technique for for InterProcess Communication (IPC) comes in the form of message passing. This method is a bit more restricted than using raw shared memory, but also easier to use ans safer because of that. Here we touch on 2 different mechanisms for achieving this: signals and pipes.\nSignals Signals are the cheapest form of IPC. They literally allows one process to send a signal to another process, through the use of the function kill(). Although the name might be a bit misleading, it can be used to send different signals. These signals are conceptually a bit similar to the Interrupts we saw in Chapter 2: they allow a program to act on information coming in from the outside without itself having requested it. The snippet below shows the different types of signals that can be sent:\njvliegen@localhost:~/$ kill -L 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX  There is also has CLI-compatible command kill that can send these signals to any running process (addressed by their PID). For more information on the kill command, add the --help argument, read the man-page (man kill), or ask the Internet.\nTwo short-cuts are typically available in the CLI that allow for the sending of signals: CTRL+C and CTRL+Z. The former sends a SIGINT signal while the latter sends a SIGTSTP signal.\nLet\u0026rsquo;s illustrate this with an example:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #define DURATION_IN_MINUTES 10  int main(void) { int i = DURATION_IN_MINUTES * 60; for(;i\u0026gt;=0;i--) { printf(\u0026#34;TIMER: 00:%02d:%02d\\n\u0026#34;, (int)((i-i%60)/60), i%60); sleep(1); } return 0; } This program will emulate an egg timer. Every second it displays how much time is left. Once the process starts running, it takes 10 minutes to complete. This process can be killed by just pressing CTRL+C. Note that you don\u0026rsquo;t have to manually do anything for this to work: your program automatically listens for this signal and exits the program when it is received.\njvliegen@localhost:~/$ ./egg_timer.bin TIMER: 00:10:00 TIMER: 00:09:59 TIMER: 00:09:58 TIMER: 00:09:57 TIMER: 00:09:56 ^C Another way to kill the process would be to explicitly send the signal through the kill command. To use this command, the PID is needed as an argument. Through a new CLI-window, this PID has to be searched for first. Note that the type of signal is an argument in the command.\njvliegen@localhost:~/$ ps ux | grep timer jvliegen 5041 0.0 0.0 4504 772 pts/1 S+ 06:04 0:00 ./egg_timer.bin jvliegen 5066 0.0 0.0 21996 1080 pts/2 S+ 06:05 0:00 grep --color=auto timer jvliegen@localhost:~/$ kill -KILL 5041 Try this for yourself. You can use the longhello program (~/osc-exercises/ch6_tasks/longhello.c) from Section 6.1. Run this program and try to kill it using both approaches that were explained above. (Ofcourse this means you shoud run it again, after you killed it the first time üòÉ )\n  Although there are numerous uses for sending signals between signals, one more example is interesting to have a closer look at. Above there was already some hinting to CTRL+Z.\nThe CLI is running a shell, as you already know by now. This offers just a single interface. If you were to start a program, that CLI is occupied (you cannot type or execute any commands). Imagine you are working remotely on a server (e.g., through ssh): this would require you to open up a new connection to the server and have a second shell at your disposal every time you executed a longer running command (e.g., starting a web server). A more convenient solution would be to send the running program to the background.\n  An example of a program that needs to be killed with CTRL-C   Before you can send processes to the background, the process has to be halted first. This can be done through the CTRL+Z shortcut. With a halted process, the command bg sends the halted process to the background. If you do not send it to the background, the process will freeze. Once it is in the background it unfreezes and continues running. Additionally, this gives you back your shell.\njvliegen@localhost:~/$ xeyes ^Z [1]+ Stopped xeyes jvliegen@localhost:~/$ bg [1]+ xeyes \u0026amp; jvliegen@localhost:~/$  For the sake of completeness we enumerate a few more usefull aspects about this:\n a process can be started in the background as well. This can be achieved by adding an ampersand after the command (e.g., xeyes \u0026amp;) the command jobs gives you an overview of which jobs are running in the background through the command fg \u0026lt;#\u0026gt; the job with index number \u0026lt;#\u0026gt; will pulled to foreground.  Try this for yourself. If the xeyes program is not installed, install it first!\n  Pipes Another option to achieve message sending is through pipes. There are two different types of pipes available:\n anonymous pipes named pipes  Anonymous pipes are like waterslides. You can put some data on it on one end (the top of the slide) and it comes out the other (the bottom), but it\u0026rsquo;s not possible to go up the waterslide from the bottom. Put differently: communication is half-duplex (single direction). One process can write into the pipe, while the other can read from of the pipe. This type of pipe can only be create between two processes that have parent-child relation ship. What happens internally is that the stdout of the first process is mapped to the stdin of the second process. For this, we use the | (pipe) character.\nWhen using the CLI, anonymous pipes are a very powerful tool for chaining different commands. The output of the first command will be the input for the next command. This can be chained multiple times.\njvliegen@localhost:~/$ xeyes \u0026amp; jvliegen@localhost:~/$ ps -ux | grep xeyes | head -1 | cut -d \u0026#34; \u0026#34; -f 3 5526 jvliegen@localhost:~/$  The example above chains the following:\n give a list of all my processes (ps = process status) only filter the lines that contain the word xeyes (grep stands for Global Regular Expression Print) filter only the first line (head) split the input on a space (\u0026rdquo; \u0026ldquo;) and report only the third field  Let\u0026rsquo;s try something similar for yourselves: * Use unnamed pipes to display all the processes of which you are the owner. From these processes only display the PID and the first 10 characters of the process\u0026rsquo;s name (the COMMAND column). From this list, only show the first 10 processes. * Then, add another command to sort the output by descending PID (so the largest PID is on top, the smallest on the bottom)\n  An example output     Do you remember the Process Control Block ? This has one field called list of open files. We\u0026rsquo;ve already touched upon stdin, stdout and stderr. Using anonymous pipes will add an entry to this list.\nWe can also relink the 3 default open files to other targets. For example, instead of writing output and errors to the command line, we can redirect them to a file. Similarly, we can read input from a file instead of from the keyboard:\n  Redirection of the standard output    The syntax for this is a bit weird though: 1\u0026gt; is meant to redirect data that normally goes to stdout, while 2\u0026gt; is used to relink stderr. You can also point directly to the existing stdout/stderr by using \u0026amp;1 or \u0026amp;2 respectively:\n process 1\u0026gt;{STDOUT} 2\u0026gt;{STDERR} process 1\u0026gt;{STDOUT} 2\u0026gt;\u0026amp;1 process \u0026lt; {STDIN} (read from a file at location {STDIN} rather than from the keyboard)  Note that here we\u0026rsquo;re using the \u0026gt; pipe here instead of | as above. The difference is subtle, but a simple explanation is that \u0026gt; deals with mapping a command to a file (or something that pretends to be a file, like stdout/stderr), while | maps a command to another command.\nNamed pipes are the other type of pipes that can be created. The main differences with anonymous pipes are the lifetime of this mechanism and their presence in the file system.\nThe anonymous pipes above only live for as long as the processes live. Named pipes instead persist and have to be closed explicitly (or are closes automatically at system-shutdown).\nNamed pipes also have an actual presence in the file system. That is, they show up as files. But unlike most files, they never appear to have contents. Even if you write a lot of data to a named pipe, the file appears to be empty. Making named pipes can be done through the mkfifo command.\nAs they are not frequently used, we leave the interested reader to man page üòÉ.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/towards/",
	"title": "7.3: Towards real-world schedulers",
	"tags": [],
	"description": "",
	"content": " The previously discussed scheduling algorithms are but a select number of a huge amount of imaginable approaches that can be thought of. We have seen that all individual algorithms come with certain challenges that make them difficult for direct use in real-world scenarios. And we haven\u0026rsquo;t even taken into account all variables that are in play in a typical OS!\nAs such, in this Section, we first look at a few factors that come into play in real systems. We then look at how the naive schedulers we\u0026rsquo;ve already seen can be adapted to deal with these new problems. Finally, we discuss how all of this has been combined in practice in the Linux OS scheduler over time.\nReponsiveness vs Efficiency Context switching As said previously, when a new task is scheduled for execution by the OS, a number of operations need to happen to swap the old task with the new one. In the previous examples, we\u0026rsquo;ve pretended these operations happen instantly, but that\u0026rsquo;s of course not the case.\nSay we have 2 user tasks: X and Y. X has already been running for a while on the processor, while Y is in the ready state, waiting for CPU-time. Say that the OS is using a preemptive scheduler (for example a pure Round-Robin algorithm) and decides to give Y some running time. Switching between the tasks involves the following steps:\nStep 1\nX has be stopped in such a way that it can continue from where it left off the next time it is scheduled. Therefore, a snapshot has to be made: what is the value of the program counter, what are the values in the registers, which address is the stack pointer pointing to, what are the open files, which parts of the heap are filled, \u0026hellip; ? All these values need to be stored. As was discussed in Chapter 6, the OS keeps a PCB (Process Control Block) for every process, which contains the fields necessary to store all these required parameters: The PCB of X needs to be stored. The kernel puts this PCB in a list of paused tasks.\nNote: something similar happens for Threads (remember there\u0026rsquo;s also a conceptual TCB), but it\u0026rsquo;s more lightweight, since there is more shared state between threads in the same program and so fewer aspects need to be updated.\nStep 2\nX is removed from the processor.\nStep 3\nThe scheduler uses the RR scheduling algorithm to determine which process is next. Since Y is the only other process, it is selected. The processor searches Y\u0026rsquo;s PCB in its list of paused tasks.\nStep 4\nEverything that happened to the PCB of X, now needs to be done in the opposite direction with the PCB of Y: the PCB of Y needs to be restored. The program counter is read and the next instruction is loaded. The values of the registers are restored. The stack pointer is updated.\nStep 5\nY starts to run on the processor.\nIn the previous Section, we have called this series of actions \u0026ldquo;Dispatching\u0026rdquo;. As such, the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency. The actions of updating (and restoring) individual PCB\u0026rsquo;s is typically referred to as a context switch (though some sources also call the entire process together a context switch).\nAs you can see, during the dispatch period, the CPU is not actually doing any useful work: it is waiting and/or updating its state so the new task can start running. As such, the act of dispatching a new process is considered 100% overhead, and it should be avoided as much as possible. Note: there are also other aspects that make context switches slower, such as the fact that it often means that data in the cache memory is no longer useful. As the cached data belongs to the previous task, the cache needs to be (partially) flushed and updated with data from the new task as well, which again takes time.\nHowever, before you get the wrong idea, it\u0026rsquo;s not all that bad. In practice, the dispatch latency is typically in the order of (10) microseconds (say about 1/100th of a millisecond). Still, if we were to switch processes for example each millisecond, we would have a full 1% overhead, which over time definitely adds up (we don\u0026rsquo;t only spend more time context switching, we also do a larger amount of context switches over time). If you recall from the previous section, we introduced the metric CPU efficiency (\u0026#0951;CPU), which helps make concrete how much overhead actually was introduced.\nWe can see that we somehow need to strike a balance between being CPU efficiency (less overhead) and keeping the system responsive (switching between tasks often enough). This is easy enough in our simple examples with just 3-10 tasks, but modern systems often run hundreds of tasks at the same time.\nThe concept of the time slice, discussed in the previous section on Round-Robin scheduling, can play a large part in this: shorter time slices make things more responsive, but cause more context switches, and vice versa. As such, we want to determine an ideal time slice length, but it\u0026rsquo;s not easy to see how this can be accomplished. In general, we can really only say that the time slice should always be quite a bit larger than the dispatch latency, but that we don\u0026rsquo;t really have an easy way to determine an upper bound.\nI/O-bound vs CPU-bound tasks A second aspect that\u0026rsquo;s highly relevant in modern systems is that there are typically two large classes of tasks: I/O-bound and CPU-bound tasks.\nThe I/O-bound tasks typically run for only short amounts of time (a few milliseconds) before they already have to wait for an I/O operation. Put differently, these tasks pause themselves (go into the \u0026ldquo;waiting\u0026rdquo; state) often. A good example is a program that\u0026rsquo;s listening for user input (keyboard/mouse). These tasks are thus sometimes also referred to as interactive tasks.\nThe CPU-bound tasks typically do not require outside input and are often computationally heavy. They run for tens of milliseconds (or much more) without ever yielding/waiting themselves. These jobs typically process data in large chunks, and are sometimes called batch tasks.\nThe fact that there are typically few processes that do \u0026ldquo;something in between\u0026rdquo; again makes it difficult to determine a good time slice length. If there are many I/O-bound tasks, shorter timeslices are probably better, as most tasks will pause themselves frequently anyway, and we don\u0026rsquo;t loose much (extra) efficiency for higher responsiveness.\nIf there are many CPU-bound tasks, longer timeslices are probably better, as processes will typically fill their slices with useful work and we reduce the amount of context switches (and those tasks typically don\u0026rsquo;t need to be very responsive).\nSimply using an average time slice that\u0026rsquo;s \u0026ldquo;somewhere in between\u0026rdquo; can produce the worst of both worlds: it lowers the responsiveness in interactive systems, while (needlessly) increasing the amount of context switches during batch processing.\nAs in the previous subsection, it\u0026rsquo;s unclear how long a time slice should ideally be to deal with both I/O and CPU-bound tasks.\nnot correct. Concluding that a system with a lot of IO-intensive tasks is better of with a smaller time slice, and a system with a lot of CPU-intensive tasks is better of with a larger time slice, would be more correct. If the latter is not the case, the CPU will not only spend a large percentage of time context switching, it will also do a larger amount of those context switches over time. As a rule of thumb it can be assumed that the time for a context switch is (a little) less than 10% of the time slice. --  Images like the one above, we've seen a number of times up until now. A question that arises is: **what happens on the dotted line ?** As was mentioned before the scheduler has two main jobs: 0. Choose the next task task that is allowed on the processor 0. Dispatching: switching tasks that are running on the processor The algorithms provide the scheduler with an approach to **choose the next task**. The second function a scheduler has is the **dispatching** of the newly chosen task. Let's break it down. ## Dispatching There are 2 user jobs: X and Y. X is running on the processor while Y is in the ready state, waiting for CPU-time. The scheduler decides that X's time is over an it's Y's turn on the processor. -- Time slice size Let\u0026rsquo;s illustrate this with an example:\n There are two I/O-bound tasks. Both run for 1ms, in which they update state and then wait/yield for more input.  Input becomes available after 3ms of wait time Both tasks do three rounds of this (wait for input 3 times in total)  There is one CPU-bound task that runs a total of 10ms without yielding All three tasks start at 0s The two I/O-bound tasks are higher priority than the CPU-bound task. Each task gets to complete its full time slice unless it yields by itself. In this very unrealistic system, the dispatch latency is a full 1ms  Draw schemas of how these tasks would be scheduled in two scenarios: (1) with a time slice of 2ms and (2) a time slice of 5ms.\nIndicate clearly each time a task goes into a ready state and don\u0026rsquo;t forget to take into account the high dispatch latency!\nFor each scenario, calculate the CPU efficiency (the percentage of time that the processor performs actual work: running task time - dispatch latency). Note that calculating AJWT is less useful here to compare both scenarios, as we have multiple waiting periods! As such, focus on the AJCT and calculate that for the three tasks as well.\nAnswer these questions:\n How many context switches are there in each scenario? Which scenario is more efficient? Why?    Try to understand the load that is put on the CPU. There is a periodic pattern.\n Try to draw the repeating pattern in this timing diagram How many context switches are there ? What is the efficiency of the CPU ? (Reminder: CPU efficiency (\u0026#0951;CPU): the percentage that the processor performs actual work.) Tip: read the title of this section !!    Time slice = 10 ms  Answer:  2. There are 11 context switches. 3. \u0026#0951;CPU = tuseful / ttotal = 110 ms / 121 ms = 0.90909  \u0026nbsp;tuseful = 10 x tIO + 1 x tCPU = 10 x 10ms + 1 x 10ms = 110ms \u0026nbsp;ttotal= tuseful + toverhead = 110 ms + 11 x 1 ms = 121 ms    ### The time slice is 100 ms Try to understand the load that is put on the CPU. There is a periodic pattern.\n Try to draw the repeating pattern in this timing diagram How many context switches are there ? What is the efficiency of the CPU ? (Reminder: CPU efficiency (\u0026#0951;CPU): the percentage that the processor performs actual work.) Tip: read the title of this section !!    Time slice = 100 ms  Answer:  2. There are 11 context switches. 3. \u0026#0951;CPU = tuseful / ttotal = 200 ms / 211 ms = 0.94787  \u0026nbsp;tuseful = 10 x tIO + 1 x tCPU = 10 x 10ms + 1 x 100ms = 200ms \u0026nbsp;ttotal= tuseful + toverhead = 200 ms + 11 x 1 ms = 211 ms    -- Priorities As discussed in the previous Section, a third aspect is that there is typically a need to indicate which processes are more important than others. This is usually done using priorities, whereby each task is assigned a number so they can be fully ordered to determine which is most important.\nIn the simple Priority-based scheduler we\u0026rsquo;ve considered, the priority was mainly used to determine when to start which process, as higher priority processes are selected earlier. However, we\u0026rsquo;ve also seen that this could lead to starvation for low-priority tasks, needing some ageing mechanism to correct this.\nHowever, can we not think of another way of enforcing priorities that solves the ageing problem in a more elegant fashion? Up until this point, we\u0026rsquo;ve also been assuming that we want to determine a single time slice length to use for all tasks, independent of how they behave. We\u0026rsquo;ve seen that this is suboptimal in several ways in terms of efficiency vs responsiveness.\nSo maybe we can solve both ageing and efficiency by moving away from a fixed-length time slice, to using multiple different time slice lengths, dynamically assigned per priority?\nFor example, high priority jobs could get a longer time slice (say 10ms) to make sure they get to do as much work as possible, while lower priority tasks could get less time (say 2ms per burst). We can then use a simple Round-Robin scheduler between the different tasks, as the priorities are enforced by the time slice length, rather than by strict execution order. Lower priority processes would get time on the CPU more often than with a direct priority-based scheduler, but in shorter bursts, solving ageing while keeping relative priorities intact.\nThis seems like a good idea, but we can again question if this will work well in practice. For example, say the high priority tasks in a system are I/O-bound and the low priority tasks are CPU-bound, the proposed system seems to do the exact opposite of what we want (as I/O-bound tasks don\u0026rsquo;t need long time slices, but batch jobs do).\nWe can see that this line of thinking is an interesting one, but that once again we\u0026rsquo;re not quite there yet with how to practically apply the concept of modelling priorities as time slice lengths.\nA dynamic solution To summarize: at this point it\u0026rsquo;s clear that we have multiple different requirements of a real world scheduler: it needs to be both responsive and CPU efficient, it needs to support both I/O-bound and CPU-bound tasks in a decent way, and it needs to have support for per-task priorities to allow further tweaking of scheduling logic. As hinted to in the last section, one possible approach for dealing with these issues, is to use a dynamic time slice length.\nAs such, a generally proposed solution to these issues is the multi-level feedback queue scheduler. In this setup, we no longer have a single long list of processes, but instead distribute them across multiple, independent \u0026ldquo;run queues\u0026rdquo;. Each of these queues can then employ their own scheduling logic (for example use FCFS or RR or even priority-based) and determine other parameters such as if the queue is processed cooperatively or preemptively (in which case, the time slice length can also vary). That\u0026rsquo;s the \u0026ldquo;multi-level\u0026rdquo; part.\nThe \u0026ldquo;feedback\u0026rdquo; part indicates that tasks can move between these separate queues over time (for example as they become more or less important, as they run for longer or shorter bursts, etc.).\nWe can then see that we also need a sort of top-level scheduler, that determines how the different run queues are processed (for example, queue 2 can only start if queue 1 is empty).\nOne of the first examples of this approach was given by Fernando J. Corbat√≥ et al. in 1962. Their setup has three specific goals:\n Give preference to short jobs. Give preference to I/O-bound tasks. Separate processes into categories based on their need for the processor.  To achieve these goals, they employ three differen run queues:\n  When a newly created process is added to the scheduler, it arrives at the back of the top queue (8ms). When it is scheduled, there are two options: (a) either it runs the full 8ms or (b) it yields before that. In the case of (b), it\u0026rsquo;s likely that we have a short and/or I/O bound task. As such, when it is done waiting, it is appended at the back of the top queue again.\nIn the case of (a) however, it\u0026rsquo;s more likely that we have a CPU-bound task. As such, after the 8ms, it is pre-empted and we move it down to the middle queue (16ms), where it should get a longer time slice next time it is run. We can see this improves efficiency, as we can assume the task will remain CPU-bound and thus we have only half the context switches for these processes!\nIf the processes in the middle queue keep running to their full time slice of 16ms multiple times, this is an indication they are very heavily CPU-bound. In response, we move them down to the bottom queue. Here, processes are run in FCFS fashion until completion.\nFinally, processes can move up to the previous queue if they yield to an I/O operation. This allows for example mostly batch tasks to still get a bit more execution time if they have phases in their programming that requires some I/O work.\nAcross the three different run queues, a simple FCFS logic is applied: the top queue is processed until it is empty and only then are tasks from the middle queue scheduled. Note: if I/O bound tasks are waiting, they are of course no longer in the top queue, otherwise the bottom queues would never get any time! Only tasks ready to execute are in the queues.\nAs said in the previous Section, it is difficult to do Shortest Job First (SJF) scheduling, since it\u0026rsquo;s difficult to know the total duration of a job. This type of setup however tries to approximate this logic by looking not at the total duration of a job, but at the duration of individual \u0026ldquo;bursts\u0026rdquo;. Longer jobs automatically move down to the lower queues, leaving more room for jobs with shorter bursts at the top.\n The setup described above is of course highly specific to those three goals and needs of a particular system. The concepts of the multi-level feedback queue are however much more flexible, as we can also envision other ways of partioning queues to model other advanced scheduling setups. For example:\n Each level can represent a separate priority (doing for example RR within each level gives us the simple Priority-based scheduler from the previous Section) Each level can represent a separate scheduler (the first level can for example do RR, the next FCFS, the next priority-based, etc.) Each level can represent a different time slice length (the first has slices of 8ms, the next 16ms, etc.)  Between the levels, we can then also employ other schedulers than FCFS of course (e.g., a RR scheduler, a priority-based scheduler etc.) to improve the responsiveness of tasks in the lower levels.\nIn practice, these aspects are often combined in specific ways to get a desired outcome (as with the example above). This outcome depends on the system and intended usage. We will see several options for this in the next Section on Linux schedulers. In some way, most modern OS schedulers are variations on the general multi-level feedback queue scheduling concept.\n #### Multiple FIFOs A priority based system might use the exact same scheduling algorithm, with the exception of priorities. A solution could be to use multiple FIFOs: one FIFO for each priority level. **When a process is created**, it is simply added to the back of the queue that matches the process's priority level.   #### Tree When a more complex algorithms are used in the scheduler, a tree might suit the needs better. Depending on the strategy a tree might be ordered in a certain way. For example, in a **shortest-job-first** algorithm, jobs may be ordered (from **short** to **long**) in the tree from **left** to **right**. **When a process is created**, calculations have to be done to determine the position in the three of the new process.   ### Multi-level feedback queue -- Linux Schedulers Now that we\u0026rsquo;ve explored some of the practical issues with real-world scheduling and introduced a basic solution framework, it\u0026rsquo;s time to look at how things are practically done in the Linux OS. This again goes one step beyond the scheduling logic, as we now need to also take into account performance of the implementations and datastructures, as well as the provided API for programmers (for example, how to actually manipulate priorities in practice).\nOver time, the Linux kernel has used different schedulers, of which we will discuss three here. Linux kernels with version 2.4 - 2.6 (before 2003) were using the O(n) scheduler, in 2.6 - 2.6.11 (2003-2007) the used scheduler was O(1), and from 2.6.12 (after 2007) onward the Completely Fair Scheduler (CFS) is mainly used. These schedulers are briefly touched upon here. All of these are preemptive schedulers that incorporate priorities, but as we will see, they do this in various different ways.\nYou might be confused by O(n) and O(1). This \u0026ldquo;Big Oh\u0026rdquo; notation is an often used concept in computer science to indicate the worst-case performance (called \u0026ldquo;time complexity\u0026rdquo;) of a program. In general, the factor inside of the O() function should be as small as possible. As such O(1) is optimal (\u0026ldquo;constant time\u0026rdquo;), while O(n) indicates that in the worst-case, the program scales linearly with (in this case) the amount of tasks (n). Exponential setups like O(n*n) and especially O(2^n) are to be avoided. In practice, O(log N) is often the best you can do.\n O(n) scheduler The O(n) scheduler got his name from the fact that choosing a new task has linear complexity. This is because this scheduler uses a single linked list to store all the tasks. Upon each context switch, the scheduler iterates over all the ready tasks in the list, (re-)calculating what is called a \u0026ldquo;goodness\u0026rdquo; value. This value is a combination of various factors, such as task priority and whether the task fully used its allotted time slice in its previous burst. The task with the highest goodness value is chosen to run next.\n  This setup combines some of the aspects of the multi-level feedback queue concept, but in a single datastructure. For example, if a task didn\u0026rsquo;t use its entire alotted time slice, it gets half of the remaining time alotted for its next run (somewhat bumping its priority, as the alotted timeslice is taken into account with the goodness value as well). As such, while each task is typically assigned the same time slice length initially, this starts to vary over time.\nIn practice, this scheduler works, but it has severe issues. Firstly, it is somewhat unpredictable (e.g., the time slice could grow unbounded for very short processes, meaning we need additional logic to deal with this). Secondly, and most importantly in practice, the performance was too low. Because each task\u0026rsquo;s goodness needs to be caclculated/checked on every context switch (the O(n)), this adds large amounts of overhead if there are many concurrent processes. Thirdly, it also does not scale well to multiple processors: each processor is typically scheduled independently, meaning that for each CPU a mutex lock had to be obtained on the single task list to fetch the next candidate.\nO(1) scheduler Given the problems of the previous O(n) scheduler, a new, much more advanced version was introduced. One of its main goals was to reduce the time it takes to identify the next task to run, which can now be done in constant time, expressed as O(1).\nTo understand how exactly this works, we first need to understand how Linux practically deals with priorities, since the O(1) scheduler is tightly integrated with this.\nLinux defines task priority as a value between 0 and 139 (so a total of 140 different priorities). 0 is the highest priority, 139 the lowest (somewhat unintuitively\u0026hellip;). The range 0-99 is reserved for so-called \u0026ldquo;real time\u0026rdquo; tasks. In practice, these are kernel-level tasks (as the kernel of course also has internal things to do). These also for example include the concrete I/O operations (for example reading from disk), as other (user-space) (I/O-bound) tasks might be waiting for that. The range between 100 and 139 then is reserved for user-space processes, sometimes called time sharing or interactive processes.\nHowever, programmers don\u0026rsquo;t manually assign priorities between 100 and 139. Instead, Linux APIs add an additional abstraction on top called the \u0026ldquo;nice value\u0026rdquo;. These nice values are from the range [-20,19], which maps directly onto the \u0026ldquo;real\u0026rdquo; priorities in [100,139]. When a process is nicer to other processes (a higher nice value), it means it doesn\u0026rsquo;t mind giving some of its time to other processes. As such, a higher nice value means a lower priority (just like a higher priority number also indicates a lower priority conceptually).\nWhat should be the default nice value (or priority) that is given to a user process?\n  The clip below tries to illustrate the effect of the overall priority.   (The code for the examples in the video can be found here).\nThe O(1) scheduler creates a new queue (linked list) for each of these 140 different priority values. For the real-time tasks (queues 0-99), processes within each priority list are scheduled either FCFS or RR, which can be toggled by the user (look for SCHED_FCFS and SCHED_RR). The user-space tasks (100-139) are typically scheduled RR per priority (SCHED_NORMAL) but they can also be scheduled based on remaining runtime to improve batch processing (SCHED_BATCH). Each priority list is emptied in full before the next priority list is considered. At every context switch (at every time slice), the highest priority list with a runnable task is selected.\n  If we were to use this setup directly, the lower-priority tasks would very often by interrupted by higher-priority ones and we again get the problem of starvation. To prevent the need for manual priority adjustment with ageing, the O(1) scheduler instead uses a clever trick, by introducing a second, parallel datastructure. As such, there are two groups of 140 queues. The first is called the \u0026ldquo;active\u0026rdquo; queue, the second the \u0026ldquo;expired\u0026rdquo; queue. When a task has consumed its time slice completely (either in 1 run, or by yielding multiple times), it is moved to the corresponding \u0026ldquo;expired\u0026rdquo; queue. This allows all processes in the active queue to get some time. When the active processes are all done, the expired and active lists are swapped and the scheduler can again start with the highest priority processes.\nThis setup is efficient, because we no longer need to loop through all tasks to find the next one: we just need the first task in the highest priority list! As long as processes are added to the correct priority queue, this can be done in constant time. Some psuedocode to illustrate these aspects can be found below:\n// pseudocode!!! // in reality, the data structures and functions look different!  struct PriorityTask { struct Task *task; // for example the PCB  struct PriorityTask *next; } struct PriorityList { struct PriorityTask *first; struct PriorityTask *last; } struct RunQueue { struct PriorityList *tasks[139]; // 140 linked lists, 1 for each priority } void appendToList(struct PriorityList *list, struct Task *newTask) { // TODO: make sure list-\u0026gt;last exists etc.  list-\u0026gt;last-\u0026gt;next = newTask; list-\u0026gt;last = newTask; } struct RunQueue *active; struct RunQueue *expired; // new task is started with priority x appendToList( active-\u0026gt;tasks[x], newTask ); // scheduler wants to start a new task // loops over \u0026#34;active-\u0026gt;tasks\u0026#34; from 0 to 139, looking for the first non-empty list, with index y // Note: in reality, a bitmap is used to prevent the need to loop (see below), keeping things O(1) struct PriorityTask *runTask = popFirstFromList( active-\u0026gt;tasks[y] ); execute( runTask-\u0026gt;task, runTask-\u0026gt;task.timeslice ) // task is done running and has consumed its timeslice completely appendToList( expired-\u0026gt;tasks[x], runTask ); // OR: task is done running and hasn\u0026#39;t consumed its timeslice yet (waiting state) runTask-\u0026gt;task.timeslice = leftoverTimeslice; appendToList( active-\u0026gt;tasks[x], runTask ); // if all lists in \u0026#34;active\u0026#34; are empty (no \u0026#34;y\u0026#34; found): swap both runqueues and start over struct RunQueue *temp = active; active = expired; expired = temp; How would you implement appendToList and popFirstFromList in practice? What other properties should struct Task have besides \u0026ldquo;timeslice\u0026rdquo;?\n  As explained in the pseudocode, we also need a clever way of knowning which priority queue still has pending tasks without looping over all of them. This can be cleverly done by using a so-called bitmap, where each individual bit of an integer is used as a boolean to indicate if there are tasks for the priority corresponding to that bit. To represent 140 bits, we need about 5 32-bit integers (total of 160 bits). Checking which bits are set can be done very efficiently. See the image below for a schematic representation:\n  As such, we can see the O(1) scheduler is an excellent example of a complex multi-level feedback queue! It utilizes several queues for the different priorities, using different schedulers per-queue depending on the real-timeness of the task. On top, it has two higher-level queues (active and expired) for which it uses an FCFS scheduler. Conceptually a different time slice could also be employed (e.g., higher priorities get a longer time slice), though this was typically not employed.\nThis scheduler is however not perfect. In practice, it turns out that I/O-bound or interactive processes could get delayed considerably by longer-running processes, due to the active vs expired setup. This caused the need for a complex set of heuristics (basically: educated guesses) that the OS would use to estimate which processes were I/O-bound or interactive. These processes would then receive an internal priority boost (again a form of ageing), while non-interactive processes would get penalized. In practice however, like with the O(n) scheduler, this process was somewhat unstable and error-prone.\nThe Completely Fair Scheduler The current default scheduler was intended to take a bit of a step back from the relatively complex O(1) scheduler and to make things a bit simpler; as we\u0026rsquo;ll see however, that\u0026rsquo;s simpler for Linux kernel developers, not necessarily for us. The CFS is a relatively complex scheduler, and as such a thorough study on this algorithm falls out of the scope of this course. We will touch upon the main concepts however.\nThe main insight in the CFS is that the size of the time slices can be highly dynamic. Previously, we\u0026rsquo;ve seen that interactive processes for example can get 8ms, while CPU-bound processes could get 16ms. That\u0026rsquo;s already nice, but it doesn\u0026rsquo;t take into account the current load of the system: if there are many different processes waiting, each will still get 8 to 16ms, causing later ones to be significantly delayed.\nThe CFS solves this problem by calculting the per-task time slice length for a given time period based on the number of ready tasks. Say that N tasks are ready and we want to schedule each of them over the next 100ms (an \u0026ldquo;epoch\u0026rdquo;). Then each task is assigned a time slice of 100ms * 1/N (if we ignore the context switching overhead for a bit). In theory, this gives each task an equal share of the processor, hence the name. As such, if there are fewer tasks active in the system, N will be lower, and the time slices will get larger, and vice versa. Of course, the CFS puts a lower bound on the time slice length (typically 4ms) as otherwise the context switching overhead could become too large.\nTo determine which task executes first within the next epoch, the CFS keeps track of how much time each task has actually spent on the CPU so far. As such, for I/O-bound processes that yield frequently, this value will be lower than for CPU-bound tasks that always use their full time slice. The scheduler always selects the process that has so far spent the least amount of time on the CPU. This automatically makes sure that interactive processes are scheduled frequently enough, but also that CPU-bound processes age correctly.\nThis timekeeping is done in a quite complex datastructure called a (binary) (self-balancing) red-black tree. The details are not important here, but this mainly means that the next task (that has spent the last amount of time on the CPU) is always the most bottom left node in the tree. As such, it can easily be retrieved with low overhead. Similarly, adding new tasks (or moving tasks around) in the tree can be done in O(log N).\n  The complexity increases even more when we look at how this setup incorporates priorities. As there are no longer explicit per-priority lists like in the O(1) scheduler, the CFS simulates this by shrinking/expanding the time slices of low/high priority processes. This is similar to what we\u0026rsquo;ve discussed above, that time slice durations can be used to emulate priorities. As such, if a high priority process executes for 10ms on the CPU, the timekeeper might only record that it spent 5ms of \u0026ldquo;virtual time\u0026rdquo;. This gives the task a \u0026ldquo;priority boost\u0026rdquo; when the scheduler next goes looking for a new task. The opposite is done for low priority tasks (e.g., 10ms of runtime can become 20ms of \u0026ldquo;virtual time\u0026rdquo;). We can see this is no longer a \u0026ldquo;completely fair\u0026rdquo; scheduler in practice, but it\u0026rsquo;s quite elegant in how it combines interactivity, priorities and time slice lengths in practice.\nFor more information on CFS you can read the kernel documentation here. For the daredevils \u0026hellip; you can even read (or modify, at your own risk) the kernel C code here.\nOther schedulers As might be expected, these are not the only schedulers that exist, even within Linux. There a many schedulers available and, certainly, there will be many more to come. Just a small grasp of existing schedulers:\n Brain F*ck Scheduler (Linux) Noop Scheduler (Linux) Task Scheduler 1.0 (Windows) Task Scheduler 2.0 (Windows) JobScheduler (iOS)  Look up at least 1 other scheduler (for example one used in Windows) and grasp its main concepts and compare it to how Linux works.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/scope/",
	"title": "8.3: Different Scopes in C",
	"tags": [],
	"description": "",
	"content": " Scoping issues Now that you have some basic knowledge on the stack and the heap, it is time to take a better look at the different scopes present in the C programming language. These concepts are very important because they in part determine whether variables live (or not) - and whether anything is pushed to the local stack (or not). Let\u0026rsquo;s start with a basic example:\n#include \u0026lt;stdio.h\u0026gt; void* whats_my_age() { int age = 30; // I\u0026#39;m being generous here! } int main() { int* my_age = (int*) whats_my_age(); printf(\u0026#34;%d\\n\u0026#34;, *my_age); } What is the output of the above program? I\u0026rsquo;m sure you know the answer, but\u0026hellip; why?\n  If you forget how void* works, please re-read chapter 3: pointers and arrays.\nLocal variables The local variabel, age, exists as soon as whats_my_age is pushed to the stack. That automatically includes all locally defined variables. When the method is done, after the } sign, things are popped from the stack to make room for future methods and their local variables. This means whats_my_age and age disappear. Forever.\nIf we were to try and change the age variable from inside the main() method (another method, so the variable is not local anymore) by adding age = 5; in the main method, we would get a compile error:\n scoping.c:9:5: error: use of undeclared identifier 'age' age = 5; ^ 1 warning and 1 error generated.  Nothing shocking there. Identifiers are undeclared when they are not present in the program stack.\nAs a side note referenced from here, there is a way to tell C to keep a stack variable around, even after its creator function exits, and that is to use the static keyword when declaring the variable. A variable declared with the static keyword thus becomes something like a global variable, but one that is only visible inside the function that created it. It\u0026rsquo;s a strange construction, one that you probably won\u0026rsquo;t need except under very specific circumstances.\nGlobal variables Once we change the above program by moving age outside of the method scope, into the global scope, we get something like this:\n#include \u0026lt;stdio.h\u0026gt; int age; void whats_my_age() { age = 30; } int main() { whats_my_age(); printf(\u0026#34;%d\\n\u0026#34;, age); } Compile the above program. It correctly prints the age. What is the biggest disadvantage of having a global variable?\n  Can you figure out where global variables resize in the memory of a program? Take another good look at the schematic in 8.1: program memory. It lives outside of the stack, and also outside of the heap, in a separate block called \u0026ldquo;data\u0026rdquo;. Note that we are not employing any kind of pointer system. Thus, we are not calling upon the heap to transfer data from one method to the other.\nWhen should I use global variables? The answer is never, if possible. In practice, in an iterative programming language such as C, that is very difficult to achieve. In essence, function and struct declarations are all part of the global scope. Methods can be accessed from any other method (provided you used forward declarations), because the method name itself is declared globally, instead of locally.\nConsider the following example in Javascript:\nfunction someMethod() { int age = 30; function someMethodInAMethod() { age = 5; // will work: the variable is part of this (closed) scope  } someMethodInAMethod(); } function otherMethod() { age = 5; // will not work: age is accessible in someMethod only  otherMethod(); // will work and cause an infinite loop  someMethodInAMethod(); // will not work: same reason as age }  Calling functions within functions is only possible in C in the form of function pointers leveraging the power of void*, but there is no change in the functionality of the scopes. In C, everything in top level is global scope. Top level is declared as the lowermost level, the body of your source file where all delcarations are put. main() is globally scoped at top level. In the above JS example, so is someMethod, but not someMethodInAMethod: that method is not part of the global scope.\nIn practice, most C programs are constructed as a sequence of method calls, all residing in the global scope. You should try to avoid using global variables as any method can change the value of that variable, causing all kinds of wreckage. If you need to pass something, use return. If you need to pass multiple things, use a struct, or refactor it!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/paging/",
	"title": "9.3: Paging",
	"tags": [],
	"description": "",
	"content": " A memory management technique that is used both in Linux and all other main operating systems is paging. The core of the concept is similar to segmentation: The main memory is divided into pieces. These pieces are mapped to the hardware and give the programmer the illusion of having a continuous memory space. However, internally these pieces might be spread all over the hardware.\nWith segmentation there might be the following issue: we need to allocate a segment of 1024 bytes. There are still 2000 open spaces, but none of them is larger than 1000 bytes. When this happens \u0026hellip; we have a problem.\n  The pieces of memory with segmentation (left) and paging (right)   With paging, this issue can be overcome. When paging is used, the memory is divided into equally sized pieces (see image above). As the tools are aware of this, the issue of having a too-large-a piece will not occur any more. Another benefit is that finding a new location for a piece becomes easier. The puzzling has become much easier.\nIn contrast with segmentation, paging does not result in external fragmentation. Any frame can be assigned to any process that needs one. However, internal fragmentation is to be expected.\nThis fragmentation in the memory space happens both in the logical address space and in the physical address space. Off course the size of the pieces is the same in both. These newly created pieces have different names, however. A piece in the logical address space is called a page, whereas a piece in the physical address space is called a frame.\n  The fragmentation of the logical and physical address space with paging   For paging to work, hardware assistance is required. Every address that is generated in the logical address space is again secretly split in two pieces. The first piece is the page number (p) and the second piece is the page offset (d). Similar to the segment table earlier, there exists a page table for every process. This page table stores the base address of each page in the physical memory. The page offset is than added to the page base address to finally have a physical memory address. The size of the logical address space is defined as 2m, and a page size as 2n bytes. The m-n left-most bits of the logical address define the page number, and the n remaining bits form the page offset.\nAnswer this question before you continue. If you can\u0026rsquo;t answer the question, find the answer in the text above!How many active page tables are there in a single system ?\n       Paging hardware    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\n  Both the page size and the frame size are defined by the supporting hardware. As hardware is involved this (typically) dictates that the sizes are powers of 2. The page size could theoretically vary between 512 bytes and 1 GB per page. On a 32-bit processor each page-table entry is typically 4 bytes long.\nWhile the idea of paging is relatively simple, the effects however are not. With a strict decoupling between the logical an physical address spaces, one does not put any restrictions on the other. A 64-bit processor, capable of addressing 264 (= 1.8 x 1019) different memory locations, can run on a physical memory with less than 264 bytes. (We\u0026rsquo;re not guaranteeing it\u0026rsquo;ll run smoothly, though üòÑ).\nLet\u0026rsquo;s do some numbers In a hypothetical 4-bit system the processor can address 24 different addresses. Each logical address has a length of 4 bits. The made-up environment has a page size of 4 bytes, and has 32 bytes of physical memory available. The page table for a certain program would be defined as: {5, 6, 1, 2}. With the definitions as declared above, this comes down to:\n m = 4 n = 2 physical memory is 8 frames page table: {5, 6, 1, 2}  The logical address 0x0 (0b0000) can be seen as a concatenation of 0b00 and 0b00. This means that the logical address refers to the content on page 0 and offset 0.\nThe page table tells us that page 0 is mapped to frame 5. Because frames and pages have an equal size, frame 5 also contains 4 bytes. Because the offset is 0, the physical address on which the logical address 0x0 is mapped is 0x14.\nThis can be calculated as follows: a frame is 4 bytes, and we\u0026rsquo;re looking for frame number 5, this gives us 5 x 4 = 20 = 0x14. Finally the offset has to be added. For logical address 0x0, this is 0. The final physical address hence is 0x14 + 0x0 = 0x14.\nLogical address 0x3 (0b0011) has the same page (page 0), but has an offset of 3. The physical address to which logical address 0x3 is mapped, hence is 0x17.\nGiven the hypothetical system as described above, what would be the corresponding physical address for the logical addresses 4, and 15?   Answer:  Logical address 4 (page 1, offset 0) maps to physical address 24 [= (6 √ó 4) + 0] = 0x18 \nLogical address 15 (page 3, offset 3) maps to physical address 11 [= (2 √ó 4) + 3] = 0x0B \n  Hierarchy Take an off-the-shelf 32-bit processor that can address 232 different locations with a page size of 4 kB (=212). If we assume that an entry in the page table has 4 bytes we can calculate the size of the page table:\n the page table on this system has 232 / 212 = 220 entries each entry is 4 = 22 bytes the total size hence is 220 * 22 = 222 bytes 222 bytes = 212 kB = 22 MB = 4 MB  Having to store a 4 MB page table for every process is a bit much. Too much!! When the scheduler performs a task switch, it has to copy this, remember ?\nThere exist multiple techniques to solve this issue. One solution will be discussed here, as discussing them all would take us too far.\n  A two-level page-table scheme    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\n One way to overcome large page tables is to use a two-level paging algorithm. This technique uses paging for page tables. The page number in the example above is 20 bits. Using the same technique again, the page number gets split into two 10-bit addresses.\n  With this setting, p1 is the index into the outer page. Similarly, p2 is the index into the inner page. When the physical address is to be searched from a logical address, first the outer tables needs to be examined using p1. With the inner table found, the base address could searched for using p2. Finally the page offset is added to the base address to end up with the mapped physical address.\nThe translation from logical to physical address happens from the outer page, inward. Therefore this scheme is known as a forward-mapped page table.\n  Throw in some more bits Paging gives us a nice page table. This could be considered as a table of contents, or the index at the back of a book. For our young(er than us) students, the following clarification:\n   A book. Source: G.I.   A book is a collection of e-readers, held together with rope or glue.\n   The index of a book   A page table for finding content in a book.\n  Additional information could be stored in the page table. One common bit of meta-data that is stored is a protection bit. Depending on whether the bit is set, the page can be read-only or read-write. Another bit that is added is the valid bit. From the name it should be clear that a valid bit shows whether the associated page is valid or not. A reason for setting resetting this bit is that the process is not using all the entries in the page table. Entries that are unused have a valid bit that is set to 0. Although more bits could be or are available, the last one touched here is the modified bit. This bit indicates that its associated block of memory has been modified and has not been saved to storage yet. It is also often referred to as the dirty bit.\nHave you ever wondered why you should eject a USB stick ? One of the reasons is the Dirty bit. It might happen that you have written data to the USB stick, by writing to its physical memory addresses, but the data has not reached its destination yet. Off course, cache memory also play a role. It has a similar dirty bit.\n In summary So, using pages:\n provides separation between the logical and physical address spaces requires specialised hardware aids in memory protection lessens the fragmentation and puzzling efforts  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/devdrivers/",
	"title": "X.3: Device Drivers",
	"tags": [],
	"description": "",
	"content": " General The kernel of an operating system has a lot of different tasks. Some of them we visited in earlier chapters. One task of the OS is the management of the different hardware devices that are typically connected to a processor in a laptop, desktop, or server. The OS has to know how to talk to a certain device. Typically an Application Programming Interface (an API) is provided to the user space, so users can interact with the hardware.\nLet\u0026rsquo;s take the timer from chapter 2 as an example. Enabling the interrupt for when Timer1 overflows was done by writing a \u0026lsquo;1\u0026rsquo; to the LSB of address 0x6F. Although this is no rocket science, it is not realistic to assume that the user simply knows this. The manufacturer of the hardware, the timer in this example, is the one that knows and should make this knowledge available to the user. This is typically done through documentation, or through a driver.\n... #define TIMSK1 _SFR_MEM8(0x6F)  ... TIMSK1 |= 0x1; ...  The user could find out what is to be done to enable the interrupt from the documentation. When that is figured out, the user has to program/use the functionality correctly.\n  This needs no additional software  This requires a \"skilled\" user   ... #include \u0026#34;timer.h\u0026#34; ... enable_interrupt_timer1_overflow(); ...  The user could find out what is to be done and execute the dedicated function.\n  This requires a \"normal\" user  This needs additional software    The argument of needing additional software, e.g. the driver, is no longer a concern. The additional kilobyte of \u0026ldquo;firmware\u0026rdquo; can easily be stored. An additional benefit from using a driver is that it provides a more flexible solution. For example, version 2.0 from our timer has this bit moved to another position in the register. The \u0026ldquo;skilled\u0026rdquo; user that uses the timer, has to update his/her source code. With the solution of a driver, the vendor simply ships an updated driver with the hardware and no modifications needs to be made by the user. Scalability is improved!\nWhere do they live ? The driver is a piece of software that allows an operating system to interact with a certain hardware component. In the first lecture on OSes, we talked about the user-space and kernel-space. So, where do device drivers live ? Take a guess \u0026hellip; I\u0026rsquo;m sure you\u0026rsquo;ll be correct.\nThe correct answer is: both üòÑ The driver can live in user-space and in kernel-space. Depending on the space, there is a benefit and drawback. Drivers in user-space will not crash the entire system is an error occurs in the driver itself. This provides improved stability. Drivers in kernel-space will run with much higher priority than user-space processes ever can. This provides improved performance.\nCharacter device drivers Character devices are the simplest types of devices to communicate with, on a Linux systems. The textbook example for this is a serial port. The serial port is the predecessor of the Universal Serial Bus (USB). Students might have been in contact with the serial port when using the RS232 protocol. This is how the UART communicates. Today, it is hard to find the original connector on a modern laptop/desktop, as they are all replaced with a USB-alternative.\n  The original serial port cable      Different types of USB connectors     As was stated before in the course, everything is a file on a Linux operating system. The serial port, which is driven by a character device driver, is represented by a file in /dev\n  The representation of a serial port on Linux system   The first character of permission modes is a c. We have seen that a d represents a directory. The c here shows that the device is a character device. A character device is a device that works with one character as a basic unit. The user can read one character at a time, or write one character at a time. As state above, the serial port is an excellent example for this.\nBlock device drivers Devices that support filesystems are referred to as block devices. The drivers, not surprisingly: block device drivers. The typical examples for these are hard drives, solid-state drives, and USB flash memories.\nSimilar to the letter c with the character devices in the permissions, the block devices show the letter b.\n  The representation of a solid-state drive on Linux system   Writing your own When you develop new hardware, or got in a situation where no drivers are around for existing hardware, you might decide it is time to write your own device driver.\nThis type of programming is very different from general application programming as you know it. If you make mistakes with general programming, you get a fault and that\u0026rsquo;s that. Maybe you need to reset the terminal, but that\u0026rsquo;s (roughly speaking) the worst that can happen.\nWhen you write bugs in your device drivers, it is a different story. Memory leaks in the kernel might crash your system periodically. Incorrectly handled exceptions lead to system failures and maybe even hardware failures.\nKernel modules Earlier we have seen that the kernel is not one impenetrable binary (monolithic). The kernel can be expanded with kernel modules. This is a small piece of code that can be added to the kernel, while the kernel is running. A number of kernel modules are present in the Virtual Machine image. You can list the kernel modules with the command: lsmod.\n  Result of lsmod in the Virtual Machine   Modules can be loaded and unloaded with insmod and rmmod, respectively.\nDriver While developing your custom driver, it is recommended you start of with a kernel module. After compiling, the kernel module can be inserted into the kernel (and hopefully, don\u0026rsquo;t crash it).\nBecause the driver is a kernel module it also has no main() function. \u0026ldquo;It\u0026rsquo;s just\u0026rdquo; a collection of functions that are attached to hooks of the kernel.\nint register_blkdev(unsigned int major, const char *name); For example: the hook above is a standard function that kernel provides to allow a driver for a block device to register itself with the kernel.\nDisplaying messages on the standard output device is also not possible. Beginning kernel hackers use the printk function:\nprintk(KERN_DEBUG \u0026#34;Debug message shown!\\n\u0026#34;); These printed message do not end up on the device\u0026rsquo;s monitor, but in the kernel ring buffer. A ring buffer is a buffer with a fixed amount of size. When overflowing, the oldest messages get deleted.\nThe ring buffer can be consulted using the dmesg command. This prints the current content of the ring buffer to the screen.\n  Example of dmesg output      Example of dmesg, after inserting a USB drive     This is also a good place to visit when you are trying to solve system problems !!!!\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/structs-labs/",
	"title": "1.4: Structs",
	"tags": [],
	"description": "",
	"content": " We are (still\u0026hellip;) at War! We\u0026rsquo;re still at war! The orcs are attacking and are looking very hungry! Look at them!\n  Orcs attacking! source: G.I.   2. Modelling the orcs Let us try to model these filthy beasts. Ask the user for a number. That number is the amount of orcs in the army. Create as many struct instances with random property values and print these on the screen. An orc has the following properties (both simple numbers, between 1 and 10, use rand() from stdlib):\n attack life   INPUT: 3 OUTPUT: orc 1: attack 3, life 5. orc 2: attack 5, life 6. orc 3: attack 1, life 1.  Tips:\n Do not forget to generate a new seed for the random value, see the rand() docs. Creating a separate function generate_orcs() will keep your main() function short and clean. The function will return a list of orcs, the \u0026ldquo;army\u0026rdquo;, so to speak. Remember that returning an array is of type Orc*.  The generate method will look like this:\nOrc* generate_orcs(int amount) { Orc* army = malloc(sizeof(Orc) * amount); // add stuff to army  return army; } Details on how the malloc() function works will be explained later.\n3. Orcs eating each other?? Vowels did not seem to fully satisfy them, now they are turning on each other!? All the better for us. Expand the program such that the first orc fights the next one. (life minus attack). Create a function Orc fight(Orc attacker, Orc defender). Is the defender still alive after the attack? Then he is victorious (and will be returned). Print the last man stending. Input stays the same.\n INPUT: 3 OUTPUT: orc 1: attack 3, life 5. orc 2: attack 5, life 6. orc 3: attack 1, life 1. orc 1 VS 2: 2 wins (6 - 3 = 3 life left) orc 2 VS 3: 2 wins (1 - 5 = dead) orc 2 is victorious!  Tips:\n You will need to loop through all orcs and take two elements out of the array to pit them against each other. Reassign Orc winner = army[0] with the result of the fight() function, within the loop. If both orcs survive, the first one wins.  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-interrupts/lab2_rocketlaunch/",
	"title": "2.4: Launch that rocket !!",
	"tags": [],
	"description": "",
	"content": "   image source: businessinsider.com   First task The first task that the Arduino should do for us is oscillate a LED at 1 Hz. Tip:  \nMake one LED oscillate using the \"Timer/Counter1 Overflow Interrupt\"  one LED oscillates at 1 Hz    Multiple tasks Let\u0026rsquo;s assume that one task is the original blinking LED which oscillates at 1 Hz. Now we want a second application which oscillates 3 times every second.\nMake two LEDs oscillate at different frequencies using the \"Timer/Counter1 Overflow Interrupt\"  one LED oscillates at 1 Hz one LED oscillates at 3 Hz    Start the countdown in \u0026hellip; 3 \u0026hellip; 2 \u0026hellip; 1 \u0026hellip; The bare-metal \\(rocket\\) on the top of this page needs to be launched. Because of the huge amount of exhaust it should be triggered after a countdown. This gives the poor operator sufficient time to get to a safe distance.\nAfter the launch is initiated through a serial port command (eg. 'L'), a countdown is started. The countdown starts at 10 and goes to 0. Once the countdown is ongoing, the 1 Hz LED should flash. In the final 3 seconds, the second 3 Hz LED should join the show. When the countdown reaches 0, both LEDs should stay on.   Pull-up resistor: https://www.youtube.com/watch?v=wxjerCHCEMg\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-pointers/practical-use/",
	"title": "3.2: Practical Use of Pointers",
	"tags": [],
	"description": "",
	"content": " 1. Changing values around Because in C everything passed by-value, we can manipulate the values of variables in a function that has been declared outside with pointers. In Java you can also change the value of member variables in objects, but not primitives! How do you switch two numbers without giving anything back?\n#include \u0026lt;stdio.h\u0026gt;void swap(int *px, int *py) { int temp; temp = *px; *px = *py; *py = temp; } int main() { int x = 10, y = 20; swap(\u0026amp;x, \u0026amp;y); printf(\u0026#34;(%d, %d)\\n\u0026#34;, x, y); // print (20, 10) } mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; x[x10] y[y20] px{px} py{py} temp[temp] px --|\u0026x, address-of stack-var x| x py --|\u0026y, address-of stack-var y| y temp -.-|*px, follow pointer for the value| px  Something like that is unthinkable in Java - we need extra tricks for that, such as passing objects. Of course this implementation is also disadvantageous: is it clear to the caller that variables are being changed? Not really. High-performance algorithm implementations benefit from these possibilities. Clear domain-driven applications are not: a higher language is used for that.\n2. Not chaining values around: const To prevent further confusion, it is possible to mark pointers with the const keyword, meaning the value should not be changed. The pointer can still point to another value! As such, this is by no means a \u0026ldquo;constant\u0026rdquo;, like in many other traditional programming languages. Take the above example, and change swap\u0026rsquo;s signature to void swap(const int *px, const int *py). While compiling the code, the following errors are generated:\n test.c:5:9: error: read-only variable is not assignable *px = *py; ~~~ ^ test.c:6:9: error: read-only variable is not assignable *py = temp; ~~~ ^ 2 errors generated.  With the const keyword, we prohibit programmers from using *ptr = ... - that is, assigning another value as a dereferenced pointer. ptr = \u0026amp;temp is still possible, however. If you do not want pointers to change addresses, use const int* const px. That\u0026rsquo;s right, two times const - this is not a mistake. This reads, from right to left, as:\n px is a constant pointer to an int constant  Introducing the second const gives the following error when attempting to change the pointer itself:\n test.c:5:8: error: read-only variable is not assignable px = \u0026temp; ~~ ^ 1 error generated.  In practice, try to use as many constant variables as possible, if you want to make sure the passed values stay the same.\n3. Arithmetics with pointers Pointers and arrays go hand-in-hand in C. Pointers can be moved around by adding and subtracting. On pointers you can also perform operations such as ++ and -- that move the pointer in the memory one place to the left or right. With char * text = \u0026quot;sup\u0026quot; the pointer refers to the first character:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A{*text} A--|begin van array|C['s'] A-.-D['u'] A-.-E['p'] A-.-F['\\0']  Creating an own version of printf(\u0026quot;%s\u0026quot;, tekst) could be implemented by looping through the pointer until nothing is left to print, like this:\nvoid print_text(char *txt) { while(*txt != \u0026#39;\\0\u0026#39;) { printf(\u0026#34;%c\u0026#34;, *txt); txt++; } } Notice txt++. We simply point to the next possible value in the memory space, which hopefully is still a character. If it is not, and it came from a string, it will be ended with \\0. Adding some value beyond the limit will result in calling upon unintended memory values, resulting in possible glitches. But C will not crash, it is very robust. You should pay extra attention while fiddling about with pointers! For instance:\nIn C, a[i] exactly the same as *(a + i)!\n #include \u0026lt;stdio.h\u0026gt;int main() { char txt[4] = \u0026#34;hey\u0026#34;; char* ptr = txt; char otherstuff[10] = \u0026#34;other\u0026#34;; for(int i = 0; i \u0026lt; 5; i++) { printf(\u0026#34;%c\u0026#34;, *ptr); ptr++; } } Depending on your compiler, the above code will print \u0026ldquo;hey ot\u0026rdquo;, meaning your ptr pointer is pointing to the next variable on the local stack after the four characters \u0026ldquo;h\u0026rdquo;, \u0026ldquo;e\u0026rdquo;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;\\0\u0026rdquo;, from the txt variable, are processed within the for loop. We will go more into detail on this in chapter 4.\nWhat happens when I change txt[4] to txt[3]?\n  Jumping to the next available address space also works with structures instead of a character array:\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; typedef struct Doos { int one; int two; } Doos; int main() { Doos doos1 = { 1, 2 }; Doos doos2 = { 3, 4 }; Doos* doosje = malloc(sizeof(Doos) * 2); doosje[0] = doos1; doosje[1] = doos2; for(int i = 0; i \u0026lt; 2; i++) { printf(\u0026#34;doos: one %d two %d\\n\u0026#34;, doosje-\u0026gt;one, doosje-\u0026gt;two); doosje++; } return 0; } The only problem here is that we cannot loop \u0026ldquo;until the end\u0026rdquo; using while(*doosje) { ... }. For that to work, we need linked lists (see lab 2).\nIn C, the new keyword does not exist. Creating instances is done using malloc() instead.\n 4. Linked Lists You will discover in exercise \u0026lsquo;the ancient library\u0026rsquo;, you can link different struct instnaces together to create a list of items, instead of simply using Arrays or other existing data structures. Internally in the C libraries, pointers are applied to connect elements of a collection. Let us try to do the same.\nThis is how the memory structure of your C code looks like without initializing any single variable:\n \nGiven the following structure:\nstruct node { char* name; char* value; struct node* next; }; When instantiating a node element using malloc(), we create a new variable on the heap instead of the stack:\n \nHowever, this representation is incomplete! We create a new local variable, a pointer, and this pointer is actually also a variable on the stack. So, this code:\nvoid create_node() { node* newelement = malloc(sizeof(node)); newelement-\u0026gt;name = \u0026#34;something\u0026#34;; newelement-\u0026gt;value = \u0026#34;value\u0026#34;; newelement-\u0026gt;next = NULL; } Reserves some space on the heap, but also creates a local variable named newelement on the stack:\n \nThe next value is pointing to NULL (\u0026ldquo;nothing\u0026rdquo;), hence the white arrow in the right side of the Figure. Now, what if I want to create a second element, and connect both together? A second local variable reserves a second block on both the stack and the heap:\n \nNow, we want to assign the second element to the next property of the first element. That\u0026rsquo;s very simple with the statement newelement-\u0026gt;next = newelement2;. Now, our memory looks like this:\n \nNotice the changed arrow in the right side of the Figure. Ok, what if we want to loop over all elements by following the arrows from each element, starting with the first, what happens with our variables in the memory space? A new method creates a new chunk in the stack space, where a new variable will be created:\nvoid print(node* printer) { while(printer != NULL) { printf(\u0026#34;%s \\n\u0026#34;, printer-\u0026gt;value); printer = printer-\u0026gt;next; } } void main() { node* head = create_node(); node* tail = create_node(); head-\u0026gt;next = tail; print(head); }  \nIn the above Figure, printer points to the first value in the heap, which is the same as the variable head. When the while() loop starts doing it\u0026rsquo;s work, the variable will point to the next value, and the next, and the next, until it points to NULL.\nThis will become clear in the exercise when you will implement these concepts yourself.\nWhen changing the value of printer, the value of head stays pointing to the first element of the heap! This is because the variable is a copy on the stack.\n What if we want to change the pointer of head using another variable? Then you will need double pointers, or node**:\nvoid change_ptr(node** ptr_to_ptr, node* new_value) { *ptr_to_ptr = new_value; }"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/scheduler/",
	"title": "4. Pseudo scheduler",
	"tags": [],
	"description": "",
	"content": "   image source: youtube.com   Scheduler In this assignment you\u0026rsquo;re going to build a (pseudo) scheduler. A number of tasks will be provided. The scheduler follows the next rules to schedule the presented tasks:\n The scheduler is preemtive and uses a round-robin approach without priorities. The (virtual) time slice is 1 second. If a new job is announcing itself, it gets a time slice right away (last come, first served)  Circular Linked List As should be clear from the theory, the (pseudo) scheduler will need to store the incoming. To be consistent with this course and Linux, the run queue is to be implemented using a linked list.\nThe struct that is to hold the tasks is predefined:\n// typedefs typedef struct ST_PCB { int arrival_time; char name[9]; int duration; struct ST_PCB * next; } T_PCB; However in stead of making a linear linked list, as we did earlier; This time you should aim for a circular linked list. This is nothing more than a normal linked list where the final element\u0026rsquo;s next field points back to the head of the list.\n  With this run queue all the tasks are on a merry-go-round (see image above) üòÉ\nInput/output Both the input and the output are in the form of files.\nINPUT 3 1 T0000003 6 2 T0000002 3 5 T0000001 4   OUTPUT 00 - idle 01 - T0000003 (new) 02 - T0000002 (new) 03 - T0000003 04 - T0000002 05 - T0000001 (new) 06 - T0000003 07 - T0000002 08 - T0000001 09 - T0000003 10 - T0000001 11 - T0000003 12 - T0000001 13 - T0000003 14 - idle 15 - idle ...    The input is formatted according to these rules:  The first line shows the number of tasks that are in the input file For each task there are three consecutive lines:  The arrival time (in seconds) Name of the task (exactly 8 characters wide) The duration of the task (in seconds)     The output is formatted according to these rules:  There is a line for every time slice If a task is scheduled it's name is mentioned If a new task is scheduled it is indicated with the (new) If there is no task to scheduled idle is reported    Some boiler plating A starting file is provided here. This file provides:\n the definition of the struct two functions to show linked lists void show_tasks_lin(T_PCB * head); void show_tasks_circ(T_PCB * head);\n two function that should be completed T_PCB * read_tasks(void); T_PCB * sort_tasks_on_arrival(T_PCB * head);\n a main functions that can only be modified between these lines /* MODIFY BELOW HERE --------------------------------------------------------*/ /* MODIFY ABOVE HERE --------------------------------------------------------*/\n  The complete boilerplate code is added to the osc-exercises repository.\n Assignment This assignment requires you to:\n complete the bodies of the two missing functions modify the main function BETWEEN THE INDICATED LINES so the correct output is produced given a certain input  The resulting C-file is to be uploaded to Toledo.\nJust, for the record:  Don\u0026rsquo;t create memory leaks !! Close files that are opened !! Your solution will be verified with multiple, different input files !!\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-debugging/",
	"title": "4: Debugging",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Debugging in C Important concepts to grasp:\n Breakpoints, stepping into/over, continuing Inspecting the stack and the heap Disassembling, objdumping  Recommended Reading  The GNU Project Debugger Documentation Hackme: exploiting heap bugs Google Test Primer  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/threads/",
	"title": "6.4: Threads",
	"tags": [],
	"description": "",
	"content": " In the previous three sections we have discussed that you can run multiple processes at the same time and how this is managed by the OS. This can be useful in two ways: either a) to use multiple programs at the same time (e.g., your Web browser and music player), or b) to make efficient use of multiple hardware CPUs or processing cores.\nWe have however also seen that this is not trivial: new processes need their own memory space and associated state (the PCB) and communication between processes is either flexible but complex (shared memory) or straightforward but limited (message passing).\nLuckily, there is another way to split up processes into smaller subtasks and make more efficient use of multicore hardware without incurring as much overhead. This is accomplished using Threads.\nA thread is an independent unit of execution within a process. Put differently, it\u0026rsquo;s a sequence of code instructions that can be executed independently from and, crucially, in parallel with other threads. As such, each thread can for example be assigned to a single CPU core for parallel processing.\nWhat\u0026rsquo;s in a thread ? Up until now all the processes that were discussed contained just one thread (sometimes called the main thread). Such a process is referred to as a single-threaded process:\n  The single threaded process   Let\u0026rsquo;s examine the image above. On the top of this image the following segments are mentioned: code, heap, and (open) files, alongside the registers, program counter and the stack.\nAs we\u0026rsquo;ve seen, when multiple processes run at the same time, every individual process has separate and isolated instances of all these segments, like shown in the image below:\n  Two single threaded processes running in parallel   However, the name \u0026ldquo;single-threaded\u0026rdquo; of course also implies the existence of multi-threaded processes, which have more than one thread. Each of these threads can execute in parallel and as such also need their own copies of some of the segments: the register contents, current program counter and the full stack are now independent for each thread. You could say that each thread has its own \u0026ldquo;Thread Control Block\u0026rdquo; (in analogy with the PCB), which tracks this metadata. However, the main difference between parallel threads and parallel processes is that threads do still share many things: the copy of the code, the (open) files and the heap memory. This is one reason why threads are more lightweight than processes, as they don\u0026rsquo;t require copying these segments when new threads are created.\n  A single-threaded vs multi-threaded process    div.twocolumns { display: flex; flex-direction: row; justify-content: space-around; } div.column { width: 35%; } div.column ul { margin-top: 0px; }  The multiple threads in a multi-threaded process share:  the code segment the heap memory the list of open files   The multiple threads in a multi-threaded process have unique/independent:  register contents program counters stack memory    Why would we want multiple threads ? There are 4 major benefits to working with multiple threads:\n Responsiveness: When a single program is broken down into multiple threads, the user experience feels more responsive. Dedicated threads can be created to handle user requests and give (visual) feedback, while other threads can for example process data in the background. Resource sharing: Programs that have multiple threads typically want some sort of communication between these threads. This is done more easily between threads than between processes, as threads implicitly share memory via their heap. Economy: Context switching becomes cheaper when switching between threads in comparison to switching between processes. This is because less per-thread state needs to be tracked, in comparison to more per-process state. Scalability: Multiple threads can run in parallel on multi-core systems in contrast to a single threaded process.  Although there are many advantages to multi-threaded programming, it requires skilled programming to cash in on these opportunities. Not all code can be easily parallelized and communication between threads is not trivial. We will focus on these aspects in the remainder of this chapter.\n Amdahl\u0026rsquo;s law While threading seems like the ideal, lightweight solution to make your programs run faster, it might be surprising to hear that program performance typically does not scale linearly with the amount of threads and/or amount of CPU cores. This is because typically not all code in a process can be parallelized (and thus run in separate threads): there are typically parts of code that need to aggregate results from the parallel computations, which can only be done in a serial fashion. This can be seen in the following image:\n  Parallel processing can lead to serial phases in a process. Source: J. Wolf et al. - Contribution to the Fluid-Structure Interaction Analysis of Ultra-Lightweight Structures using an Embedded Approach   A theoretical model for assessing the maximal gains from multithreading a program was developed by Gene Amdahl in 1967. His formula identifies potential performance gains from adding additional computing cores to an application that has both serial and parallel components:\n\\( speedup In this formula S stands for the portion of the application that has to be run serially. N stands for the number of cores on which the parallel portion is spread. We can see that to maximize the speedup, we need to keep the divisor of the fraction as small as possible. This is done by keeping S as small as possible, and N as high as possible.\n  Source: Wikipedia   The graph above visualises Amdahl\u0026rsquo;s law. For example, as marked by the red dot, if a program has 80% of its code that can be run in parallel (and so 20% of the code has to be run serially), it can be run at maximum 2.5 times faster, using 4 cores. If only 50% of the code can be run in parallel, the amount of cores matters much less, with a maximum speedup of 2x being possible even with 16 cores.\nHowever, as stated above, it requires a skilled programmer to achieve maximal speed-up even if a large part of the program can be parallelized. If the program needs a lot of data that needs to be communicated between the serial and parallel portions this becomes even harder to achieve, as this can also cause additional slowdowns.\nUser threads vs system threads By now, you might still think that using threads is only useful if you have multiple processors, or, if you do have multiple CPU cores, that you should only use as many threads as you have cores. However, this is somewhat incorrect.\nWhile it is true that you might not get a speedup if the number of threads is larger than the number of cores, that does not mean you cannot run tasks in a seemingly parallel fashion. Take for example the simplest case where you have two threads and just a single CPU core. Like with the processes, this does not mean that thread 1 will run fully before thread 2 can start. Instead, the OS will again schedule the threads, pausing one and (re-)starting the other several times in quick succession.\nAs processor speeds are many times what the typical human would notice, this often gives the illusion of parallel execution. A good example is the \u0026ldquo;Responsiveness\u0026rdquo; benefit mentioned above, achieved by using a separate thread for User input/User interface updates. Even if the program has a thread executing heavy calculations, the UI thread will get enough CPU time to listen to User input.\nAs such, there is often not a direct one-to-one mapping between threads and actual processor cores. Internally, the OS has a concept of Kernel threads that it schedules and divides between the processors. The threads we create, sometimes called User threads can be mapped onto those kernel threads in several different ways (e.g., one-to-one, one-to-many, many-to-many) and this mapping can also change over time (e.g., transferring a thread to a different CPU). The exact details are out of scope for this course, but it\u0026rsquo;s useful to know that you typically won\u0026rsquo;t control directly how your threads run. In a multi-process setup (which is typical for most OSes) you typically can\u0026rsquo;t even guarantee that all the threads for a given process are really all running in parallel, as other threads from other processes might also require CPU time. This makes it even more difficult in practice to measure and ensure the speedup from using multiple threads.\nCreating threads Before discussing how to communicate between threads however, we first look at how we can create threads in C.\nIn the previous sections on processes, multiple processes could be created through the fork and exec functions. These function wrap the OS system-calls that are required to achieve this. Therefore, this comes intrinsically with the OS.\nThreads are of course also fully supported by the OS, but they typically require a more high-level API to easily work with. Most programming languages provide their own versions of these APIs, but they all internally call the OS-provided functionality. As we are working with Linux, we will be using the standard POSIX API library called Pthreads.\nPthreads An example for creating a new pthread is given below.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; int counter; void* doSomeThing() { unsigned long i = 0; counter += 1; printf(\u0026#34; Job %d started\\n\u0026#34;, counter); for(i=0; i\u0026lt;(0xFFFFFFFF); i++) { // do nothing  } printf(\u0026#34; Job %d finished\\n\u0026#34;, counter); return NULL; } int main(void) { int i = 0, err; pthread_t tid[2]; counter = 0; while(i \u0026lt; 2) { // 1. pointer to pthread_t to keep thread state, 2. configuration arguments for the thread (we use the defaults here)  // 3. pointer to function that runs in separate thread, 4. parameters to pass to the thread (empty for doSomeThing)  err = pthread_create(\u0026amp;(tid[i]), NULL, \u0026amp;doSomeThing, NULL); if (err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create thread :[%s]\u0026#34;, strerror(err)); } i++; } // Pauze the main thread until the thread in the first argument is terminated  // If the thread was already terminated, pthread_join continues immediately  // The second argument can be used to store the return value of the thread  pthread_join(tid[0], NULL); pthread_join(tid[1], NULL); return 0; } When the code above gets compiled, the pthread library has to be used. This library contains the object files that implemented the functions pthread_create() and pthread_join(). Compilation can be done because of the #include \u0026lt;pthread.h\u0026gt; line. For linking however, you have to separately tell gcc to use the pthread library as well by adding -lpthread (e.g., gcc -o program program.c -lpthread).\n   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/lab2_niceguy/",
	"title": "7.4: Mr. nice guy(lab)",
	"tags": [],
	"description": "",
	"content": "  image source: amazon.com    Write a program in C that fetches its own PID and priority. Display the PID, the nice value and the priority.  hint: use getpriority() in #include \u0026lt;sys/resource.h\u0026gt; and use PRIO_PROCESS     An example output    Adjust the program above (after copying it, off course) so it sleeps for 5 seconds after those operations. Repeat that process for 100 times. Compile and run that program. While that program is running, and is reporting back every 5 seconds:  change the nice value (on another terminal. Search on google for the command you need for this.) verify in the reporting that it works change the nice value so the overall priority is at its lowest change the nice value so the overall priority is at its highest     An example output    Adjust the program above (after copying it, off course) so it goes through the exact same changes in priority. Instead of changing it externally the program should change it internally, on itself.  hint: use setpriority()     An example output    Write a program in C that forks 25 new processes. Each forked process re-nices itself to one specific value (equally distributed over -20, -10, 0, 10, 19). That re-niced value won\u0026rsquo;t change anymore. After doing that, the processes count the number prime numbers lower than 10000. When they are finished, they print their count value (together with their ID and the PRIORITY).  Make sure the parent process waits until all its children are done. The printing of the count value is important \u0026hellip; Experiment what happens when you don\u0026rsquo;t do that.     An example output    Use the top command to inspect priority and nice values for running tasks on the system\n What are the PR and NI columns? How do you interpret their values? How do you see which tasks are real-time tasks? Use top while running the previous two exercises. Do you see the values changing as you\u0026rsquo;d expect? Why (not)?  Adjust the blink program\n By default, you can configure this to give all of the processes either high, normal or low priority (see the defines on top) Change the logic so that the first half of the processes (top of the screen) get highest priority, and the bottom half get lowest priority Describe the results. Is this what you expected? Why (not)? Note: since you\u0026rsquo;re running Ubuntu in a Virtual Machine, it might be that it can\u0026rsquo;t handle the default amount of processes. If necessary, lower the count and try again.   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/lab2_browsing/",
	"title": "9.4: Simply browsing through pages",
	"tags": [],
	"description": "",
	"content": "  image source: wikipedia.com    Get page size on the Linux system in the Virtual Box through a C program   An example output  \n Assume a system with a 32-bit logical addresses and a 4-kB page size. Write a C program that is passed a logical address (in decimal) through command line arguments. Print the page number and the offset for the given address. Validate that only a single command line argument is given!   An example output  \n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/ecosystems/",
	"title": "1.5: C Ecosystems",
	"tags": [],
	"description": "",
	"content": " Separation of concerns: functions in different C files To make the division clearer, we prepare the following C code, split into two different files:\nFile hello.c:\n// hello.c char* hello() { return \u0026#34;heykes\u0026#34;; } File main.c:\n// main.c #include \u0026lt;printf.h\u0026gt;int main() { printf(\u0026#34;%s\u0026#34;, hello()); return 0; } The main function has no knowledge of hello() because it lives in a different source file. This will be fine if we link the machine code together after compiling. compiling main.c separately gives this:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ gcc -c main.c main.c:5:18: warning: implicit declaration of function 'hello' is invalid in C99 [-Wimplicit-function-declaration] printf(\"%s\", hello()); ^ 1 warning generated.  It is a WARNING - not an ERROR - so it still compiles! Wow! That is thanks to the -c flag (compile only). The warning is easily solved with a forward function declarations before the main function: char* hello ();. This is the crucial difference between declaration and definition. However, the problems are not yet solved if we want to link this without hello.c:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ gcc main.o Undefined symbols for architecture x86_64: \"_hello\", referenced from: _main in main.o ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation)  Okay, so now a blocking ERROR was generated. We also need the hello.o binaries to arrive at a successful working program. For that, we first have to execute gcc -c hello.c and thengcc main.o hello.o -o hey.\nWith the UNIX tool nm we can view the addresses that the linker needs to arrive at the hey executable. Try opening hello.o with a text editor. You then see something like this:\n cffa edfe 0700 0001 0300 0000 0100 0000 0300 0000 f001 0000 0020 0000 0000 0000 1900 0000 8801 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 7800 0000 0000 0000 1002 0000 0000 0000 7800 0000 0000 0000 0700 0000 0700 0000 0400 0000 0000 0000 5f5f 7465 7874 0000 0000 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 0000 0000 0000 0000 0d00 0000 0000 0000 1002 0000 0400 0000 8802 0000 0100 0000 0004 0080 0000 0000 0000 0000 0000 0000 5f5f 6373 7472 696e 6700 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 0d00 0000 0000 0000 0700 0000 0000 0000 1d02 0000 0000 0000 0000 0000 0000 0000 0200 0000 0000 0000 0000 0000 0000 0000 5f5f 636f 6d70 6163 745f 756e 7769 6e64 5f5f 4c44 0000 0000 0000 0000 0000 0000 1800 0000 0000 0000 2000 0000 0000 0000 2802 0000 0300 0000 9002 0000 0100 0000 0000 0002 0000 0000 0000 0000 0000 0000 5f5f 6568 5f66 7261 6d65 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 3800 0000 0000 0000 4000 0000 0000 0000 4802 0000 0300 0000 0000 0000 0000 0000 0b00 0068 0000 0000 0000 0000 0000 0000 0200 0000 1800 0000 9802 0000 0400 0000 d802 0000 2400 0000 0b00 0000 5000 0000 0000 0000 0200 0000 0200 0000 0200 0000 0400 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 5548 89e5 488d 0500 0000 005d c368 6579 6b65 7300 0000 0000 0000 0000 0000 0000 0d00 0000 0000 0001 0000 0000 0000 0000 0000 0000 0000 0000 1400 0000 0000 0000 017a 5200 0178 1001 100c 0708 9001 0000 2400 0000 1c00 0000 a8ff ffff ffff ffff 0d00 0000 0000 0000 0041 0e10 8602 430d 0600 0000 0000 0000 0700 0000 0000 001d 0000 0000 0100 0006 1200 0000 0e02 0000 0d00 0000 0000 0000 1900 0000 0e04 0000 3800 0000 0000 0000 0100 0000 0f01 0000 0000 0000 0000 0000 0800 0000 0f04 0000 5000 0000 0000 0000 005f 6861 6c6c 6f00 5f68 616c 6c6f 2e65 6800 4c5f 2e73 7472 0045 485f 6672 616d 6530 0000  Beautiful, but not very clear. nm does help some:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ nm main.o 0000000000000060 s EH_frame0 0000000000000037 s L_.str U _hello 0000000000000000 T _main 0000000000000078 S _main.eh U _printf Wouters-MacBook-Air:cmake-build-debug wgroenev$ nm hello.o 0000000000000038 s EH_frame0 000000000000000d s L_.str 0000000000000000 T _hello 0000000000000050 S _hello.eh  You can see that in main.o the function _hello is assigned an unknown address (hence the U). This means that the left hand should assume that it is yet to come - and luckily it is correctly defined in hello.o at address 0000000000000000 (there is only 1 function).\nNote that both .o files have overlapping \u0026ldquo;address spaces\u0026rdquo;: both _hello and _main are at address 0000000000000000; this is normal. The linker will properly change/offset these addresses when creating the final program so they no longer overlap.\n This is the way the files will be coupled to each other:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A[hello.c] B[main.c] Chello.o] Dmain.o] E{hey executable} E -- C E --|\"search main() via linker\"|D C -- A D -- B D -.-|\"search hello() via linker\"|C  Functions that have been declared in other source files must therefore be redefined (possibly with the external keyword) in your own source file where you wish to use the function. This way the compiler knows that there is a function with that signature, but \u0026ldquo;he will still come across it\u0026rdquo;. This will be further elaborated in the next labs.\nCompiling everything We use the UNIX GNU gcc compiler to compile C source files and convert them into binaries. The simplest possible way to do so is:\n gcc *.c\n Due to the lack of a target file name, the compiler creates an \u0026ldquo;a.out\u0026rdquo; file that you can execute (chmod +x has already been done for you). You can specify this with the \u0026ldquo;-o\u0026rdquo; flag. If you have something more to link, put everything in a row one after the other.\nHowever, there are still a lot of compiler options that are explained at gcc.gnu.org that you can play with.\nWhen targeting another platform, you will need a cross-compiler that compiles on your computer for another computer. That is, the instruction set might differ! (64 or 32-BIT, RISC/ARM, \u0026hellip;) Instead of using the default GCC compiler: gcc bla.c, you will download and install a custom cross-compiler and evoke it the same way: arm-eabi-none-gcc bla.c. The Game Boy Advance (GBA) or RaspberryPi for instance have an ARM chip-set and require this cross-compiler. This differs from most x86 chip-sets that leverages gcc.\n Are you still cross-compiling if you are compiling on an ARM machine yourself, using gcc, compiled for that chip-set? What if you compile code on the Raspberry for your laptop?\n  Step 1: compiling As seen in the above schematic, executing your source code requires the activation of two steps: compiling (1), and linking (2). C Preprocessor flags get parsed just before compiling. Simply calling the gcc compiler executes all steps at once. Only compiling is done using the -c statement (source input) and providing the source files as arguments, producing object files, which can be then linked into a binary.\nStep 2: linking After obtaining object files it is simply a matter of concatenating them (\u0026lsquo;linking\u0026rsquo;), to create the native executable binary file, using the -o flag and providing the object files as arguments. After linking, inspecting the disassembly (see chapter 4 on how to do so in detail) shows the concatenated results.\nRepeatedly compiling (1) using a script It is annoying to have to type the same command all the time, so a simple alternative is to put your gcc command in a shell script:\n#!/bin/sh clear \u0026amp;\u0026amp; gcc -o mystuff source.c \u0026amp;\u0026amp; ./mystuff (2) Makefiles In the C world there is such a thing as a \u0026ldquo;Makefile\u0026rdquo; that defines which source files should be compiled, and in which order. This is useful for large applications where an overview must be kept.\nWith Makefiles you can describe \u0026ldquo;targets\u0026rdquo; that perform certain actions for you. For example, cleaning up binaries, compiling and linking, all as a separate step. Stringing steps together is of course also possible.\nFile Makefile:\n.DEFAULT_GOAL := all CC=gcc clean: rm -rf *.o rm -rf *.out compile: $(CC) -c main.c -o main.o link: $(CC) -o main.out main.o all: clean compile link Typically, the compiler used is set as a shell variable (CC = gcc). You can see here that compiling (gcc with the -c option does not link) and linking is split into separate make \u0026lsquo;actions\u0026rsquo;. This is not really necessary in our exercises, but shows the organizational strength of Make here.\nExecuting the above steps can be done using the make command and a default goal (all), or make compile for a specific goal (executes the \u0026lsquo;compile\u0026rsquo; steps only).\nFor more information on the correct Makefile syntax, see the GNU make documentation.\nCreate a Makefile that contains two targets: compile and run. The default target should execute both in sequence. As for what to compile, write a simple program that outputs \u0026ldquo;hello, (name)\u0026rdquo;. The name is something you ask from the user using the stdio function gets().\n  (3) IDEs Lightweights A source file consists of simply plain text. Any text editor is sufficient to write your C program. However, it may be useful to use Sublime Text or Visual Studio Code. These modern powerful editors have built-in auto-completion and build tools.\n Sublime Text 3 Build Systems en C/C++ and docs Visual Studio Code C/C++ integration  We will not stop old-school fans from using Emacs or Vi(m).\nHeavyweights CLion is the perfect cross-platform and cross-compiler candidate to do the heavy C/C++ lifting for you, and it comes with integrated debugging, stack inspection, and everything else you might expect from an IDE. If you are familiar with IntelliJ, you will love CLion: it\u0026rsquo;s built on the same platform (IDEA) - even most shortcuts are the same.\n  CLion is not free, but it is highly recommended for students (and they get a free license if you register with your university e-mail address). CLion works with CMake: CMakeLists.txt contains instructions to generate aMakefile:\n cmake_minimum_required(VERSION 3.10) project(my_little_pony) set(CMAKE_CXX_STANDARD 11) add_executable(my_little_pony main.cpp biblio.cpp biblio.h animals.cpp animals.h)  A simple CMake file is much easier to read and write than a Makefile. See CMake tutorial for more information. CLion manages the add_executable arguments for you: adding new files to your project will also automatically add them to the list.\nSince CMake builds Makefiles and Makefiles use cmdline to evoke the compiler, you are essentially using high-level tools that use low-level tools. This makes it easier to repeatedly compile bigger projects, instead of calling gcc file.c yourself every single time.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD cmake[CMake] make[Makefiles] cmd[gcc \u0026 cmdline] cmake -- make make -- cmd  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-pointers/lab/",
	"title": "3.3: The Ancient Library",
	"tags": [],
	"description": "",
	"content": " The Ancient Library  Stuff just got interesting: you find yourself inside an extremely old library, where things clearly have not been touched for ages. A green glowing ball rests in the center of the room, giving the books an odd, but compelling color. Do you dare to touch the books? Let us go ahead and clean this place up a bit.\n   Source: dublin2019.com   1. Modeling the books The back of the first row of books is barely readable, but you can distinguish the following titles and authors:\n Cleaning the streets Beyond the Gate, by Ulder Ravengard Beyond the depths of the Underdark, by Drizzt Do\u0026rsquo;Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera  How should we proceed to model the concept of a book? Right, a struct! Keep track of the title and the author using char* properties.  Proceed with caution and whatever you do, do not directly look into the green light! Use the following blueprint:\n#define SIZE 4 Book* create_books() { // ?? } void print_books(Book* library) { // ?? } int main() { Book* library = create_books(); print_books(library); // should print the above list ([title], by [author])  return 0; } 2. Linking things together The strcuture you provided can be expanded with a third property: Book* next, pointing to the next element in the row. That way, when looping through all books, we simply need to follow the \u0026lsquo;next\u0026rsquo; pointer, until a NULL is reached, indicating the end of the library. This technique makes it possible to loop through things without knowing it\u0026rsquo;s size!\nRework your implementation by removing the #define SIZE 4 statement, and by relying on the next pointer in the print_books() method.\nTips:\n What should the value of next be when creating the book instances in create_books()? What should that function return? A handle to what? You can use while(ptr) { ... } instead of a for loop. The value within the while() statement always evaluates to true, unless a \u0026ldquo;nullpointer\u0026rdquo; (a value of 0 or NULL) is detected. That is exactly what we want. Another method to ease use of the malloc() statements might come in handy, such as Book* create_book(char* title, char* author).  3. Cleaning up the library The place looks dusty, doesn\u0026rsquo;t it?  Also, the books seem to be placed in a random order. Why don\u0026rsquo;t we take the time, now that we are here and enjoy the green glow, to order these books alphabetically by author? You can ignore the surnames, simply sort on the property author. That is, the expected output of your program should be:\n Beyond the depths of the Underdark, by Drizzt Do'Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera Cleaning the streets Beyond the Gate, by Ulder Ravengard  Create and call void sort_books(Book* library) before printing them.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; H{head} T{tail} A[book 1] B[book 2] C[book 3] D[book 4] NNULL] A --|next| B B --|next| C C --|next| D D --|next| N H -- A T -- B  Tips:\n Use the strcmp() function from \u0026lt;string.h\u0026gt; to compare two strings. Sorting a linked list is not as difficult as you might think. You will need some sort of swapping function. Take a look at the above drawing. If book 4 needs to be first, how do you swap it with book 1, while keeping the links intact? Use recursion to repeatedly call sort_books() within the same function. Which book should be placed in the beginning? What should you process next? Think in terms of head and tail.  4. Wild Surge! ASC/DESC  A sudden gust of wind enters the old library. You immediately halt what you were doing to carefully listen if imminent threats are upon you and your party. A cloaked figure swings open a side-door and starts casting a spell that sounds very unfamiliar to you. Praeses, Alia, Fero\u0026hellip; An alteration, but which one?! Quick, hide the books! But alas, it was too late\u0026hellip;  While the spell completes, you hear the figure cursing \u0026ldquo;annoying wild surges!\u0026rdquo;. The voice sounds like a woman. She rushes off, but the part of her face you were able to catch in the glimpse of an eye seems oddly familiar\u0026hellip; Suddenly, you know it. The book! The cow book! That was the author! Neera\u0026hellip;\n Neera managed to completely reverse our sorting strategy, sorting Z to A instead of A to Z. She used a function pointer to encapsulate strcmp(). The following is your main function:\nint main() { Book* library = create_books(); neera_encounter(); // changes a function pointer from asc to desc  sort_books(library); print_books(library); // prints Z -\u0026gt; A  return 0; } Create a function pointer that is initialized to the address of your ascending sorter. Neera her Wild Magic switches this to the address of a descending sorter. That pointer will be used inside sort_books() instead of strcmp().\nTips:\n Re-read chapter 3 (pointers)! You will need two extra functions, asc() and desc(), besides neera_encounter().  5. Do you rest until fully healed?  The rumble of Neera\u0026rsquo;s spell made your head rush and you fill dizzy. Maybe it would be better for you and your party to stay for the night and set up camp. During the night, you make the time to write up on the past events. It looks like it will be quite a thick book, something new to add to the old library. Now, should you add the book at the beginning of the row, or all the way at the end?\n Create a function called void add_book(Book* library, Book* book), that adds a new book to the end of the library.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; new(new book) A[book 1] B[book 2] NNULL] B -.-|next| new new --|next| N A --|next| B  That was quite simple, wasn\u0026rsquo;t it? What if we want it to add new books to the beginning of the library? You cannot change the Book* library pointer as it is copied over by-value. For this to work, we need to change the signature to a pointer of a pointer: void add_book(Book** library, Book* book).\nUse the following main function to test your code:\nint main() { Book* library = create_books(); Book* newbook = create_book(\u0026#34;My adventures\u0026#34;, \u0026#34;The Hero\u0026#34;); // add newbook to the library. But how?  print_books(library); return 0; }"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/lab_ll_rtos/",
	"title": "3.3: The Ancient Library (RTOS)",
	"tags": [],
	"description": "",
	"content": " The Ancient Library  Stuff just got interesting: you find yourself inside an extremely old library, where things clearly have not been touched for ages. A green glowing ball rests in the center of the room, giving the books an odd, but compelling color. Do you dare to touch the books? Let us go ahead and clean this place up a bit. (img src)\n 1. Modeling the books The back of the first row of books is barely readable, but you can distinguish the following titles and authors:\n Cleaning the streets Beyond the Gate, by Ulder Ravengard Beyond the depths of the Underdark, by Drizzt Do\u0026rsquo;Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera  How should we proceed to model the concept of a book? Right, a struct! Keep track of the title and the author using char* properties.  Proceed with caution and whatever you do, do not directly look into the green light! Use the following blueprint:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt; // typedef of \u0026#34;book\u0026#34;  // forward declarations book* create_books(void); void print_books(book* library); // main functions int main() { book* library = create_books(); print_books(library); return 0; } // other functions book* create_books(void) { // ?? } void print_books(book* library) { // ?? } The structure you provided can be expanded with a third property: book* next, pointing to the next element in the row. That way, when looping through all books, we simply need to follow the \u0026lsquo;next\u0026rsquo; pointer, until a NULL is reached, indicating the end of the library. This technique makes it possible to loop through things without knowing it\u0026rsquo;s size!\nTips: You may hardcode the four books mentioned above in the create_books() function. Pay attention that you keep the order of the entries !!\nWrite the required struct and complete the function bodies of create_books() and print_books().\n  2. Cleaning up the library The place looks dusty, doesn\u0026rsquo;t it?  Also, the books seem to be placed in a random order. Why don\u0026rsquo;t we take the time, now that we are here and enjoy the green glow, to order these books alphabetically by author? You can ignore the surnames, simply sort on the property author. That is, the expected output of your program should be:\n Beyond the depths of the Underdark, by Drizzt Do'Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera Cleaning the streets Beyond the Gate, by Ulder Ravengard  Tips:\n Use the strcmp() function from \u0026lt;string.h\u0026gt; to compare two strings. The sorting algorithm that you use is not important. You can simply use bubble sort. However, if you want to use a fancy approach, feel welcome to do so. Sorting should be done by moving the objects, not the content of the objects. For the sake of simplicity, you may assume that the length of the library is fixed to 4.  \u0026nbsp;\nint main() { book* library = create_books(); print_books(library); library = sort_books(library); print_books(library); return 0; } Create the function book* sort_books(book* library). Display the original library and then print it again, after applying the sort_books() function.\n  [OPTIONAL] 3. Wild Surge! ASC/DESC  A sudden gust of wind enters the old library. You immediately halt what you were doing to carefully listen if imminent threats are upon you and your party. A cloaked figure swings open a side-door and starts casting a spell that sounds very unfamiliar to you. Praeses, Alia, Fero\u0026hellip; An alteration, but which one?! Quick, hide the books! But alas, it was too late\u0026hellip;  While the spell completes, you hear the figure cursing \u0026ldquo;annoying wild surges!\u0026rdquo;. The voice sounds like a woman. She rushes off, but the part of her face you were able to catch in the glimpse of an eye seems oddly familiar\u0026hellip; Suddenly, you know it. The book! The cow book! That was the author! Neera\u0026hellip;\n Neera managed to completely mess up our sorting strategy, sorting on the title of the book instead of on the author.\nint main() { book* library = create_books(); print_books(library); library = sort_books(library); print_books(library); // NEERA ENCOUNTER:  // changes a function pointer of sort_books() from sort_books_by_author() to sort_books_by_title()  library = sort_books(library); print_books(library); return 0; } Tips:\n Re-read chapter 3 (pointers)! You will need two extra functions, sort_books_by_author() and sort_books_by_title().  Changes the function pointer of sort_books() from sort_books_by_author() to sort_books_by_title()\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-introos/",
	"title": "5: Intro in OS",
	"tags": [],
	"description": "",
	"content": " Chapter 5 5.1: Intro to OS using Linux\n5.2: (lab) Getting your CLI-feet wet\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab2_threadsmgmt/",
	"title": "6.5: Threads (lab)",
	"tags": [],
	"description": "",
	"content": "    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nCreating and inspecting threads  Write a C-program that has 3 threads. Each thread announces its existence through a printf() together with its PID. Note that, unlike when spawning multiple processes, the PID is of course the same for all threads.    An example output    Take a closer look at the Pthreads example code from the previous section (with the doSomeThing function).\n Without executing the code, what do you think will be printed to the screen (with regards to the \u0026ldquo;Job started and finished\u0026rdquo; messages)? Why? Compile and run the code on your machine What is the actual output? Is it the same as what you guessed? Why (not)? Note: you do not have to think of a \u0026ldquo;fix\u0026rdquo; for this problem just yet: we\u0026rsquo;ll discuss some possible solutions in the next Section.  In the processes lab, you had to implement a program that spawned different parallel processes, each calculating all the primes between 0 and a given number. Let\u0026rsquo;s adapt this to threads, but with a twist.\n Write a C-program that has 4 threads (in addition to the main thread). Each thread calculates all prime numbers between a lower and larger limit. Given a maximum number N which is divisible by 4 (e.g., 100000):  Thread 1 computes all primes between 2 and N/4 Thread 2 computes all primes between N/4 + 1 and N/2 Thread 3 computes all primes between N/2 + 1 and 3N/4 Thread 4 computes all primes between 3N/4 + 1 and N  After all threads are done, the main thread prints all the resulting primes in the proper order (low to high) to the screen.  Note: this means that the threads dont print the primes themselves like in the processes exercise: you have to instead use heap-allocated memory to communicate back the results to the main thread!  For this exercise, it\u0026rsquo;s sufficient for each thread to store their results separately and communicate them back at the end. We will see other options later.  There are several ways of doing this and none of them are perfect (we\u0026rsquo;ll see how to make things better in the next Section). For now, just pick a method you think will work, don\u0026rsquo;t worry too much about cleanliness.  Imagine you wouldn\u0026rsquo;t have to print the results fully ordered from low to high (but still in the main thread) (You don\u0026rsquo;t actually have to implement this). Would you be able to make the program more efficient then? Why (not)?   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-c/ecosystems-labs/",
	"title": "1.6: Splitting up Code",
	"tags": [],
	"description": "",
	"content": " Building with Makefiles It is time to split up our Orc exercise code into different parts:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD E{Executable} A[main.c] AAmain.o] B[orcs.c] BBorcs.o] D[orcs.h] E -- AA E -- BB AA -- A AA -- D BB -- B BB -- D  Create four different files:\n orcs.h - this is where your struct definition resides. orcs.c - this is where your methods related to orcs reside. Include the orc header file. main.c - this is where your main method resides. Include the orc header file. Makefile - builds everything. Create a compile, link, and clean target.  Compile both C files with separate gcc -c commands, merging them together with a third command, as seen in the previous section.\n  When you think you can manage using the gcc command in the commandline, automate everything by leveraging the power of Makefiles.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/interthread/",
	"title": "6.6: Inter Thread communication",
	"tags": [],
	"description": "",
	"content": " One raison d\u0026rsquo;√™tre for multi-threaded applications is resource sharing. In the example that was given earlier a global variable \u0026lsquo;counter\u0026rsquo; was used. No measures were taken for securing this approach and we got some unexpected results. The output of the example looks like shown below:\n  This probably came as a surprise üòÉ  It should be clear that what we wanted to see was Job 1 started followed by Job 1 finished and that this would be repeated again for job number 2.\nThe first of both threads that gets to its function increments the counter from 0 to 1. However, before this thread has finished its execution, the second thread has started and has incremented the counter from 1 to 2. By the time the first thread finishes, the counter value is 2, which is in contrast with the intended value of 1.\nCritical Section and Race Conditions At first glance, it might seem that this problem is easily solvable by copying the global counter to a local, per-thread variable for later use:\nvoid* doSomeThing(void *arg) { unsigned long i = 0; int local_id; // new, per-thread variable  counter += 1; local_id = counter; // make a local copy of counter for this thread  printf(\u0026#34; Job %d started\\n\u0026#34;, id); for(i=0; i\u0026lt;(0xFFFFFFFF);i++); printf(\u0026#34; Job %d finished\\n\u0026#34;, id); return NULL; } If you tried to implement this, it would probably work\u0026hellip; most of the time at least. As explained in the first section on threads, threads are not necessarily run in parallel all the time, especially not if the number of threads is larger than the number of CPU cores. In fact, there are many situations where given 2 threads, the first one will get some time to execute, is then paused to give the second one some time to execute, unpaused to get some time etc.\nAs such, if two Threads executing doSomeThing() would be running on a single CPU, the following sequence of actions could occur:\n// Code that both threads will execute: counter += 1; local_id = counter; // -----------------------------------  // Thread 1 counter += 1; // after this, counter == 1  // Before Thread 1 can execute local_id = counter, it is paused by the CPU  // Thread 2  counter += 1; // after this, counter == 2  local_id = counter; // thus, in Thread 2, local_id is correctly 2 // Here, the CPU switches back to Thread 1 // Thread 1 local_id = counter; // now, local_id is still 2, instead of 1... mission failed We can see that the exact problem we were trying to prevent can still occur, but it depends on how the threads are scheduled by the OS. This is what makes multithreaded programming so difficult in practice: your program can execute without problems 99 times, but still fail the next. These bugs can be very hard to reproduce. They are often called race conditions, referring to e.g., a car/horse race where it\u0026rsquo;s not always the same contestant that wins, depending on the conditions of the racetrack. Note that this does not only occur in the case of multiple threads on the same core: similar problems can of course also happen if you have actual parallel threads as well.\nIn general, we can see that once multiple threads start accessing shared memory, things can go wrong. As such, these specific points in a multithreaded program are often referred to as the critical section: it\u0026rsquo;s of the highest importance (critical) that this section is executed by itself, without outside interference. Otherwise, race conditions can occur and we can introduce bugs.\nTo make this all a bit more tangible, we will be using an outside interactive tool, called Deadlock Empire. Instead of having you write correct code for a problem, this website challenges you to find bugs in existing code to show the many caveats of multithreaded programming. While the website uses the C# programming language and is a bit different from the C syntax we\u0026rsquo;re using, the high-level concepts are 100% the same.\nGo to the Deadlock Empire website and do:\n Tutorial: Tutorial 1: Interface Unsynchronized Code: Boolean Flags Are Enough For Everyone Unsynchronized Code: Simple Counter    Atomic operations Now that you have a feeling for race conditions and critical sections, it\u0026rsquo;s time to make things worse.\nThe example above isn\u0026rsquo;t only vulnerable through the \u0026ldquo;local_id = counter\u0026rdquo; code: the \u0026ldquo;counter += 1\u0026rdquo; code is also vulnerable to a race condition. This is because incrementing a variable (counter += 1) is not a so-called atomic operation. This means that internally, it is not implemented with a single CPU instruction, but rather composed out of a series of different operations/instructions.\nFor example, for counter += 1, the series of executed steps might look like this:\n Fetch value of counter from RAM and store it in the CPU cache memory Fetch value from CPU cache memory and store it in a CPU register for the calculation Add 1 to the register value Write the register value back to the CPU cache Write the cached value back to the RAM  Put differently: the CPU does not act on the value stored in memory directly, but rather a copy in a register. Copying from/to a register is not atomic, so bugs can occur. To make things simpler to reason about, we can boil this down to just two lines of code:\n// In simple pseudocode, counter += 1 might look like this: int temp = counter + 1; // temp is for example the CPU register here, and \u0026#34;counter\u0026#34; is the value in memory counter = temp; // the register value is written back out to the memory To the CPU, all of these steps are one or more instructions, each of which can also have a certain delay associated with them. As such, what can happen in practice is the following sequence of events:\n// Code that both threads will execute: int temp = counter + 1; counter = temp; // -----------------------------------  // Thread 1 int temp = counter + 1; // after this, Thread 1\u0026#39;s temp register contains the value 1. The \u0026#34;counter\u0026#34; value in RAM is still 0.  // Before Thread 1 can actually store this temporary register result in memory, the CPU gives control to Thread 2  // Thread 2  int temp = counter + 1; // \u0026#34;counter\u0026#34; was still 0 in RAM, so Thread 2\u0026#39;s temp register now also contains the value 1  counter = temp; // The \u0026#34;counter\u0026#34; value in RAM is now 1 // Here, the CPU switches back to Thread 1 // Thread 1 counter = temp; // The \u0026#34;counter\u0026#34; value in RAM is still 1 We can see that, even though two \u0026ldquo;counter += 1\u0026rdquo; lines of code were executed, the resulting value in memory is just 1 instead of 2. Again: you probably never saw this when testing the exercise in the lab, but theoretically it -could- happen, leading to randomly failing programmes.\nNote that this is a direct consequence of the fact that threads have separate program counters and register values, as discussed in the first Section on threads. Once individual threads are started, even if they are executing the same code, they do so in a partially separated context.\nGo to the Deadlock Empire website and do:\n Tutorial: Tutorial 2: Non-Atomic Instructions Unsynchronized Code: Confused Counter    You might now think that this can only happen if two threads execute the exact same code. However, you would be wrong, as illustrated by the following example from Wikipedia:\n// Thread 1 // ... other code b = x + 5; // ... other code  // Thread 2  // ... other code  x = 3 + z; // ... other code Give a practical, numerical example of how, depending on CPU scheduling, b can end up with two very different values.\n  Mutexes and Locking As we\u0026rsquo;ve seen in many examples, things can go wrong really quickly when dealing with shared memory. Making matters worse, simple solutions (such as making thread-local copies of shared memory) are also doomed to fail eventually.\nAs such, it should be clear that we need a way to secure the critical sections in a thread. We need a way to make sure the OS will not pause a thread during a critical section, to make sure that all instructions are done before another thread gets execution time. Put differently: we need a way to turn groups of non-atomic operations into a single big atomic block.\nThe simplest way of doing this, is by means of a mutex. This term is a portmanteau of Mutual Exclusion. The behaviour can be seen as the lock on a toilet door. If you are using the toilet, you lock the door. Others that want to occupy the toilet have to wait until you\u0026rsquo;re finished and you unlock the door (and get out, after washing your hands). Hence, the mutex has only two states: one or zero, on or off, locked or unlocked.\nInternally, a mutex lock is typically implemented by means of a single, atomic CPU instruction. Otherwise, simply locking or unlocking the mutex would of course potentially lead to the problems it is trying to solve!\nThe concept of a mutex is implemented in the pthreads library as a new variable type: pthread_mutex_t. Locking and unlocking can be done through functions pthread_mutex_lock() and pthread_mutex_unlock(). As always, read the documentation for the exact usage of these functions.\nThe problematic example of the shared counter can be rewritten using a mutex:\nint counter; pthread_mutex_t counter_lock; /* THIS LINE HAS BEEN ADDED */ void* doSomeThing(void *arg) { unsigned long i = 0; int local_id; /* THIS LINE HAS BEEN ADDED */ pthread_mutex_lock(\u0026amp;counter_lock); /* THIS LINE HAS BEEN ADDED */ counter += 1; local_id = counter; /* THIS LINE HAS BEEN ADDED */ pthread_mutex_unlock(\u0026amp;counter_lock); /* THIS LINE HAS BEEN ADDED */ printf(\u0026#34; Job %d started\\n\u0026#34;, local_id); /* THIS LINE HAS BEEN CHANGED */ for(i=0; i\u0026lt;(0xFFFFFFFF);i++); printf(\u0026#34; Job %d finished\\n\u0026#34;, local_id); /* THIS LINE HAS BEEN CHANGED */ return NULL; } This solves the issue that was encountered above. Before the counter is accessed, the mutex is locked. This provides exclusive access to the following 2-line critical section untill the mutex is unlocked again. The counter is then incremented and copied in to variable local_id. Finally, the mutex is unlocked.\nWith this measure in place, the result is as was originally intended.\n  Note: The amount of code/instructions between locking and unlocking a mutex should of course be kept to a minimum. If you put a mutex around your entire threading function, you undo the entire possible benefit of using threads: the fact that they can run in parallel! As such, inter-thread communication via shared memory should be kept to a minimum, and only that should be protected using Mutexes/locks.\nGo to the Deadlock Empire website and do:\n Locks: Insufficient lock Locks: Deadlock    Deadlocks While locks help with many critical section problems, the Deadlock Empire examples above show that they are also not always trivial to apply correctly. This is especially the case if there is more than one shared resource/variable/memory region in play.\nFor example, a typical problem that can arise is when two threads need to obtain access to two different shared resources, but do so in opposite orders. This can for example happen in complex programs that often interact with outside peripherals like the hard disk and the network:\n// Thread 1 wants to first read from the network, then write result to the hard disk // Thread 2 wants to first read the contents of a file, then send it on the network  // Thread 1: network.lock() // CPU pauses Thread 1 and activates Thread 2  harddisk.lock() // CPU pauses Thread 2 and activates Thread 1 again harddisk.lock() // this will not complete, because Thread 2 is holding this lock already. Thread 1 has to wait  // CPU pauses Thread 1 because it has to wait and activates Thread 2 again  network.lock() // this will not complete, because Thread 1 is holding this lock already. Thread 2 has to wait  // Both threads are waiting endlessly AND also locking the network and the harddisk for any other threads that might arrive Note: this example is not entirely realistic, as the network and harddisk can generally be used by multiple threads and processes concurrently. This is because the OS (or even the hardware itself) has layers of abstraction, but also complex ways of detecting and preventing deadlocks from happening (for extra information, see this presentation (this is not course material)). Still, you can pretty easily make this mistake in your own program when accessing multiple pieces of shared memory, so watch out!\nSemaphore While mutex locks are useful, the are also relatively simple and limited in what they can convey. As such, over time more advanced thread synchronization techniques have evolved that make it a bit easier to deal with often occurring scenarios. One such more advanced technique is a semaphore. A semaphore makes it easier to track how many threads are requesting access or have already been given access to a particular resource. To illustrate this, a semaphore can be thought of as a bowl with tokens. For example, in a child daycare there can be a room with toys:\n  This photo of Cafe Boulevard is courtesy of Tripadvisor   Only 5 children are allowed in that room. Outside, there is a bowl with bracelets. When a child wants to enter the room to play, they need to take a bracelet and put it on. When there are no more bracelets in the bowl, a child that also wants to play in the room has to wait until another child leaves the room and places their bracelet back in the bowl.\nThis technique is used in e.g., producer-consumer problems, amongst many other types. In those settings, several producer threads prepare data and put it in shared memory, ready to be used by one or more consumers. Instead of having a separate counter that tracks how many produced items are waiting for processing, a semaphore can be used directly, making state keeping a bit easier.\nThis is because in contrast with the mutex, a semaphore is a count of tokens. Put differently, a mutex limits access to a single resource (e.g., a single memory location) to one thread at a time. A semaphore instead allows threads to share a limited pool of resources (e.g., 4 different hard disks), with multiple threads potentially active at the same time. Note though that if there is only a single token in the semaphore, this behaves exactly the same as a mutex (this specific type is referred to as a binary semaphore).\nThe pthreads library also provides an API to program with semaphores (include semaphore.h to use). It contains, amongst others, functions like:\n sem_init(): initialises a semaphore. sem_wait(): decrements the number inside of the semaphore. sem_post(): increments the number inside of the semaphore. sem_destroy(): destroys the semaphore when it\u0026rsquo;s no longer needed.  Note that the sem_wait() function is blocking (similar to pthread_join). If the value of the semaphore is set to zero when sem_wait is called, the thread is paused until another thread invokes sem_post() (put differently: until the semaphore is \u0026ldquo;unlocked\u0026rdquo;). In other programming languages, the post() operation is often referred to as signal(). There is also a non-blocking alternative to sem_wait: sem_trywait(). See the manual pages and search Google for more info and the correct usage.\nNote that multiple threads can be waiting on the same semaphore at any given time. When a sem_post occurs, they of course cannot all start immediately, since only one token was added. Here there are again several ways for the OS to decide which thread gets preference (e.g., the one that has been waiting the longest, the one that was most recently started) and gets to consume the token.\nMore info on the differences between a semaphore and a mutex are given here.\nGo to the Deadlock Empire website and do:\n Semaphores: Semaphores Semaphores: Producer-Consumer Semaphores: Producer-Consumer (Variant)    Note that next to mutexes and semaphores, there are many other thread synchronization utilities and concepts (such as for example conditional variables and barriers). Especially more modern programming languages like Java and C# typically have highly advanced threading options built-in. Some of these you can (optionally) explore and experiment with using the exercises from Deadlock Empire that we skipped.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/",
	"title": "6: Task management",
	"tags": [],
	"description": "",
	"content": " Chapter 6 Task management 6.1: Processes\n What\u0026rsquo;s in a process ? Run process !  Process Control Block Process state Open files list  Creating processes  6.2: Processes (lab)\n6.3: Inter-Process Communication (IPC)\n6.4: Threads\n What\u0026rsquo;s in a thread ? Why would we want multiple threads ?  Amdahl\u0026rsquo;s law  Creating threads  Pthreads   6.5: Threads (lab)\n6.6 Inter-Thread communication * Critical Section * Communication between threads * Locking * Mutex * Semaphore\n6.7 Inter-Thread communication (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab3_interthreadcomm/",
	"title": "6.7: Inter-Thread communication (lab)",
	"tags": [],
	"description": "",
	"content": "  Source: G.I.    Write a C-program that has 1 king thread and n servant threads. The number of servant threads should be configurable through a define. The king thread sleeps for a random amount of time (a few seconds) and signals the servants when it awakes. All servant threads politely say Good morning, sire together with a servant ID. The king thread goes to sleep again.\n Every servant needs to greet the king once (no more, no less) The king doesn\u0026rsquo;t wait for the greetings before he goes back to sleep You do not need to make sure that the servants always reply in the same order Repeat the wake-greet cycle at least 5 times Tip: use the rand() function to get random numbers  REVISIT, WITH A TWEAK Change the previous king-servant setup with a single change: now, the king does explicitly wait for the greetings of all his servants before he goes back to sleep.\n    An example output    Let\u0026rsquo;s revisit the prime numbers for one last time. Like the prime-exercise in the previous lab, we will have 4 separate threads, each calculating a subsection of primes between 2 and N. Now however, we will not have each thread store their full results in thread-specific memory. Instead, each prime-calculting thread (the producers) communicates each discovered prime as soon as possible back to a separate \u0026ldquo;processing\u0026rdquo; entity that prints the primes to the screen (one or several consumers). Again, this can be accomplished in several different ways:\n You can use a single shared array and a shared counter (indicating how many items are in the array) to store the results  Access to the array is protected by a Mutex that both the producers and consumer(s) lock Extra points if you can think of a way to not make this array take up excessive memory by the time the calculations are done  You can pass each prime as an individual value between producer/consumer threads instead of using a shared array  Here, you can use Semaphores and/or Mutexes to coordinate this communication  Tip: sem_trywait() might be useful here, depending your approach  Extra points if you can think of a way of keeping the producers producing (at least for a small amount of time) while all consumers are busy    Tip: the complexity of the solution is of course higher if there are multiple consumer threads. You can start by using a single consumer thread, then extending it to use multiple.\n The goal is to implement one of these options (you choose), think deeply about the rest, and then answer the following questions:\n Which approach should be faster: this one with shared memory/direct value passing or the one from the previous lab using per-thread memory? Which tradeoffs were made here in comparison with the previous version? (e.g., faster but less flexible, slower but less memory, \u0026hellip;) Are you able to keep the primes sorted in this new version? Why (not)? If not, can you think of a way to still sort the results afterwards? Explain this in terms of parallel vs sequential phases of a program.\n   Go to the Deadlock Empire website and complete these two exercises:\n The Final Stretch: Dragonfire The Final Stretch: Triple Danger   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/",
	"title": "7: CPU scheduling",
	"tags": [],
	"description": "",
	"content": " Chapter 7 7.1: Scheduling algorithms\n (Don\u0026rsquo;t) Interrupt me !! Scheduler algorithms  FCFS SJF  Preemptive scheduling  Priority-based scheduling Round-Robin scheduling   7.2: Scheduling algorithms (lab)\n7.3: Towards real-world schedulers\n Dispatching Time slice size  The time slice is 10 ms The time slice is 100 ms What does it all mean ?  Real-world schedulers  Run queue - Hey, you there, get in line !!! Multi-level feedback queue Linux Scheduler   7.4: Towards real-world schedulers (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/",
	"title": "8: The Stack &amp; The Heap",
	"tags": [],
	"description": "",
	"content": " Chapter 8 The Stack \u0026amp; The Heap "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/",
	"title": "9: Memory Management",
	"tags": [],
	"description": "",
	"content": " Chapter 9 9.1: Memory management\n The flat earth of Arduino Let\u0026rsquo;s share Address Binding The memory-management unit Segmentation  9.2: Scheduling algorithms (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/",
	"title": "X: Capita Selecta",
	"tags": [],
	"description": "",
	"content": " Chapter X X.1: File systems\n Introduction in Unix filesystems  Files in Unix File attributes  File system (FS) flavors  EXT (Extended File System) FAT (File Allocation Table) NTFS (New Tech. File System) NFS (Network File System)  Common File System techniques  Fragmentation Transactions Journaling Compressing  More Resources  X.2: RTOS\n Operating Systems Real-time Operating Systems  FreeRTOS  Outdated ?  X.3: Device drivers\n General  Where do they live ?  Character device drivers Block device drivers  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/",
	"title": "A. Exercises Assignments",
	"tags": [],
	"description": "",
	"content": " Part A Exercises - Assignments This part contains exercises for students to complete and return to the teaching staff. These will be graded and make up for a portion of the total grading of this course. Every few weeks, new exercises will be made available, so be sure to check out this section now and then.\nThis is an individual assignment. Working together on the same code base is not permitted, and plagiatism will be punished heavily.\n Submitting your exercises Exercises can be submitted via Toledo, when assignments are available on the platform.\nFile submission format: lastname_firstname_[assignmentnr].zip. E.g. for assignment 1: groeneveld_wouter_1.zip. Do not submit single uncompressed files, always adhere to the zip extension.\nDeadlines are also visible on the assignment page via Toledo. Usually, when instructors announce the exercise that has to be submitted, the deadline will be in that same week, on a day specified in the lab (and on toledo), at 12H PM midnight. Be sure to double check the deadline date at Toledo.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/",
	"title": "B. Appendix",
	"tags": [],
	"description": "",
	"content": " Part B Appendix Recommended reading, instructions, etc\u0026hellip;\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/reading/",
	"title": "1. Recommended reading",
	"tags": [],
	"description": "",
	"content": " Operating Systems Operating System Concepts  Another defining moment in the evolution of operating systems Small footprint operating systems, such as those driving the handheld devices that the baby dinosaurs are using on the cover, are just one of the cutting-edge applications you\u0026rsquo;ll find in Silberschatz, Galvin, and Gagne\u0026rsquo;s Operating System Concepts, Seventh Edition.\n Website: os-book.com\nProgramming in C The (Ansi) C Programming Language  This book is meant to help the reader learn how to program in C. It is the definitive reference guide, now in a second edition. Although the first edition was written in 1978, it continues to be a worldwide best-seller. This second edition brings the classic original up to date to include the ANSI standard.\n Ebook\nExpert C Programming  C programming is a craft that takes years to perfect. A reasonably sharp person can learn the basics of C quite quickly. But it takes much longer to master the nuances of the language and to write enough programs, and enough different programs, to become an expert. This book explains quirks, weird syntax, and more \u0026ldquo;deep C secrets\u0026rdquo;.\n Ebook\nTechniques for memory safety in C  In this essay, first temporal memory errors will be explained. After that the three main techniques to fully prevent these errors will be ex- plored by means of example. Furthermore for each technique the compatibility, with legacy code, and the performance will be discussed to provide a overview of the major advantages and disadvantages.\n Ebook\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/installing/",
	"title": "2. Linux Installation Instructions",
	"tags": [],
	"description": "",
	"content": " The exercises for this course are to be done on the Linux Operating System (OS). For those unfamiliar, Linux is an open source OS that comes in many different forms called distributions. Linux is used extensively worldwide and you will need to use it many times in your future career.\nWe would of course not ask you to remove Windows to install Linux on your personal machine and installing them side-by-side (called dual boot) is also a hassle. So for this course we use a third option, called a \u0026ldquo;virtual machine\u0026rdquo;. We will install Linux inside of your Windows OS through an additional program called VirtualBox, which will make Linux think it\u0026rsquo;s running on normal hardware.\nNotes:\n If you are already using Linux on your laptop, you can skip steps 1-4 below, but make sure you do steps 5-6! If you are using MacOS, you should still install VirtualBox. Even though MacOS also runs on a flavor of UNIX, they are not fully compatible. If you are using MacOS 11 (Big Sur) on a M1 Apple architecture (ARM64 - click on \u0026ldquo;apple\u0026rdquo; - \u0026ldquo;about\u0026rdquo;: does \u0026ldquo;Chip\u0026rdquo; say \u0026ldquo;Apple M1\u0026rdquo;?), then you\u0026rsquo;re out of luck. VirtualBox does not run under the M1 chip, but QEmu does. See https://www.sevarg.net/2021/01/09/arm-mac-mini-and-boinc/ - although beware and follow it at your own risk!  Recipe for installing the OSC virtual machine  Download and install VirtualBox\n Download Linux\n There are many types of Linux available. We will use either Ubuntu or Bodhi (which is a more lightweight, smaller version of Ubuntu)  download Ubuntu (choose the Ubuntu 20.04.2 LTS, filename ubuntu-20.04.2-desktop-amd64.iso) (2.7GB image, will require 10GB disk space) OR download Bodhi (choose the Standard release, filename bodhi-5.1.0-64.iso) (800MB image, will require 6GB disk space)   Prepare VirtualBox\n For this first part, we will follow an existing guide  Note that this is for an older version of Ubuntu, but everything is the same up until where you need to actually install Linux (the \u0026ldquo;Install Ubuntu\u0026rdquo; heading), which is where you go to step 4 below  Follow the guide until the \u0026ldquo;Install Ubuntu\u0026rdquo; Heading, taking into account these changes/options:  Memory size:  Best to choose at least 2048MB of RAM for the Virtual machine  Disk size:  For Ubuntu: choose a disk size of at least 10GB For Bodhi: choose a disk size of at least 6GB For both: you may choose the \u0026ldquo;Dynamically allocated\u0026rdquo; disk for this course if you don\u0026rsquo;t have much disk space on your machine    Install Linux\n In theory, you can run Linux without installing it (the \u0026ldquo;Try Ubuntu\u0026rdquo; option or the default state that Bodhi starts in). However, you would loose all files every time you shutdown the Virtual machine, so we recommend installing it proper for the duration of this course For Ubuntu:  Ubuntu starts into a selection screen where you can choose to either Try Ubuntu or install it. Choose install. Follow the default installation instructions in the wizard (similar to the guide we\u0026rsquo;ve been following) You can safely erase the disk: this only erases the virtual disk, not your real hard drive You do NOT need to choose additional encryption  For Bodhi:  Bodhi starts into a selection screen with several options. You need to be fast, because after a while it auto-selects the first option (= try without installing). Instead, use the arrow keys to choose \u0026ldquo;Install Bodhi\u0026rdquo; Follow the default install wizard, choosing the default options You do NOT need to choose additional encryption  After installation, reboot and login with your created account  Prepare GCC\n For many exercises in this course, we will use the \u0026ldquo;GNU Compiler Collection\u0026rdquo; (GCC) to run our C program code. We will do this from the \u0026ldquo;terminal\u0026rdquo; or \u0026ldquo;command line\u0026rdquo;, a textual interface. This comes pre-installed in Linux. Look for an icon like this one, which should open a screen that looks a bit like this one. To check if GCC is installed (it should be), type the following command in the terminal and hit enter: gcc -v  the output should be a list of details of the GCC program, and should end with a line starting with \u0026ldquo;gcc version\u0026rdquo; if this is not the case and you get an error message saying GCC is not installed, please execute the following command sudo apt update \u0026amp;\u0026amp; sudo apt install -y gcc (it will ask you for your password) and try again   Get the course files\n For some exercises, we provide some basic code to help you get started This code is hosted in a GitHub repository. If that sounds like Chinese to you, no worries: the Software Engineering course will explain all about Git and version control soon For now, you just need to execute following commands in the terminal:  Install git: sudo apt update \u0026amp;\u0026amp; sudo apt install -y git Download the course files: git clone https://github.com/KULeuven-Diepenbeek/osc-exercises.git course-files  This will create a new directory named \u0026ldquo;course-files\u0026rdquo; containing the necessary items     You should now be ready to follow along with the classes!\nFAQ  [WINDOWS] The screen looks very interlaced  Settings \u0026gt; Display Graphics Controller VBoxVGA -\u0026gt; VMSVGA  I want to change the keyboard layout!  (https://www.bodhilinux.com/w/configuring-your-new-bodhi-system/ Go to Menu \u0026gt; Applications \u0026gt; System Tools \u0026gt; System Settings \u0026gt; Keyboard Layout to open the Keyboard Layout dialog box. Click your language from the list and hit Apply Selected.)  I want to be able to copy-and-paste text between my Host (my own computer) and the Guest (Linux ISO)!  This is not possible without the installation of an additional package: https://askubuntu.com/questions/22743/how-do-i-install-guest-additions-in-a-virtualbox-vm/22745#22745  I want to install something else such as my own favorite editor!  sudo apt install [package]  I want to change the resolution of the VirtualBox display  This is sadly non-trivial with VirtualBox, where by default you can either choose the small/windowed view (default), or a fullscreen view from the menu. To choose a dynamic resolution, you need to install the \u0026ldquo;VirtualBox Guest Additions\u0026rdquo; Boot the Virtual Machine and login In the VirtualBox menu on top, choose \u0026ldquo;Mount Guest Additions CD Image\u0026rdquo; (bottom option) This should allow you to auto-install the necessary software. After this, reboot the VM and you should be able to choose new screen sizing options in the VirtualBox View menu  Sometimes this step requires additional software to be installed first. This typically does the trick: sudo apt install -y make perl. After this, reboot the VM and re-Mount the CD image.     * This can be verified with the command **lsusb**. If the Arduino is not found, it should not be present in the result of this command:   * In the bottom right of your VirtualBox machine, you can give permission to your Arduino on the VM:   * The VM should now be able to see the Arduino. Again ... verify with **lsusb**   -- "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/cheat_sheet/",
	"title": "Cheat sheet",
	"tags": [],
	"description": "",
	"content": " This cheat sheet summarises a set of commands, so you don\u0026rsquo;t have to Google everything.\nLinux commands Below is a brief list of Linux commands. Remember that you can get more info on a command by typing \u0026ldquo;[command] -h\u0026quot;, \u0026ldquo;man [command]\u0026ldquo;, or even google the manpage on Google.\nNavigation    command goal     cd change directory   pwd \u0026lsquo;present working directory\u0026rsquo;   ls list (show content of current directory) - params: -la   mkdir create a directory   history show a history of used commands. Re-execute with ![number].   clear clears all text from terminal window    CLI tools    command goal     rm remove a file/directory - params: -rf   touch create a file   cat print raw file contents to terminal output   grep search for content in a file   vi powerful editor with a steep learning curve (FYI: exiting is through: \u0026ldquo;:q!\u0026ldquo;)   chmod change modifiers of a file (r/w/x/d) - params: +/-[modifier]   time time a simple command or give resource usage    Compiling    command goal     gcc GNU C Compiler - params: -c, -o, \u0026hellip;   make Execute a Makefile - params: [target]    System administration    command goal     lsusb list USB devices   ps report a snaptshot of the current processes   pstree display a tree of processes   kill send a signal to a process   bg send process to the background   fg retrieve process to the background   jobs list processes in the background    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/",
	"title": "Operating Systems and C",
	"tags": [],
	"description": "",
	"content": " Operating systems and C   source: xkcd     source: xkcd   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]