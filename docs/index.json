[
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-introos/intro-os/",
	"title": "1.1: Intro to OS using Linux",
	"tags": [],
	"description": "",
	"content": "Operating systems (OSes) are prevalent everywhere as soon as you touch anything that resembles a computer: from your smartphone, video game system interface, to the digital dashboard of your car. You, as a user, will probably never directly interact with the \u0026ldquo;bare metal\u0026rdquo;\u0026mdash;the hardware. Even as a programmer, you\u0026rsquo;ll always have to go through the operating system and ask for permissions and so forth. In this chapter, we\u0026rsquo;ll have our first look at how this works.\nThe image below shows the classic picture when introducing OSes. The user never talks directly to hardware (or the OS), but always to the software.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A[User] B[Software] C[Operating System] D[Hardware] A -- B B -- A B -- C C -- B C -- D D -- C  Giving a single, clear definition of what an operating system is, is no simple job. The OS is basicaly a layer of software that is responsible for a number of things. Mainly, but certainly not exclusively, these tasks include:\n Allowing separate independent programs to use the hardware (ideally at the same time); Directly accessing the hardware; Hiding most of the complexity of the computer for the user and user-space software; Guaranteeing that different tasks are isolated and protected (from each other); security in general; \u0026hellip;  One could say an OS is an abstraction layer that makes it easier to write software that interfaces with different types of hardware and that ensures a measure of robustness and security. By providing common services for computer programs, the OS negotiates between different layers: the hardware and the user application.\nDifferent OSes exist for different computing platforms.\n Laptops, desktops, and serversOn laptops, desktops, and servers, the most well known operating systems are used. These include: Microsoft's Windows, Linux, and MacOS. It goes without saying that there are many more operating systems for these platforms, but some/many of them are fairly unknown to the wider public. These might include: DOS, BeOS, BSD, Unix, Solaris, SunOS, ...   Source: imimg.com       Source: skyfilabs.com    Embedded systemsEmbedded systems come in many flavours, colours and sizes. Typically, these devices are smaller and have fewer features than the laptops and co do. It goes without saying that the OSes that run on embedded systems are different to, or at least ported from, the other OSes. A number of OSes for embedded systems are: Android, FreeRTOS, Symbian, mbedOS, and brickOS.  In recent years, the distinction between these types of OSes has started to fade, with systems like Android and iOS becoming more full-featured, and even low cost hardware systems like Raspberry PIs being able to run a full Linux OS.\nWhich operating system is the best ?\n  Perhaps a more interesting question:\nDo you know which flavor of OS runs on:\n An Evercade system that emulates older games? The original Xbox and Xbox 360? What about the later Xbox One X/S? The Nokia 500 smartphone? Your average Virtual Private Server? (trick question) the Game Boy Advance? (trick question) An arcade system that hosts Street Fighter II?    Types of operating systems The image above showed that the OS places itself between general software and hardware. In the most inner core of an OS resides the kernel, the heart of the OS. Depending on the type of the kernel, a typical classification of OSes can be made:\n  source: Wikipedia. (IPC: Inter-Process communication. VFS: Virtual Filesystem Switch)     A Monotlithic kernel is a kernel that runs the complete OS in kernel space. Linux is a example of an OS that uses a Monolithic kernel.\n  A Microkernel is a kernel that runs the bare minimum of an OS in kernel space. FreeRTOS is a example of an OS that uses a Microkernel.\n  A Hybrid kernel is a type of kernel that is a combination of the two types above. No surprise there, right? Windows 10 is an example of an OS that uses a Hybrid kernel\n  In the definition the Microkernel it states that it runs the bare minimum of software. Generally this contains the following mechanisms:\n Task management/Scheduling: the ability to run multiple programs/tasks on the same hardware at (apparently) the same time. Inter-Process Communication (IPC): the ability for two different programs to communicate with each other. Address space management: manages how the available (RAM) memory is divided between programs.  These three mechanisms form the core of an OS and will be elaborated on in the remainder of this course.\nSystem calls The area marked in yellowÂ  in the figure above is called the user mode. The area marked in red  in the figure above is called the kernel mode or the privileged mode. A program in user mode has no direct access to the hardware or to the entire memory space: it has to go through the kernel first.\nWhen the user, or the software on the user\u0026rsquo;s behalf, needs something from the more privileged world, the border between the user mode and kernel mode needs to be crossed. This is done using special functions that facilitate this, called System Calls. A list of all the system calls in a Linux operating system can be found here. These include methods to start a new program, send a message to a different program, read from a file, allocate memory, send a message over the network, etc.\nThese System Calls form the API (Application Programming Interface) between the higher-level software/applications in user mode (what you will typically write) and the lower-level features and hardware in kernel mode.\nLinux Since there are many OSes, we can of course not discuss them all. Most are however very similar in concept and in the basic systems they provide. As such, we will mainly focus on explaining these basic mechanisms in this course (see above).\nStill, to be able to get some hands-on experience with these systems, we need to choose one OS: the Linux OS. We do this because it is a very advanced and stable OS that is used extensively worldwide, and because it is open source (meaning that, unlike Windows and MacOS, we can view all the code in the kernel and even change it).\nThe Linux kernel, available at kernel.org, almost never comes on its own but is packaged in a distribution (a.k.a. a \u0026ldquo;distro\u0026rdquo;). Such a distro is the combination of various pieces of software and the linux kernel itself. A distro can for example include a Graphical User Interface (GUI) layer, Web browsers, file management programs etc.\nAs such, there is a large number of Linux distributions available, which mainly differ in the additional software they provide on top of the Linux kernel (which is pretty much the same across distros). Which one to pick was (and probably is) the start of multiple programmer wars, as everyone has their own preference. Our recommendation is to take into consideration what you want to use it for. For example when using Linux on a Web server, you probably don\u0026rsquo;t need a full GUI or a Web browser, which is different from when you want to use it as your main OS on your laptop.\n  source: Wikimedia   If the figure above doesn\u0026rsquo;t contain a distribution to your liking you can always Do It Yourself: Linux from scratch. Happy compiling !!\nOne of the main aspects in which these distro\u0026rsquo;s differ is the packaging system with which it\u0026rsquo;s distributed. These packaging systems allow you to install/uninstall/\u0026hellip; your software (a bit like the Windows Store or the mobile App Stores, but with more control over individual software libraries). The people that are making distro\u0026rsquo;s take source code from main software packets and compile them using the distro\u0026rsquo;s dependencies. This is then packaged and made available for package managers to install from. Typical examples of package mangers are:\n   Name Extension Typical distro\u0026rsquo;s     dpkg .deb Debian and Ubuntu a.o.   RPM .rpm Red Hat, Fedora, and CentOS a.o.   packman .pkg.tar.xz Arch Linux, FeLi Linux a.o.    If you search for Bodhi in the image above, you\u0026rsquo;ll learn that Bodhi is based on Ubuntu, which is based on Debian. Therefore APT (Advanced Package Tool) is used. This is why we used apt install SOFTWARE_NAME when setting up our Virtual Machine.\n  Use apt to see a list of the installed packages in the VM Update the list of packages in the VM Install frozen-bubble in your VM and launch it with the frozen-bubble command Upgrade all packages to the most recently available version (this can take a while\u0026hellip; Interrupting with CTRL+C/CTRL+Z shouldn\u0026rsquo;t be dangerous, but sometimes can be!)  (tip: adding -h or \u0026ndash;help to a Linux command typically shows you the main options. Try apt -h.)\n  Note that Windows nowadays also has a decent first- and third-party package manager (Chocolatey), as does MacOS (Homebrew). These systems work exactly like the above Linux package managers and can be interacted with through the command line.\nWhere does Linux fit in with other UNIX-like OSes? Good question. The huge image above merely displays different distros of the Linux OS, but Linux originated from experimental builds of the research OS called UNIX in the sixties\u0026mdash;as does MacOS, BSD, and the like (source):\n   For more information on how UNIX fits within Linux and other OSes, see Awesome-UNIX.\nBut what about the difference between GNU/Linux and Linux? \u0026ldquo;Linux\u0026rdquo;, as it is, is just kernel.org: the kernel on its own. A great start, but not exactly an operating system. To be a complete OS, you\u0026rsquo;ll need other applications that help managing the software on top of it. Such as a \u0026ldquo;toolchain\u0026rdquo;: compilers (GCC), makefiles, text editors (Emacs), etc\u0026mdash;which we\u0026rsquo;ll use in chapter 2 and forward.\nModern Linux distros always come equipped with the GNU toolchain. The creator of GNU, Richard Stalman, envisioned a fully open source version of UNIX (GNU stands for \u0026ldquo;GNU\u0026rsquo;s Not UNIX!\u0026quot;), but by 1991, only the toolchains were done. Thanks to Linus Torvalds, the creator of the Linux kernel, finally an open source version of an OS could exist, hence GNU/Linux. The \u0026ldquo;pure\u0026rdquo; GNU kernel (GNU/Hurd on the image above) still isn\u0026rsquo;t production-ready\u0026hellip;\nConclusion: when we talk about \u0026ldquo;Linux\u0026rdquo; as a complete OS, we actually mean GNU/Linux.\nGot Root ? Most operating systems allow for multiple users to share one system and provide ways to clearly separate those users (and what they can access) from each other. Like in most OSes, Linux also has an administrative user, or super-user: the root user, who can access -everything- on the system. This highly privileged user is typically not used when doing day-to-day work, as it can be dangerous (for example, the root user can remove all files on the hard drive with a single command).\nA better way of approaching your day-to-day work on a Linux system is to use a standard user. Whenever you need a higher privilege-level, you can use sudo (Super User DO). This is a simple tool that allows a regular user to execute only certain commands as the root-user (only when needed). In your VM, your main user that you use to login with is normally also the root user, but you still need to use \u0026ldquo;sudo\u0026rdquo; to execute sensitive commands.\n  source: xkcd.com   Every user that has a login on a Linux system also automatically belongs to a group. Depending on the distribution, this group can have multiple names. On Bodhi for example, a new group is created for every user that bares the same name. Linux allows you to assign certain privileges and access rights to entire groups at a time, instead of only to individual users.\nFind the single command that can remove all files on the hard drive online (but please don\u0026rsquo;t try it!)\n  Out of fuel ? Take a Shell When a user logs in on a Linux computer, typically one of the following approaches is used:\n a login through a Graphical User Interface (GUI) a login through a Command Line Interface (CLI)  For a desktop/laptop that is running Linux, the GUI approach is typically used. An example is the Ubuntu/Bodhi Virtual Machine you use for the labs. On those systems there are terminal emulators which emulate the CLI. Many flavours of these terminals are available: gnome-terminal, xterm, konsole, \u0026hellip;\n  Example of a Terminal emulator. Source: linuxcommand.org   For embedded systems or Linux running on servers, the CLI is more appropriate. Running the Graphical User Interface requires CPU time and storage. Both are scarce on an embedded system. Since everything can be done through the command line, removing the GUI is a win-win. Additionally, it is useful to learn the CLI commands even if you normally use the GUI, because they play a large part in writing automation scripts for the OS: automation scripts typically do not indicate commands like \u0026ldquo;click button at location x,y\u0026rdquo; but rather execute the necessary CLI commands directly.\nNo idea what terminal you\u0026rsquo;re running? No problem, echo $TERM will tell you exactly that. On my MacOS, it outputs xterm-256color (using the iTerm2 terminal).\nWhen a CLI is used in Linux (or an emulator), a shell is started. The shell is a small program that translates commands, given by the user, and gives these translations to the OS. As with anything, there are many flavours of shells: sh, bash, ksh, fish, zsh, \u0026hellip; Most shells provide the same basic commands, but others allow additional functionality and even full programming environments. You can write so-called \u0026ldquo;shell scripts\u0026rdquo; (typically have the file extension \u0026ldquo;.sh\u0026rdquo;), which are mostly lists of CLI commands, that are used extensively to automate operations on Linux systems.\nOnce the shell is running and the user asks to create new processes, all of these newly create processes will have the shell as a parent process (this will become important later on).\nAs a side note: Linux is not the only OS that uses a CLI. Windows for example has multiple, including the older \u0026ldquo;Command Prompt\u0026rdquo; and the more recent \u0026ldquo;Powershell\u0026rdquo;.\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-introos/",
	"title": "1: Intro in OS",
	"tags": [],
	"description": "",
	"content": "Chapter 1 1.1: Intro to OS using Linux\n1.2: Filesystems in UNIX\n1.2: (lab) Getting your CLI-feet wet\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/intro/",
	"title": "2.1: The C programming language",
	"tags": [],
	"description": "",
	"content": "The C language is a programming language developed in the \u0026rsquo;70s to make it easier to interface with hardware. C was/is in essence nothing more than a few abstraction layers on top of assembly itself.\nA quick intro to C C is an imperative programming language. You will notice this when writing some code yourself: we write instructions as statements. These rules or statements are structured in function and struct types. There is little declarative to it, compared to other higher level languages. C\u0026rsquo;s footprint is quite small, the syntax is concise and easy to learn. Statements always express how to do things, instead of what it is doing. Increasing readability is of course important. We could for instance use #define to give meaning to a few symbols, or write clear function blocks.\nC is primarily being used in embedded system development because it is so closely related to the hardware itself. The UNIX, Windows and OSX kernels are fully written in C. The operating system of your cellphone, smartwatch or handheld all build on top of C. A huge amount of languages such as Java (JVM), Python, Ruby and PHP are first and foremost implemented in C.\n Comparison with Java import java.io.IOException; import java.nio.*; class FileReader { @Override public String read(String file) throws IOException { return new String(Files.readAllBytes(Paths.get(file))); } } class Main { public static void main(String[] args) { System.out.println(\u0026#34;reading file: \u0026#34;); System.out.println(new FileReader().read(\u0026#34;sup.txt\u0026#34;)); } } How would one go around doing something like that in C? That will become difficult as C does not have a class system! A lower level implementation could look like this:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; char* read(char* file) { FILE *filePointer = fopen(file, \u0026#34;r\u0026#34;); char *buffer = malloc(255); fgets(buffer, 255, (FILE*) filePointer); fclose(filePointer); return buffer; } int main() { printf(\u0026#34;reading file: \\n\u0026#34;); printf(\u0026#34;%s\\n\u0026#34;, read(\u0026#34;sup.txt\u0026#34;)); return 0; }  Compile the above with gcc -o fileio fileio.c.  Save some text in a file called \u0026ldquo;sup.txt\u0026rdquo;, and execute the program with ./fileio.  Congratulations on your first compiled C program!\n  There are a lot of problems with this implementation: the buffer length is hardcoded and the memory has not been released. A FileReader Java class that does everything for you simply cannot be created. As you can see it\u0026rsquo;s a lot more low-level work than Java\u0026rsquo;s one-liners like Files.readAllBytes! C does not even have the keyword new. Ouch.\nKey differences between C and a higher level language such as C#:\n C\u0026rsquo;s syntax footprint is small: no private/protected/class/interface/inheritance/bool/string\u0026hellip; No \u0026ldquo;standard\u0026rdquo; libraries. C does NOT have exceptions! It works with interrupts and error codes (return 0). C does NOT have garbage collection: you manage the memory yourself. C does NOT have a virtual machine (JVM, CLR) but gets compiled to native machine code. C code usually is full of pointer variables to manipulate memory directly. C allows for combination-integer-types (unsigned short int) C works with headers (.h) and source (.c) files. An executable file requires two steps: compiling and linking. Linking allows for mixing with assembly.  The following figure represents the Java Virtual Machine you have been using in the INF1 course (source):\n   Note that there is only one source code and byte code block. Any .class file can be executed on any Linux/Windows/MacOS machine, provided you installed the correct JVM on top of the OS, as also pictured. When you compile .c C files, they do not translate into byte code but into OS-specific binaries! That means you cannot simply share your Windows binaries with a friend that runs Linux on his or her machine.\nWhy should you learn C? Good question. A few answers:\n Take a look at the TIOBE index and guess which programming language is the single most used throughout the world. Since this course is part of an engineering curriculum, it needs to stay close to its engineering roots: the hardware (embedded systems). Controlling hardware components on a PCB chip can only be done with low level machine instructions, that are coming from a low-level programming language such as C. Of course, writing in Assembly is even more precise, but hurts just a tad bit more. This is more of an operating systems course than it is a programming course. However, to learn concepts of an OS, you\u0026rsquo;ll need a firm grasp of the basics in C.    The TIOBE Index  Basic C Hello World #include \u0026lt;stdio.h\u0026gt; int main() { int nr = 42; printf(\u0026#34;sup? %d\u0026#34;, nr); return 0; } The main() function returns a number that determines whether or not your program was executed successfully (0), else some kind of error code will be returned. printf is a function in the default IO header that we need to include, just like Java\u0026rsquo;s import.\nThe \u0026ldquo;f\u0026rdquo; of printf stands for \u0026ldquo;formatting\u0026rdquo; as you can see in the example. See Formatted output.\nWrite a program that outputs the following:  \u0026ldquo;pi isÂ 3.1415\u0026rdquo; Based on the floating-point variable pi with a value of 3.1415. The output should end with a new line and contain a tab.\n  Use functions to structure code Done with function. Blocks such as if, for, while, do are familiar and work just like in other languages:\n#include \u0026lt;stdio.h\u0026gt; void say_something_if_positive(int number) { if(number \u0026gt; 5) { printf(\u0026#34;wow, positive or what? \\n\u0026#34;); for(int i = 1; i \u0026lt;= number; i++) { printf(\u0026#34;%d \u0026#34;, i); } printf(\u0026#34;\\n\u0026#34;); } } int main() { say_something_if_positive(5); return 0; } You cannot overload functions in C, unlike in C++ and Java. That means each function name is unique:\nint yay() { return 1; } int yay() { return 0; } int main() { return yay(); // ?? } Does, depending on the compiler, not compile:\n test.c:5:5: error: redefinition of 'yay' int yay() { ^ test.c:1:5: note: previous definition is here int yay() { ^ 1 error generated.  Primitives and combinational types The C language provides the four basic arithmetic type specifiers:\n char (1 byte) int (4 bytes) float (4 bytes) double (8 bytes)  Together with the modifiers:\n signed (minus and plus signs supported) unsigned (only plus signs, greater range) short (/2 bytes) long (x2 bytes)  The table on Wikipedia lists the permissible combinations to specify a large set of storage size-specific declarations. char is the smallest addressable unit, and long double is the biggest. For instance, unsigned char gives you a range of 0 to 255, while signed char works from -127 to 127.\nActual byte sizes are dependent on the target platform - the amount of bytes given above is usually correct for a 64-BIT machine. This can be retrieved using sizeof(type).   Try this out for yourself! See the combinational types demo C file in osc-exercises/ch1_c/combinational.c\n Strings? What do you mean? Forget it: char[] or a char* pointer is the only possibility. And no, it is not as easy as in Java to handle arrays due to the way they are defined.\n#include \u0026lt;stdio.h\u0026gt;#define SIZE 10  int main() { int arr[SIZE]; for(int i = 0; i \u0026lt; SIZE; i++) { arr[i] = i * 10; } for(int j = 0; j \u0026lt; SIZE; j++) { printf(\u0026#34;array index %d has value %d \\n\u0026#34;, j, arr[j]); } char string[] = \u0026#34;hi there\u0026#34;; printf(\u0026#34;%s\u0026#34;, string); return 0; } C reserves the right amount of memory with string literals you know from Java. The string[] char array does contain 9 characters and not 8! That is because the end of the array is determined by a magical NULL terminator, \\0. That makes it easier to loop through all characters and print them - or just let printf do that for you by formatting using %s.\nIn C, strings are just concatenations of characters, terminated with \\0. For instance, string[] msg = \u0026quot;hey\u0026quot;; looks like this in memory1:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR H[h] E[e] Y[y] T{/0} H -- E E -- Y Y -- T  An elaborated example that illustrates how character sequences work, and how they are terminated with \\0, can be found at http://www.cplusplus.com/doc/oldtutorial/ntcs/.\nHandy string utility functions reside in the header file \u0026lt;string.h\u0026gt; (copying, concatenating, asking for the length, \u0026hellip;) See GNU C: String utils.\nWhat is the result of strcmp(\u0026quot;hello\u0026quot;, \u0026quot;Hello\u0026quot;)?  And of strncmp(\u0026quot;hello, world\u0026quot;, \u0026quot;hello, amazing world!!\u0026quot;, 5)?\n  Hint: man strcmp!\nOnline C compilers:\n https://rextester.com/ https://godbolt.org/ https://repl.it/ (requires logging in)  The C Language - a quick reference Actually, the ANSI C syntax fits in just two index cards - that\u0026rsquo;s how small it really is. Download the quick reference cards here. Compared to Java, C# or C++, C is very much falls in the category \u0026ldquo;easy to learn, hard to master\u0026rdquo;. See the handbook comparison of C++ and C and you\u0026rsquo;ll know what we mean.\n  The backslash (\\) is depicted as a forward slash (/) in the Figure. \u0026#x21a9;\u0026#xfe0e;\n   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-introcpu/1_vonneumann/",
	"title": "3.1 Von Neumann Architecture",
	"tags": [],
	"description": "",
	"content": "In order to properly understand some concepts later in this course (such as the C pointers, stack and heap and context switching overhead), it is useful to understand some of the basics of how a CPU works internally and how the OS (and other software) interfaces with it.\nThe ALU At its core, a CPU is hardware that enables the execution of a (limited) number of relatively simple/basic mathematical and logical operations. These operations can then be used to construct higher-level fuctionality, eventually leading to full programming language concepts like loops and functions (see later).\nA few of these possible operations, applied to one or two parameters (called operands) A and B, are listed below:\n   Mathematical Logical     Addition: A + B A AND B   Subtraction: A - B A OR B   Multiplication: A * B NOT A   Division: A / B A XOR B   Shift left: A \u0026lt;\u0026lt; B    Shift right: A \u0026gt;\u0026gt; B    Negation: -A     \rMost of these operations are typically implemented directly in hardware by a combination of logical ports (AND, OR, NOT, etc.). What that looks like exactly is discussed in your other course Digital and Electronic Circuits (DISCH). For now, the main thing we need to know is that all the operations are typically combined in a single hardware element: the Arithmetic Logical Unit (ALU).\n  The ALU (wikipedia)   \rAs the single ALU supports multiple operations, we need a way to select which operation is chosen at a given time. This is done using the operation code (opcode), which is really just a number. For example, we can define that number 0 means ADD, 1 means SUB(TRACT), 2 means MUL(TIPLY), etc.\nAdditionally, the ALU has not one but two outputs: 1) the actual result of the operation, and 2) a number of status signals. One simple example of such a signal is the carry bit, which is used when adding or multiplying binary numbers. This is also why the ALU can receive status signals as input: if the ALU for example works on 32 bits at a time, and we want to add two 64-bit numbers, the carry bit of adding the first two 32-bit parts needs to be passed onto the addition of the second two 32-bit parts. Other signals could for example be if the result was negative, positive, or zero, which helps with comparing two numbers.\nNot all possible operations are typically implemented directly in hardware, as they can be reconstructed/created by using other operations in a clever way. For example, negation (-A) can be done by using subtraction (0 - A).\nCan you find a clever way of doing the following operations?\n Multiplication by iterative addition Multiplication by using shift left (\u0026lt;\u0026lt;), division by shift right (\u0026gt;\u0026gt;) Negation by using XOR Comparison using subtraction  Answer:\r\r1. 3 * 5 = 5 + 5 + 5 \r2. shift left is multiplication by 2 (A * 4 = A  2) \r3. -A = A XOR 1 (produces ones where zeroes, and vice versa). \r4. A - B. If result == 0, A and B are equal. If result \r\rIn practice, full division is difficult to implement directly in hardware and typically uses other operations in a loop, making it a slow operation!\r\r  Memory To function the ALU needs to be passed in the operand values, but it also produces an output value that we probably want to re-use in the future (potentially in the very next calculation!). As such, we need the concept of a memory which stores values (both inputs and outputs). For now, let\u0026rsquo;s call this the RAM (Random Access Memory), indicating we can access, read and write any part of it at will.\nMemory cells are again implemented in hardware by using elements called flip-flops (or similar) and their details are out of scope for this course. It is important however that we need a way to somehow indicate which exact memory cell we want to use at a given time. After all, modern RAM memory can be several GigaBytes (GB) large, and we often only use 32 or 64 bits of that at a time to represent a given number.\nTo achieve this, most memory assigns a unique number to each group of 8 bits (a single byte) it stores. For example, the first byte in memory is assigned the number 0, the second is number 1, etc. In a 4GB RAM, there are 4,294,967,295 bytes, each of which has its own unique number.\nWe often call these unique numbers memory addresses (and later in C pointers)! Much like when using the real-life postal system, the address is used to uniquely identify a single byte so we can read/write it.\nFor example, let\u0026rsquo;s assume three different byte variables A, B and C are stored in a RAM of 256 bytes. A is at address 3 and has value 15, B is at address 0 and has value 20 and C is at address 1 and has no value yet. The other addresses are empty for now.\nThe first 8 bytes of the RAM look like this:\n\r   Address Value Comment     0x00 20 B   0x01  C   0x02     0x03 15 A   0x04     0x05     0x06     0x07      \r\rAs such, a simple program performing the operation C = A + B might look like this:\n1. Read the value of A from address 3\r2. Read the value of B from address 0\r3. Pass the value of A and B and the opcode for + to the ALU\r4. Write the output value of the operation (20 + 15 = 35) to address 1\rThe (first part of the) RAM now looks like this:\n\r   Address Value Comment     0x00 20 B   0x01 35 C   0x02     0x03 15 A   0x04     0x05     0x06     0x07      \r\rInstructions Now you might wonder: but how do we actually know we need to read A and B, perform the + and write to C? We don\u0026rsquo;t just need a way to store the operand values, we also somehow need to store the operation itself!\nThe way we do this is typically called an instruction, which specifies not just the operation we want the ALU to perform (+) but also on which values to perform it and where to put the output. For the example above, the instruction would have to encode the following four parameters to be useful:\ninstruction type, address of first operand, address of second operand, address of output\rCrucially, each of these four parameters can just be represented as a number!\nIn the original computers, these values would often be hardcoded into hardware (fixed/single program) or be set manually, for example using buttons and knobs (you can imagine how slow that was\u0026hellip;). Later computers would use so-called punch cards: thick papers with holes punched in specific locations to indicate a binary 1 or 0 value to make up the numbers.\nLater however, several scientists (Johh von Neumann and others) had a revolutionary idea: since instructions can be represented as numbers, just like the data they operate upon, instructions can be stored in the same memory used to store the data! This way, we can very easily write computers that can run arbitrary programs efficiently: just change the instructions in the memory!\nThe above example C = A + B could then look something like this:\n\r   Address Value Comment     0x00 20 B   0x01  C   0x02     0x03 15 A   0x04     0x05     0x06     0x07      \r\r   Address Value Comment     0x08 0 ADD instr. type   0x09 0x03 A address   0x0A 0x00 B address   0x0B 0x01 output address   0x0C     0x0D     0x0E     0x0F      \r\rNote: here we represent addresses as hexadecimal values and non-addresses as decimal numbers. This is just for convenience and by convention, not because it\u0026rsquo;s really needed!\nWe see that we\u0026rsquo;ve split a single instruction over 4 different bytes in memory. Conceptually, we could have tried to encode it all in a single byte as well, for example by taking the first 2 bits as an opcode, the next 2 bits as address 1, another 2 for address 2, etc. However, as you can see, that would severly limit the amount of values we could represent (2 bits can only mean 4 options (0, 1, 2, or 3) and so only 4 opcodes or 4 memory addressess\u0026hellip;). By using more memory for each, we allow a wider range of values.\nNote that 8 bits (1 byte) for an address is still very little (only allows us to address 256 bytes). Real instruction encodings typically either use more than 1 byte to represent an address or use multiple steps to load a full address (for example for a 32-bit address, first load one 16-bit part with 1 instruction, then the second 16-bit part with another instruction). Other clever tricks are used, such as using an offset from a starting address (for example when storing an array), so you don\u0026rsquo;t always need to create larger addresses. This is however not important for this course yet. Here we can just assume a simple 256-byte RAM where all addresses fit nicely into a single byte!\n This concept of the \u0026ldquo;stored program\u0026rdquo; might seem trivial to you today (after all it\u0026rsquo;s how all modern computers work), but it was quite revolutionary at the time!\nUp until now we\u0026rsquo;ve only discussed a very simple program with just one instruction. Let\u0026rsquo;s think about what happens if we have a slightly more complex program:\nC = A + B E = C - D Show what the first 16 bytes of the RAM might look like after executing this program if we encode the second instruction right after the first (so starting at address 0x0C), if D has the value 10 and if the - operation has an opcode of 3. You can choose yourself where to store D and E.\n  \r   Address Value Comment     0x00 20 B   0x01 35 C   0x02 10 D   0x03 15 A   0x04 25 E   0x05     0x06     0x07      \r\r   Address Value Comment     0x08 0 ADD instr. type   0x09 0x03 A address   0x0A 0x00 B address   0x0B 0x01 C address   0x0C 3 SUB instr. type   0x0D 0x01 C address   0x0E 0x02 D address   0x0F 0x04 E address    \r\rImmediates Up until now we\u0026rsquo;ve assumed that all the initial values we\u0026rsquo;ve used (say 15 for A and 20 for B) are somehow already present in memory. But how do they actually get there in the first place?\nYou might think of a scheme were you put every possible number somewhere in memory at boot time and then just read the necessary address if you need a fixed number. However, that would be quite inefficient!\nInstead, we can define some new instruction variants that don\u0026rsquo;t work on addresses, but on numbers instead:\n  Previous format: all addresses: for C = A + B\ninstruction type, address of first operand, address of second operand, address of output\n  New format 1: all constants: for C = 12 + 33\ninstruction type, constant value, constant value, address of output\n  New format 2: one address and one constant: for C = A + 33\ninstruction type, address of first operand, constant value, address of output\n  This means that our basic operations have 3 versions each, so we should also have different names:\n no suffix: 2 addresses (ex. ADD) d suffix (direct): 2 immediates (ex. ADDd) i suffix (immediate): 1 address, 1 immediate (ex. ADDi)  Now you can also see why the instruction number for SUB was 3 instead of 1 above! We were reserving room for extra ADD instruction variants. Now, we can say that ADD = 0, ADDd = 1, ADDi = 2, SUB = 3, SUBd = 4, and SUBi = 5;\nWe use the term immediate here for constant values that are directly encoded into an instruction, because that\u0026rsquo;s what they\u0026rsquo;re typically called in real Assembly languages. Put differently: this value is \u0026ldquo;immediately available\u0026rdquo; once we\u0026rsquo;ve read the instruction from memory; we don\u0026rsquo;t have to fetch another value from an address to be able to use it. This is because addresses are just numbers in memory, and so they can also be replaced with other numbers!\n With this we can re-write the previous example so it does not rely on values already being in memory before starting:\nC = 15 + 20 E = C - 10 \r   Address Value Comment     0x00     0x01 35 C   0x02     0x03     0x04 25 E   0x05     0x06     0x07      \r\r   Address Value Comment     0x08 1 ADDd instr. type   0x09 15 immediate value   0x0A 20 immediate value   0x0B 0x01 C address   0x0C 5 SUBi instr. type   0x0D 0x01 C address   0x0E 10 immediate value   0x0F 0x04 E address    \r\rNote that we no longer need a separate memory address for A, B or D, as their values are now encoded into the instructions themselves (see address 0x09, 0x0A and 0x0E)!\nHow would you load a hardcoded value into a memory address, using only the instructions seen so far? For example: int X = 50; where X is at address 0x07?\n\rADDd 0 50 0x07\r\r  Branching and the Program Counter For now, we\u0026rsquo;ve assumed that instructions that are sequential in memory also get executed one-by-one. Additionally, we\u0026rsquo;ve assumed we always start execution at address 0x08 (as our first instruction has always been there). However, this is of course not always the case, nor something we always want.\nA simple example would be the following C (pseudo)code:\nint C = A + 10; if ( C != 20 ) { C = 0; } int D = C - 10; In this case, we see that we can\u0026rsquo;t just encode the instructions in sequence and execute them one-by-one, as that would always do the C = 0 operation! As such, we need a way to skip one or more lines if a certain condition is true (in this case if C == 20).\nFor this reason, we will introduce the concept of a Program Counter (PC). This PC keeps track of which instruction we\u0026rsquo;re executing at any given time. Since our instructions are really just numbers in our RAM memory, the PC really just contains an address pointing to an instruction!\nLet\u0026rsquo;s use our original example without the if-test. In these cases, the PC started at a value of 0x08, executed the instruction there, and was then incremented by 4 to 0x0C, the start of the next instruction. Note that it\u0026rsquo;s incremented by 4 instead of just 1, because each instruction is spread over 4 bytes!!! As such, usually we can just do PC = PC + 4 after each instruction and the program will run as intended.\nWritten out, this is what happens with the PC during the simple program\u0026rsquo;s execution:\n- initialize the PC at address 0x08\r- load the instruction at 0x08 (and the three bytes after that)\r- execute the instruction (C = A + B)\r- PC = PC + 4. New value is 0x0C\r- load the instruction at 0x0C (and the three bytes after that)\r- execute the instruction (E = C - D)\r- PC = PC + 4. New value is 0x10\r- there is nothing at 0x10, the execution stops\rHowever, once we encounter an if-test, we need a way to change the PC to another value. In our example, the ADDi is at address 0x08, and it is executed normally. Then, the instruction for the if-test (see below) is at 0x0C. If C == 20, then we want to skip the next instruction at PC + 4 (at 0x10, which is C = 0) and instead go to the one beneath the if-test at PC + 8 (at 0x14, D = C - 10). So we want to do either PC = PC + 8, or maybe better: PC = 0x14, instead of the normal PC = PC + 4.\n\r   Address Value Comment     0x00 10 A   0x01     0x02  C   0x03  D   0x04     0x05     0x06     0x07      \r\r   Address Value Comment     0x08 2 ADDi instr. type   0x09 0x00 A address   0x0A 10 immediate value   0x0B 0x01 C address   0x0C 13 BEQi instr. type   0x0D 0x01 C address   0x0E 20 immediate value   0x0F 0x14 branch address    \r\r   Address Value Comment     0x10 3 ADDd instr. type   0x11 0 immediate value   0x12 0 immediate value   0x13 0x01 C address   0x14 5 SUBi instr. type   0x15 0x01 C address   0x16 10 immediate value   0x17 0x03 D address    \r\rAs such, you see we have created a \u0026ldquo;split\u0026rdquo; in the way the program can execute: it can always take just 1 of 2 paths, not both at the same time (either we skip 0x10 or not)! This is often compared to a tree branch, which splits off into several smaller branches and you can only follow one. Hence, in (low-level) programming, an if-test is often called a branch.\nIt should be no surprise then that the instruction to represent our if-test is called BEQ (Branch if EQual). This one is a bit different from the previous ones, since it doesn\u0026rsquo;t have a real \u0026ldquo;output\u0026rdquo; address. Instead, it has an address (0x14) that the PC should be set to if the condition (C == 20) is true. If the condition is NOT true, nothing special happens (the BEQ does \u0026ldquo;nothing\u0026rdquo;) and the PC is just increased by 4 as usual.\nWritten out, this is what happens with the PC during the if-test program\u0026rsquo;s execution if A was 10 at the start:\n- initialize the PC at address 0x08\r- load the instruction at 0x08 (and the three bytes after that)\r- execute the instruction (C = A + 10)\r- PC = PC + 4. New value is 0x0C\r- load the instruction at 0x0C (and the three bytes after that)\r- execute the instruction ( C == 20? )\r- after the last instruction, the value at adress 0x01 (C) is indeed 20, so we want to manually set the PC\r- PC = third byte of the current instruction = 0x14\r- load the instruction at 0x14 (and the three bytes after that)\r- execute the instruction (D = C - 10)\r- PC = PC + 4. New value is 0x18\r- there is nothing at 0x18, the execution stops\rNote that we here have just two variants: BEQ with 2 addresses and BEQi with an address and an immediate, as BEQd wouldn\u0026rsquo;t be very useful in a real programme (can you explain why?). The two variants look like this:\nBEQ : instruction type, address of first operand, address of second operand, address to set the PC to if condition is true\rBEQi: instruction type, address of first operand, constant value, address to set the PC to if condition is true\rNote also that we use inverse logic to the C pseudocode. There, the if-test is C != 20, while here we use BEQ to test C == 20 if we want to SKIP the contents of the if-test. As you can imagine, there are also the BNE (Branch if Not Equal) and BNEi instructions that do the opposite logic. Other similar instructions are BLT (Branch if Less Than) and BLE (Branch if Less than or Equal). Commonly, most CPUs don\u0026rsquo;t have BGT or BGE instructions (can you explain why?).\nNote that we\u0026rsquo;ve now seen many examples where the instruction type does not map directly to the ALU opcode. For example, the BEQ instruction internally probably uses a SUB to compare two values (remember from before, if A - B == 0, they were equal). As such, the CPU internally often needs to \u0026ldquo;decode\u0026rdquo; an instruction and perform some extra steps (map instruction type to an ALU opcode, fetch values from memory if they\u0026rsquo;re not immediates) before actually executing it.\n We\u0026rsquo;ve now been using two different notations interchangeably. On one hand, we have C = A + B and on the other we have ADD A B C (which is the way the instruction is encoded in the RAM). While we\u0026rsquo;re most used to the first, the latter is typically how low-level \u0026ldquo;Assembly\u0026rdquo; code is written.\nAs such, our program\nint C = A + 10;\rif ( C != 20 ) {\rC = 0;\r}\rint D = C - 10;\rCan also be written as:\nADDi A 10 C\rBEQi C 20 0x14\rADDd 0 0 C\rSUBi C 10 D\rFor a real-life example, you can execute the following command objdump -d /bin/bash | head -n 30, which shows you the first 30 lines of x86 assembly of the bash program. This assembly is quite a bit more advanced than our dummy examples and uses some different names, but the concepts are the same!\n Jumping By now, you might have noticed that our branching instructions won\u0026rsquo;t be enough to represent even slighty more complex programs such as the following if-else. With our current understanding of Assembly we don\u0026rsquo;t really know what to do with the else, so this would translate to:\n\rint C = A + 10; if ( C != 20 ) { C = 0; } else { // if C == 20  C = 50; } int D = C - 10; \r\r0x08: ADDi A 10 C\r0x0C: BEQi C 20 0x14 // the if test, jumps to the else body if equal\r0x10: ADDd 0 0 C // body of the if\r// I don't know the instruction for an \u0026quot;else\u0026quot; yet, so just do nothing?\r0x14: ADDd 0 50 C // body of the else\r0x18: SUBi C 10 D\r\r\rIf we would implement this as before, where the BEQi is essentially ignored if C != 20, this would first execute C = 0 and then ALSO the C = 50 (the else body).\nYou can see we really need to skip not just 1 line if C == 20 as before, but now we also need to skip a line if C != 20! In the latter case, we need to skip the C = 50. The naive way of doing this would be to just add the opposite branch instruction before the else:\n\r0x08: ADDi A 10 C\r0x0C: BEQi C 20 0x14 // the if test, jumps to the else if equal\r0x10: ADDd 0 0 C // body of the if\r0x14: BNEi C 20 0x1C // the else, jumps to the end if not equal\r0x18: ADDd 0 50 C // body of the else\r0x1C: SUBi C 10 D\r\r\rint C = A + 10; if ( C != 20 ) { C = 0; } if ( C == 20 ) { C = 50; } int D = C - 10; \r\rYou can see this approach is equivalent to changing the else to an if (C == 20) in C. This works, but really only in this particular case. Let\u0026rsquo;s think about what happens if the body of the if-test would look a little different, and instead of C = 0 it would do C = 20:\n\rint C = A + 10; if ( C != 20 ) { C = 20; } if ( C == 20 ) { C = 50; } int D = C - 10; \r\rint C = A + 10; if ( C != 20 ) { C = 20; } else { C = 50; } int D = C - 10; \r\rWith our approach on the left, we would ALSO execute the \u0026ldquo;else\u0026rdquo; (which is now the second if-test C == 20), while with the original code on the right, it would properly skip the else.\nWe can see that, ideally, we would have another way of specifying the else without having to repeat the original condition somehow (C != 20). What we really want is to just skip the else block entirely if the if is taken. Put differently, at the end if the if-block, we want to change the PC so that it goes to the instruction right after the else! This is something we always want to do for that if (we always want to skip the else) and thus we don\u0026rsquo;t really need to have a condition: it just needs to happen every time.\nFor these situations (change the PC every time, not just if a condition is true), we can use the JMPi (Jump immediate) instruction. If a JMPi is executed, it just sets the PC to the specified value, no questions asked! As such, the JMPi is about the simplest instruction we\u0026rsquo;ve seen yet:\nJMPi: instruction type, address to set the PC to\rWith the JMPi, our code remains simple to write but now also executes correctly:\n\rint C = A + 10; if ( C != 20 ) { C = 20; } else { // if C == 20  C = 50; } int D = C - 10; \r\r0x08: ADDi A 10 C\r0x0C: BEQi C 20 0x18 // the if test, jumps to the else body if equal\r0x10: ADDd 0 20 C // body of the if\r0x14: JMPi 0x1C // always skip the else if the if was executed\r0x18: ADDd 0 50 C // body of the else\r0x1C: SUBi C 10 D\r\r\rNote: the JMPi is conceptually part of the if-block, not the else block. The else really is just the code inside. It is the end of the if-block that jumps OVER the else code if it was executed!\nNote that there of course also exists a JMP instruction. However, that one doesn\u0026rsquo;t just encode the new value for the PC directly. Instead, it encodes an address in memory. To get the new value for the PC, we need to load the value of that memory address, which itself is also an address, and then set the PC to that value, not the immediate encoded in the instruction itself.\nThis idea that an address can point to a value that is itself another address that can point to another thing is a very powerful concept in C/C++ that allows many fancy features. In the next part, we\u0026rsquo;ll see how we can use it to implement function calls. For now however, we\u0026rsquo;ll just use the JMPi variant.\nNote finally, for completeness, that \u0026ldquo;real\u0026rdquo; Assembly typically does not work with full addresses but \u0026ldquo;offsets\u0026rdquo;. For example if the PC is currently at 0x10 and we want to JMPi to 0x18, we wouldn\u0026rsquo;t say 0x18, but rather 8, because the current value (0x10) plus that offset (8) equals the target address 0x18. Can you guess why this is the common approach?\n Clock A final aspect that is needed to make this all work is called the CPU Clock. This is a timer that runs at a given frequency (say 2-4GHz in modern processors). Each \u0026ldquo;tick\u0026rdquo; of the timer means a single instruction can be executed. Put differently: each tick the program counter is changed (either it is incremented by 4 automatically, or moved to a different address by an instruction like BEQ or JMPi).\nAs such, for a 2 GHz processor, it could execute up to 2 billion (2.000.000.000!!!) instructions per second. You can see that modern CPUs really are crazy fast.\nHowever, in real systems there are challenges that prevent real CPUs from always reaching their full potential (pipeline stalls, data and control hazards, branch prediction errors, port contention, etc.). These aspects are discussed in later courses like \u0026ldquo;Computer Architectures\u0026rdquo; and \u0026ldquo;Software Engineering for Integrated Systems\u0026rdquo;.\nVon Neumann Architecture The system and its instructions we\u0026rsquo;ve discussed form the basis of what\u0026rsquo;s called a Von Neumann architecture. As said before, this is named after John Von Neumann, who was one of the first to describe such as system.\nAdapted from wikipedia, a possible (partial) definition for a Von Neumann system is \u0026ldquo;a design architecture for an electronic digital computer with these components:\n A processing unit that contains an arithmetic logic unit (ALU) A control unit that contains a program counter (PC) Memory that stores both data and instructions External mass storage Input and output mechanisms    The high-level Von Neumann architecture (source: wikipedia)   \rA final note on safety Note again that in a Von Neumann architecture, both data and instructions are stored in memory. Additionally, there are instructions that can write/manipulate what\u0026rsquo;s in that memory.\nConsequently, you could think of a hacker that writes code that has the goal to overwrite/change other, existing code, and make it do bad/wrong things! For example, a hacker could overwrite the target address of a JMPi instruction (just place a new number at the correct address in memory) and make the else of an if-else execute their code instead of the \u0026ldquo;real\u0026rdquo; code!\nFor this reason, there are often strict separations in place between memory that stores instructions and memory that stores data (we will call them \u0026ldquo;segments\u0026rdquo; later in this course). Furthermore, instruction memory is often read-only, except in a few very special circumstances, in which there are other safety measures put in place.\nFinally, program A typically cannot access the memory of program B at all! This is one of the core responsibilities of the operating system: to make sure programs cannot access each other\u0026rsquo;s memory. Not just so they can\u0026rsquo;t read each others data, but as we\u0026rsquo;ve now seen, also so they can\u0026rsquo;t change each other\u0026rsquo;s instructions! There are exceptions to the rule, such as shared memory models, which will be explained in chapter 6: inter process communication.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-debugging/debugging-ide/",
	"title": "5.1: The Easy Way: IDEs",
	"tags": [],
	"description": "",
	"content": "1. The easy way: Debugging using an IDE Arguably one of the better integrated C/C++ IDEs out there is CLion, a toolkit from Jetbrains based on the IDEA platform you all know from IntelliJ. It has exactly the same tools and capabilities but is fully geared towards C and C++. Cross-compiling and toolchain setup is also very easy using CLion. The Figure below is a screen capture of CLion showcasing it\u0026rsquo;s integrated unit testing capabilities which we will expand upon in the coming sections.\nA quick glance at the screenshot reveals the following buttons:\n Play: Compile and Run Debug Attach to process Run tests (step through, \u0026hellip;) File management window Gutter with line numbers and possibility to add breakpoints \u0026hellip;  A short live demo of CLion\u0026rsquo;s debugging capabilities is in order here.\n CLion is not free but a 30-day trail is, and as a student you can apply for a one-year license for free using your student e-mail address. Bigger development environments like this are typically used when developing large applications with a lot of source and header files. In this course, we will not be needing that. That is why the usage of a tool like this is not needed for now.\nInstead of relying on visual debug tools like CLion, another \u0026lsquo;hard-core\u0026rsquo; commandline alternative exists for Linux: gdb (The GNU debug tool).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/processes/",
	"title": "6.1: Processes",
	"tags": [],
	"description": "",
	"content": "One of the three core tasks of an OS is the management of tasks. These different tasks are all running on the same processor. To make this magic happen some form of management is required.\nWhat\u0026rsquo;s in a process ? By definition, a process is an instance of a program. As was dicussed in the \u0026ldquo;C-portion\u0026rdquo; of this course, a basic program can be divided in multiple segments. When source code is compiled into a binary, these segments are fixed.\n text/code: is the machine code in assembly. This section is compiled for one (type of) processor(s). data: is the segment that contains initialised variables. These variables could be global, static, or external. A further distinction can be made between normal initialised variables and constants. This distinction is often referred to with read-only (ro-data) and read-write (rw-data), with the former the part where the constants reside and the latter that of the normal variables. bss: (Block Starting Symbol) this segment contains the global and static variables that are not initialised. stack: dynamic part of memory used to store typically short-lived data. See Chapter 8 heap: dynamic part of memory used to store typically longer-lived data. See Chapter 8  Let\u0026rsquo;s put these definitions to the test. A very simple program could be written as follows:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { printf(\u0026#34;hello world !\\n\u0026#34;); return 0; } As you know by now, this program can be compiled with the command below:\njvliegen@localhost:~/$ gcc -o hello hello.c jvliegen@localhost:~/$ On the CLI there is a command size that can be used to examine these segments.\njvliegen@localhost:~/$ size -B hello text data bss dec hex filename 1516 600 8 2124 84c hello jvliegen@localhost:~/$ This command shows the sizes of the sections that are discussed above. The text, data and bss sections are 1516, 600, and 8 bytes. In total this sums up to 2124 bytes, which can be written hexadecimally as 0x84C.\nOK, this makes sense :)\nRun process ! By the time you\u0026rsquo;re reading this part of the course, running the binary that was compiled above should be a walk in the park.\njvliegen@localhost:~/$ ./hello hello world ! jvliegen@localhost:~/$ When a program is started, an instance of the program is created, which we now call a process. The program\u0026rsquo;s executable file on disk typically contains only the text, bss and data segments (and not exactly those, it\u0026rsquo;s a bit more complicated, as we\u0026rsquo;ll see later). The Stack and the Heap are assigned by the OS when creating the process of the program. The OS is thus responsible for copying the program\u0026rsquo;s code into memory (as we\u0026rsquo;ve seen in Chapter 2, Assembly code instructions are also just \u0026ldquo;numbers\u0026rdquo; in memory) and allocating (or at least reserving) additional room for the stack and the heap. How this is done concretely will be discussed in chapter 9.\nSo now the program was loaded into the memory. But to actually run it as a process, we need to keep some additional state for each instance. This state is kept in the Process Control Block (PCB), which is created and stored in the kernel upon mapping an instance of the program in memory.\nProcess Control Block The Process Control Block (PCB) is a representation of the process in the OS. It is created as a task_struct object. Such a PCB is made for every process and the definition of its struct is in the kernel source.\nHOLD ON Linux is an open source OS, so does that mean we should be able to find this struct in the source code ?\nWELL YES Inspecting the sched.h file verifies:\n that we can read the source code that the task_struct is indeed there, that the PCB has a lot of parameters. The definition of the struct starts at line 737 and ends at line 1546 for Linux kernel release 6.1. As might be clear from this code (or even from thinking about the number of lines of codes), this struct is rather large. A (very) small subset of the fields in the struct represent:   the process state (see below) the process identifier (PID), a unique number the process priority (see Chapter 7, scheduling) CPU scheduling info (see Chapter 7, scheduling) list of open files memory limits the CPU register contents (containing things like the stack pointer, local variables, return addres, etc. see end of Chapter 3) the program counter (PC)  Especially these last two parts are crucial: they store the actual execution state of the program at a given time. The PC points to the current instruction the CPU will execute. As such, you can already see how it becomes easy to for example pause a process for a while: simply stop updating the PC.\nThe PCBs of all the processes are contained in a doubly-linked list data structure in the kernel, which is headed by the mother of all processes. This process is called init and has PID 0. The doubly-linked list is commonly known as the process table.\nLet\u0026rsquo;s verify all of the above. To do that, we need to keep a process running for long enough to determine its PID and read other information. For this, we\u0026rsquo;ll adapt our \u0026ldquo;hello world\u0026rdquo; C program so it takes much more time to run:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #define DURATION_IN_MINUTES 2  int main(void) { int i; for(i=0;i\u0026lt;(DURATION_IN_MINUTES*60);i++) { printf(\u0026#34;hello world !\\n\u0026#34;); sleep(1); } return 0; } After starting the program, the PID of the ./longhello process can now be looked for using the ps command (\u0026ldquo;Process Status\u0026rdquo;). The -u flag lists only the processes started by the current user. Use -aux to get a list of all processes. Use ps -u | grep longhello to get info on only this specific process.\njvliegen@localhost:~/$ ps -u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND jvliegen 14339 0.0 0.0 30408 5500 pts/3 Ss 09:57 0:00 bash jvliegen 20162 0.0 0.0 4504 804 pts/1 S+ 11:49 0:00 ./longhello jvliegen 20244 3.0 0.0 47232 3616 pts/3 R+ 11:51 0:00 ps -u jvliegen 27675 0.0 0.0 30544 5556 pts/0 Ss+ 08:55 0:00 bash jvliegen 27700 0.2 0.5 207400 96300 pts/0 Sl 08:55 0:30 /usr/local/bin/ jvliegen 29414 0.0 0.0 30412 5504 pts/1 Ss 09:13 0:00 bash ... In the example above the PID is 20162. Now we want to get some information on this process. Remember how we said that in Linux, almost everything is a file? Well, while this process is running, Linux magically creates a folder in the /proc map that has the same name as the PID. There is a lot of metadata on the process that can be consulted here. For example, certain fields in the struct_task are reflected in this folder.\njvliegen@localhost:~/$ ls /proc/20162 attr exe mounts projid_map status autogroup fd mountstats root syscall auxv fdinfo net sched task cgroup gid_map ns schedstat timers clear_refs io numa_maps sessionid timerslack_ns cmdline limits oom_adj setgroups uid_map comm loginuid oom_score smaps wchan coredump_filter map_files oom_score_adj smaps_rollup cpuset maps pagemap stack cwd mem patch_state stat environ mountinfo personality statm  Let\u0026rsquo;s try to execute some commands here and see what they do:\n ls -al /proc/20162/cwd cat /proc/20162/sched cat /proc/20162/status cat /proc/20162/limits ls -al /proc/20162/net  (tip: the cat command (short for concatenate) can be used to read a file and display its contents.)\n  Process state One of the fields in the PCB is the process state. This value can be set to any of these values:\n New The process is being created Ready The process is waiting to be assigned to a processor Running Instructions are being executed Waiting The process is waiting for some event to occur (such as an I/O completion or reception of a signal) Terminated The process has finished execution    The different process states and their transitions    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nIn practice, a process will (very) frequently switch between the Ready/Running/Waiting states in a normal execution. This is because a single CPU can run many processes \u0026ldquo;at the same time\u0026rdquo;. However, this is not really at the same time! What actually happens is that the CPU runs process A for a little while, pauses it (goes to Ready state), runs process B for a little while (goes to Running state), pauses it, runs process C for a little while, etc. We will see how this works in more detail in Chapter 7.\nOpen files list Every process that is created has a list of open files. Even programs that do not actually work with files have this list, and it is never empty. Typically three files are open by default:\n standard input (stdin) standard output (stdout) standard error (stderr)  As you might have understood by now, these are not -actual- files on your hard disk. This is again some Linux magic, where input/output logic is represented as files, to make them easy to read/write from.\nFor example, by default, stdin is mapped to the keyboard and both stdout and stderr are mapped to the command line. These mappings can be altered and redirected, however. For example, we might want to send errors (stderr) to a file instead. Redirecting can be done and will be briefly touched in section 6.5.\nCreating processes In Linux there are two typical ways for users to create processes: using fork() or exec(). There is a third option using a system() function, but because it is less secure and less efficient then the other two, it is not discussed here. Both fork and exec create a new process, but in very different ways.\nThe fork function copies the PCB and the entire memory space (including stack and heap!) of its current process to a new PCB and memory location. The process that calls the fork() is referred to as the parent process, while the new process is its child. The child process will continue operations on the same line as the parent process (because of the copying of the PCB, the program counter is also copied!). This method is useful if you want to for example process a lot of data on multiple processors: you first get everything ready, then fork() new child processes that each deal with a part of the data. This is easier than manually starting new processes from scratch.\nThe exec function in contrast doesn\u0026rsquo;t really spawn a new process, but instead replaces itself entirely with a new program. It starts by replacing its internal \u0026lsquo;program\u0026rsquo; (read the text, data, and bss) with the new program to be executed. It then clears all its heap and stack memory and starts executing the new program from the first instruction (reset PC). Note that the new program can be a copy of the current one (essentially \u0026ldquo;starting over\u0026rdquo;), but more typically it\u0026rsquo;s an entirely new program. The main way in which Linux starts new processes is to first fork() the current process, and then to call exec() directly inside of the new fork\u0026rsquo;ed clone, replacing it with the intended new program.\nIt is mentioned earlier that the first process that is started is the init process. When the complete OS starts, the init process spawns a lot of other processes. There are processes that handle connecting to the network, processes that do logging, and so on. Using the pstree command we can see the processes in a tree, and have a visual representation of which child-parent relations exist between processes.\n  An example of the pstree command. The left image shows the result of Linux on an embedded system. The right image shows the result of Linux running on a laptop   When you start a new process from your command line (say ./longhello), how is that started internally do you think?\nVerify your hunch by using the ps -efj command and looking at the PPID column (\u0026ldquo;Process Parent ID\u0026rdquo;).\nAlternatively, you can also use the pstree -hp command.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/algorithms/",
	"title": "7.1: Scheduling algorithms",
	"tags": [],
	"description": "",
	"content": "In the previous chapter on Tasks, we\u0026rsquo;ve discussed one of the main responsibilities of an operating system: task management. Well to be fair, we have only been creating tasks and stopping or killing tasks. The necessary component that allows tasks to be run on one or multiple processors, the scheduler, is discussed in this chapter. Note that \u0026ldquo;tasks\u0026rdquo; or \u0026ldquo;jobs\u0026rdquo; can refer to either processes and/or threads.\nThe scheduler has two main responsibilities:\nChoose the next task that is allowed to run on a given processor Dispatching: switching tasks that are running on a given processor  Remember the image below? The first responsibility of the scheduler is the transition between the \u0026ldquo;ready\u0026rdquo; and \u0026ldquo;running\u0026rdquo; states by interrupting (pausing) and dispatching (starting/resuming) individual tasks:\n  The different process states and their transitions    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nWhile the image above is mainly for processes, similar logic of course exists for Threads as well, as they go through similar conceptual lifecycle phases as processes.\n For the sake of simplicity, in this chapter we assume a system which has only a single processor with a single CPU core. However, the concepts introduced here also (largely) hold for multi-core systems.\n(Don\u0026rsquo;t) Interrupt me !! When working with a single core, only a single task can be active at the same time. Say that the scheduler starts with the execution of the first task. It then has two options to determine when the next job is dispatched:\nnon-preemptive / cooperative scheduling: Let the current task run until it is finished or until it (explicitly) yields (to give up/away, to release) its time on the processor. One way of yielding is for example by using the sleep(), but also pthread_join() or sem_wait() can indicate a task can be paused for the time being. preemptive scheduling: Interrupt the current task at any time. This is possible because the PCB and equivalent TCB\u0026rsquo;s track fine-grained per-task state.  While the cooperative scheduling approach is the simplest, it also has some severe downsides. If a given task takes a long time to complete or doesn\u0026rsquo;t properly yield at appropriate intervals, it can end up hogging the CPU for a long time, causing other tasks to stall. As such, most modern OSes will employ a form of preemptive scheduling.\nIf the scheduler needs to preempt jobs after a certain amount of time (or execution ticks), it requires hardware assistance. CPUs contain a specialized timer component that can trigger an \u0026ldquo;interrupt\u0026rdquo; to the CPU after a certain amount of time has passed. This interrupt then triggers a pre-defined OS function call that handles the interrupt. In this case, the handling code will pause the current process and schedule a new one. As such, the hardware timer is a crucial piece necessary for implementing pre-emptive scheduling!\n Scheduler algorithms Independent of whether cooperative or preemptive scheduling is used, there exist many algorithms the scheduler may use to determine which job is to be scheduled next. A (very select) number of algorithms are given here.\nTo be able to reason about different scheduling algorithms, there is a need of some sort of metric to determine which approach is best. When studying schedulers, the following metrics are typically used:\n Average Throughput: the number of tasks that are executed in one second. Average Job Wait Time (AJWT): the average time that a job needs to wait before it gets scheduled for the first time (first time - creation time). Average Job Completion Time (AJCT): the average time that a job needs to wait before it has fully finished (last time - creation time). CPU efficiency (Î·CPU): the percentage of time that the processor performs useful work. Remember that every time the CPU switches between tasks, there is a certain amount of overhead due to context switching. As such, the more transitions between tasks there are, the less efficient the CPU is being used. This metric is primarily used in 7.3, as we will assume the overhead is 0 in this chapter.  FCFS A simple algorithm that a scheduler can follow is: First Come, First served (FCFS). The order in which the jobs arrive (are started) is the same as the order on which the jobs are allowed on the processor.\nThe image below shows three tasks that arrive very close to each other. The result of the cooperative scheduler\u0026rsquo;s job is shown in the image:\n  FCFS with cooperative scheduling.   Applying the first three metrics on the example above gives the following results:\nAverage Throughput:\n 3 jobs over 12 seconds =\u0026gt; 0.25 jobs / s  AJWT:\n Task 1 arrives at 0 sec and can immediately start running =\u0026gt; wait time: 0s Task 2 arrives at 0.1 sec and can start after 1s =\u0026gt; wait time: 0.9s =\u0026gt; 1s (rounded) Task 3 arrives at 0.2 sec and can start after 11s =\u0026gt; wait time: 10.8s =\u0026gt; 11s (rounded) Average Job Wait Time = (0 s + 1 s + 11s)/3 = 12s / 3 = 4s  AJCT:\n Task 1 arrives at 0 sec, waits 0s and takes 1s =\u0026gt; duration: 1s Task 2 arrives at 0.1 sec, waits 0.9s and takes 10s =\u0026gt; duration: 11s (rounded) Task 3 arrives at 0.2 sec, waits 10.8s and takes 1s =\u0026gt; duration: 12s Average Job Completion Time = (1 s + 11 s + 12s)/3 = 24s / 3 = 8s  For these examples, the decimal portion can be rounded away. It is only used to make a distinction in the order of arrival.\n SJF By looking at the FCFS metrics, we can immediately see an easy way to improve the AJWT and AJCT metrics: schedule Task 3 before Task 2!\nOne algorithm that would allow such an optimization is called Shortest Job First (SJF). With this algorithm the scheduler looks at the tasks that are in the ready state. The shortest job within this queue is allowed first on the processor.\nIf the scheduler applies the SJF algorithm on the same example, the occupation of the processor looks like shown below.\n  SJF with cooperative scheduling.   Calculate the three metrics for the result of the SJF example, above: Throughput, AJWT, and AJCT.  \nAnswer:  Throughput = 3 taken / 12 s = 0.25 jobs/s  AJWT = ( 0 s + 1 s + 2 s ) / 3 = 1s  AJCT = ( 1 s + 2 s + 12 s ) / 3 = 5s  We can see that the AJWT and AJCT metrics are indeed improved considerably for this example using SJF!    Preemptive scheduling Both of the examples for FCFS and SJF have so far been for non-preemptive/cooperative scheduling. Tasks have been allowed to run to their full completion. Let\u0026rsquo;s now compare this to preemptive scheduling, where the scheduler can pause a task running on the processor. Note that for the practical example we\u0026rsquo;ve been using, nothing much would change with preemptive scheduling: the selected next job would always be the same (either the first one started that hasn\u0026rsquo;t finished yet, or the shortest one remaining).\nAs such, let\u0026rsquo;s use a slightly more advanced example:\n  Preemptive scheduling.   For preemptive scheduling, there are again several options to determine when to preempt a running task, as here we\u0026rsquo;re no longer waiting for a task to end/yield. You could for example switch tasks each x milliseconds/x processor ticks. In our example, the scheduler preempts only when a new job comes in: it stops the currently running job and starts the most recently added job.\nIn the example above the following actions are taken:\n @ 0s, there is only one job: T2 =\u0026gt; schedule T2 @ 2s, there are two new jobs (T1 and T3) and one old (T2) =\u0026gt; schedule T1 (or T3) @ 3s, T1 (or T3) is done; there is one old job (T3 (or T1)) and one even older (T2) =\u0026gt; schedule T3 (or T1) @ 4s, T3 (or T1) has finished; there is one older (T2) =\u0026gt; schedule T2 @ 12s, T2 has finished; there are no more jobs  As such, this example demonstrates a sort-of Last-Come-First-Served (LCFS) approach.\nCalculate the three metrics for the result of the preemption example, above: Throughput, AJWT, and AJCT.  \nAnswer:  Throughput = 3 taken / 12 s = 0.25 taken/s  AJWT = ( 0 s + 0 s + 1 s ) / 3 = 0.33 s  AJCT = ( 1 s + 12 s + 2 s ) / 3 = 5 s    Apply cooperative FCFS and SJF scheduling to the new example tasks and calculate the necessary metrics. Compare the results to the preemptive LCFS.\n  Priority-based scheduling At this point we could try a SJF approach with preemption (which here would be called shortest-remaining-time-first). Although this a perfectly fine exercise (wink), in practice estimating the duration of a job is not an easy task, as even the program itself typically doesn\u0026rsquo;t know how long it will run for! The OS could base itself on earlier runs of the program (or similar programs), or on the length of the program, but it remains guesswork. As such, SJF is rarely used in practice. In our example, it also wouldn\u0026rsquo;t be the perfect approach, since both T1 and T3 have equal (estimated) durations, and it wouldn\u0026rsquo;t help the OS to decide which should be run first. Put differently, the scheduler wouldn\u0026rsquo;t be deterministic.\nA more practical approach is priority-based scheduling. In this setup, you can assign a given priority to each task, and have jobs with higher priority run before those of lower priority. This still leaves some uncertainty/non-determinism for processes with the same priority, but it\u0026rsquo;s a good first approach.\nLet\u0026rsquo;s assume the priorities as mentioned in the image below. Try to complete the graph with the correct scheduler decision.\n  Preemptive scheduling.    As can be seen from the example above, this approach might hold a potential risk: starvation. Some jobs with lower priority (in our case T1) might not get any processor time until all other processes are done: they starve. One solution for starvation is priority ageing. This mechanism allows the priority of a job to increase over time in case of starvation, leading to the job eventually being scheduled. The actual priority thus becomes a function of the original priority and the age of the task. Again, as you can imagine, there are several ways to do this priority ageing (for example at which time intervals to update the priority and by how much, or by how/if you change the priority after the task has been scheduled for its first time slot). We will later see how this is practically approached in Linux.\nRound-Robin scheduling As you can see, scheduling algorithms can get quite complex and it\u0026rsquo;s not always clear which approach will give the best results for any given job load. As such, it might be easier to just do the simplest preemptive scheduling we can think of: switch between tasks at fixed time intervals in a fixed order (for example ordered by descending Task start time). This is called Round-Robin scheduling (RR).\nAs such, RR allows multiple tasks to effectively time-share the processor. The smallest amount of time that a job can stay on the processor is called a time quantum or time slice. Typically the duration of a time slice in a modern OS is between 10 and 100 ms. All jobs in the ready queue get assigned a time slice in a circular fashion. An (unrealistic) example with a time slice of 1s is shown below:\n  Round Robin scheduling.   Calculate the three metrics for the result of the preemption example, above: Throughput, AJWT, and AJCT.  \nAnswer:  Throughput = 3 taken / 12 s = 0.25 taken/s  AJWT = ( 0 s + 1 s + 2 s ) / 3 = 1 s  AJCT = ( 10 s + 11 s + 12 s ) / 3 = 11 s    As can be seen from the example above, RR also has some downsides. While each task gets some CPU time very early on (low AJWT), the average completion time (AJCT) is of course very high, as all tasks are interrupted several times. If we were to compute CPU efficiency, this would also be lowest here, due to the high amount of context switches between tasks.\nA preemptive scheduler does not wait until a task yields the CPU, but interrupts its execution after a single time slice (or, in previous examples, when a new task arrives). This does NOT mean however that a task cannot yield the CPU !!!   Another way of putting it is: a job can either run until the time slice has run out (this is when the scheduler interrupts the job) or until the job itself yields the processor. In practice, both of course happen often during normal executing of tasks in an OS.\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/stackvsheap/",
	"title": "8.1: The Stack &amp; the Heap",
	"tags": [],
	"description": "",
	"content": "The Stack and the Heap Program Memory  Compiled computer programs are divided into different sections, each with their own specific needs and properties. Together, they form the program memory. The following image represents these sections, from bottom to top:   text Read-only, fixed size. Contains executable instructions.   data Can be modified. Contains global or static variables that are initialized, such as static int i = 5;. Global variables are variables that live outside of any function scope, and are accessible everywhere, such as int i = 5; int main() {Â printf(\"%d\", i); }. See Section 8.3.   bss Can be modified. Contains uninitialized data, such as static int i;.   heap Dynamically growing. Contains data maintained by malloc() and free(), meaning most pointer variables. The heap is shared by all threads, shared librarys, and dynamically loaded modules in a process. Can be modified while the process is running.   stack Set size. Contains automatic variables: variables created when (automatically) entered a function, such as int main() { int i = 5; }. Can be modified manually using the command ulimit - but this cannot be modified once the process is running.      \u0026nbsp;  The Stack Besides (automatic) variables, a few more important things also live in the stack section of the program. These are the stack pointer (SP) and the \u0026lsquo;program stack\u0026rsquo; itself.\nContrary to initialized pointers, statically sized arrays within functions are also bound to the stack, such as char line[512];. This is an important distinction, since those same arrays are translated into pointers when passing through functions! You can easily remember the difference by looking at the size: if it\u0026rsquo;s var[XX], then it\u0026rsquo;s a stack variable - even if XX is not a literal but a computed value.\nThe Heap Contrary to arrays, initialized pointers are bound to the heap, such as char* line = malloc(sizeof(char) * 10) - except for pointer values that are being assigned directly with a string constant such as char* line = \u0026quot;hello\u0026quot;;. Freeing the last line would result in the error munmap_chunk(): invalid pointer.\nThe usage of malloc() and such is required if you want to reserve space on the heap. memcpy() from \u0026lt;string.h\u0026gt; makes it possible to copy values from the stack to the heap, without having to reassign every single property. Make sure to reserve space first!\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt; typedef struct Data { int one; int two; } Data; Data* from_stack() { Data data = { 1, 2 }; Data *heap_data = malloc(sizeof(Data)); memcpy(heap_data, \u0026amp;data, sizeof(Data)); return heap_data; } int main() { Data* heap = from_stack(); printf(\u0026#34;one: %d, two: %d\\n\u0026#34;, heap-\u0026gt;one, heap-\u0026gt;two); } This is called creating a deep copy, while a shallow copy creates a copy of a pointer, still pointing to the same value in memory space.\nWhat happens when you omit malloc() and simply write Data *heap_data = memcpy(heap_data, \u0026amp;data, sizeof(Data));?\n  As another side node, it is possible to resize variables on the heap, using realloc(). This is simply not possible on the stack: they cannot be resized. Also, using calloc instead of malloc initializes the allocated memory to zero instead of \u0026ldquo;nothing\u0026rdquo;. So now you know how to use malloc, calloc, realloc, and free.\nInspecting program memory in the OS Unix-like operating systems implement procfs, a special filesystem mapped to /proc, that makes it easy for us to inspect program running program state. You will need the process ID (PID) as it is the subdir name. Interesting files to inspect are:\n /proc/PID/maps, a text file containing information about mapped files and blocks (like heap and stack). /proc/PID/mem, a binary image representing the process\u0026rsquo;s virtual memory, can only be accessed by a ptrace\u0026rsquo;ing process.  We will take a closer look at these during the labs.\nMac OSX Does not have procfs. Instead, you will have to rely on commandline tools such as sysctl to query process information.\nShould I use the stack or the heap? Good question. The answer is obviously both. Use the stack when:\n You do not want to de-allocate variables yourself. You need speed (space is managed efficiently by the CPU). Variable size is static.  Use the heap when:\n You require a large amount of space (virtually no memory limit). You don\u0026rsquo;t mind a bit slower access (fragmentation problems can occur). You want to pass on (global) objects between functions. You like managing things yourself. Variable size could be dynamic.  What piece of code could be dynamic in size? Data structures, such as linked lists from chapter 4: pointers and arrays, are a good candidate for this: arrays, sets, maps, and any other form of collection can grow and shrink in size, therefore need dynamic memory mapped. Using [] will, in most cases, not suffice in the C programming language, unless you are doing something very simple.\nMemory management Freeing up space In order to create an instance of a structure and return it, you have to allocate memory using malloc() from \u0026lt;stdlib.h\u0026gt; - we now that already. In contrast with higher level languages such as Java, C requires programmes to clean up the allocaed memory themselves! This means calling free() to free up the space for future use. For instance:\nstruct Stuff { int number; }; typedef struct Stuff Stuff; void do_nasty_things() { // ...  Stuff* ugly = malloc(sizeof(Stuff)); ugly-\u0026gt;number = 10; // ... } int main() { do_nasty_things(); // other things } As soon as the method do_nasty_things() ends, ugly is not accessible anymore because it was not returned and there are no other references to it. However, after the function, memory is still reserved for it. To counter memory leaks such as these, you can do a few things:\n Keep things local, by keeping things on the stack. The Stack, for that function, will be cleared after calling it. Change Stuff* to Stuff. Free the pointer space at the end of the function by calling free(ugly).  Since this is a small program that ends after main() statements are executed, it does not matter much. However, programs with a main loop that require a lot of work can also contain memory leaks. If that is the case, leak upon leak will cause the program to take op too much memory and simply freeze.\nDo not make the mistake to free up stack memory, such as in this nice example, from the \u0026lsquo;Top 20 C pointer mistakes':\nint main() { int* p1; int m = 100; // stack var  p1 = \u0026amp;m; // pointer to stack var  free(p1); // BOOM headshot!  return 0; }  a.out(83471,0x7fff7e136000) malloc: *** error for object 0x7fff5a24046c: pointer being freed was not allocated *** set a breakpoint in malloc_error_break to debug Abort trap: 6  Let\u0026rsquo;s try and write a program that gradually increases heap memory consumption by malloc()-ing inside a never-ending loop. For brevity, add sleep(1) in-between calls (include from #include \u0026lt;unistd.h\u0026gt;). Can you see this in increase in your task manager?\n  Some operating systems provide easy to use sampling tools, such as MacOS\u0026rsquo; sampler. Go to Activity Monitor, rightclick on a process and choose \u0026ldquo;sample\u0026rdquo;. The above task outputs:\n Date/Time: 2021-04-18 20:18:48.499 +0200 Launch Time: 2021-04-18 20:18:22.379 +0200 OS Version: macOS 11.2 (20D5042d) Report Version: 7 Analysis Tool: /usr/bin/sample Physical footprint: 977K Physical footprint (peak): 977K ---- Call graph: 2891 Thread_357865 DispatchQueue_1: com.apple.main-thread (serial) 2891 start (in libdyld.dylib) + 4 [0x1a2999f34] 2891 main (in a.out) + 24 [0x10445bf08] 2891 sleep (in libsystem_c.dylib) + 48 [0x1a28c2184] 2891 nanosleep (in libsystem_c.dylib) + 216 [0x1a28c23a0] 2891 __semwait_signal (in libsystem_kernel.dylib) + 8,16 [0x1a2948284,0x1a294828c] Total number in stack (recursive counted multiple, when =5): Sort by top of stack, same collapsed (when = 5): __semwait_signal (in libsystem_kernel.dylib) 2891  The stack sample is clearly visible, but the heap is not.\nDangling pointers A second mistake could be that things are indeed being freed, but pointers still refer to the freed up space, which is now being rendered invalid. This is called a dangling pointer, and can happen both on the heap (while dereferencing an invalid pointer after freeing up space):\nint *p, *q, *r; p = malloc(8); // ... q = p; // ... free(p); r = malloc(8); // ... something = *q; // aha! , and on the stack (while dereferencing an invalid pointer after returning an address to a local variable that gets cleaned up because it resides on the stack):\nint *q; void foo() { int a; q = \u0026amp;a; } int main() { foo(); something = *q; // aha! } Garbage Collection - not happening in C\u0026hellip; The above mistakes are easily made if you are used to Java:\nvoid foo() { Animal cow = new Animal(); cow.eat(); // ... } public static void main(String[] args) { foo(); // cow instances are cleaned up for you... } This cleaning process, that automatically frees up space in multiple parts of the allocated memory space, is called garbage collecting.  And it is completely absent in C, so beware!\nWhat happens when the stack and heap collide? That is platform-dependent and will hopefully crash instead of cause all forms of pain. There are a few possibilities:\n Stack \u0026ndash;\u0026gt; heap. The C compiler will silently overwrite the heap datastructure! On modern OS systems, there are guard pages that prevent the stack from growing, resulting in a segmentation fault. Also, modern compilers throw exceptions such as stack overflow if you attempt to go outside the reserved space (= segfault). Heap \u0026ndash;\u0026gt; Stack. The malloc() implementation will notice this and return NULL. It is up to you to do something with that result.  Write a program with an infinite loop that puts stuff on the stack. What is the program output? Do the same with infinite malloc()\u0026rsquo;s. What happens now?\n  What\u0026rsquo;s a stack overflow? The stack is a limited, but fast piece of program memory, made available for your program by the OS. The keyword here is limited. Unlike the heap, it will not dynamically grow, and it is typically hard-wired in the OS. Simply keeping on adding stuff to the stack, such as calling methods within methods without a stop condition (infinite recursion), will cause a stack overflow exception, signaling that the OS prevented your program from taking over everything:\n// forward definition void flow(); void flow() { // on the stack  int x = 5; // on the stack  flow(); // keep on going } int main() { flow(); } This causes a segmentation fault, signaling that it was killed by the OS. Add printf() statements to your liking.\nHow do I know how big the stack can be on my OS? Use ulimit -a to find out: (Executed on a 2012 MacBbook Air)\n outers-MacBook-Air:ch8-stack wgroeneveld$ ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited file size (blocks, -f) unlimited max locked memory (kbytes, -l) unlimited max memory size (kbytes, -m) unlimited open files (-n) 4864 pipe size (512 bytes, -p) 1 stack size (kbytes, -s) 8192 **BINGO** cpu time (seconds, -t) unlimited max user processes (-u) 709 virtual memory (kbytes, -v) unlimited  So it\u0026rsquo;s 8.19 MB.\nInterestingly, the same command outputs something radically different on a 2021 MacBook Air with an ARM64 chipset:\n â osc-course git:(master) â ulimit -a -t: cpu time (seconds) unlimited -f: file size (blocks) unlimited -d: data seg size (kbytes) unlimited -s: stack size (kbytes) 8176 -c: core file size (blocks) 0 -v: address space (kbytes) unlimited -l: locked-in-memory size (kbytes) unlimited -u: processes 1333 -n: file descriptors 256  Although the stack size stays more or less the same, the max processes number dramatically increased. Yay for technological advances!\nCheck your stack limit with ulimit. Write a program that blows up the stack using statically sized arrays by defining more than the applied limit. Which data type did you pick? How big is your array? Why?\n  Optimizing C code Compiler flags Depending on your compiler and your target platform, the C compiler will try to optimize code by rearranging declarations and possibly even removing lines such as completely unused variables. The GNU and LLVM gcc compilers offer multiple levels of optimization that can be enabled by passing along -O1, -O2, and -O3 flags (O = Optimize). Consider the following code:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;void stuff() { char dingdong[] = \u0026#34;hello? who\u0026#39;s there?\u0026#34;; printf(\u0026#34;doing things\\n\u0026#34;); } int main() { stuff(); } stuff is not doging anything with the char array. Compile with gcc -g -O3 test.c to enable debug output and optimize. When disassembling using lldb (LLVM) or gdb (GNU), we see something like this:\n (lldb) disassemble --name stuff a.out`stuff at test.c:3: a.out[0x100000f30]: pushq %rbp a.out[0x100000f31]: movq %rsp, %rbp a.out[0x100000f34]: leaq 0x4b(%rip), %rdi ; \"doing things\" a.out[0x100000f3b]: popq %rbp a.out[0x100000f3c]: jmp 0x100000f64 ; symbol stub for: puts a.out`main + 4 [inlined] stuff at test.c:12 a.out`main + 4 at test.c:12: a.out[0x100000f54]: leaq 0x2b(%rip), %rdi ; \"doing things\" a.out[0x100000f5b]: callq 0x100000f64 ; symbol stub for: puts a.out[0x100000f60]: xorl %eax, %eax  Where is dingdong? The compiler saw it was not used and removed it. Without the -O3 flag:\n (lldb) disassemble --name stuff a.out`stuff at test.c:3: a.out[0x100000e90]: pushq %rbp a.out[0x100000e91]: movq %rsp, %rbp a.out[0x100000e94]: subq $0x30, %rsp a.out[0x100000e98]: movq 0x171(%rip), %rax ; (void *)0x0000000000000000 a.out[0x100000e9f]: movq (%rax), %rax a.out[0x100000ea2]: movq %rax, -0x8(%rbp) a.out[0x100000ea6]: movq 0xc3(%rip), %rax ; \"hello? who's there?\" a.out[0x100000ead]: movq %rax, -0x20(%rbp) a.out[0x100000eb1]: movq 0xc0(%rip), %rax ; \"ho's there?\" a.out[0x100000eb8]: movq %rax, -0x18(%rbp) a.out[0x100000ebc]: movl 0xbe(%rip), %ecx ; \"re?\" a.out[0x100000ec2]: movl %ecx, -0x10(%rbp) a.out[0x100000ec5]: movl $0x0, -0x24(%rbp) a.out[0x100000ecc]: cmpl $0xa, -0x24(%rbp) a.out[0x100000ed3]: jge 0x100000ef2 ; stuff + 98 at test.c:5 a.out[0x100000ed9]: movslq -0x24(%rbp), %rax a.out[0x100000edd]: movb $0x63, -0x20(%rbp,%rax) a.out[0x100000ee2]: movl -0x24(%rbp), %eax a.out[0x100000ee5]: addl $0x1, %eax a.out[0x100000eea]: movl %eax, -0x24(%rbp) a.out[0x100000eed]: jmp 0x100000ecc ; stuff + 60 at test.c:5 a.out[0x100000ef2]: leaq 0x8b(%rip), %rdi ; \"doing things\\n\" a.out[0x100000ef9]: movb $0x0, %al a.out[0x100000efb]: callq 0x100000f46 ; symbol stub for: printf a.out[0x100000f00]: movq 0x109(%rip), %rdi ; (void *)0x0000000000000000 a.out[0x100000f07]: movq (%rdi), %rdi a.out[0x100000f0a]: cmpq -0x8(%rbp), %rdi a.out[0x100000f0e]: movl %eax, -0x28(%rbp) a.out[0x100000f11]: jne 0x100000f1d ; stuff + 141 at test.c:9 a.out[0x100000f17]: addq $0x30, %rsp a.out[0x100000f1b]: popq %rbp a.out[0x100000f1c]: retq a.out[0x100000f1d]: callq 0x100000f40 ; symbol stub for: __stack_chk_fail  You can fiddle with options and such yourself in godbolt.org.\nInstead of bootstrapping the debugger to inspect disassembly, you can also simply dump the object contents using objdump -D (GNU) or otool -tV (OSX).\n What\u0026rsquo;s the difference between -O3, -O2, etc? These are actually shorthand for a collection of optimizations done by the GCC compiler, and are listed in full in the online GCC documentation: Optimize Options. The higher the number, the more options are enabled. -Os optimizes for size\u0026mdash;this actually reduces a few optimization options that might increase the file size of the binary. For more information, please consult the GCC documentation.\nvolatile When heavily optimizing, sometimes you do not want the compiler to leave things out. This is especially important on embedded devices with raw pointer access to certain memory mapped spaces. In that case, use the volatile keyword on a variable to tell the compiler to \u0026ldquo;leave this variable alone\u0026rdquo; - do not move it\u0026rsquo;s declaration and do not leave it out. For instance:\nint array[1024]; int main (void) { int x; for (int i = 0; i \u0026lt; 1024; i++) { x = array[i]; } } Does pretty much nothing. Compiling with -O3 results in 2 assembly instructions:\n main: xor eax, eax ret  However, if you want x to be left alone, use volatile int x; and recompile:\n main: mov eax, OFFSET FLAT:array .L2: mov edx, DWORD PTR [rax] add rax, 4 mov DWORD PTR [rsp-4], edx cmp rax, OFFSET FLAT:array+4096 jne .L2 xor eax, eax ret  That\u0026rsquo;s a big difference.\nFunction call order Another part of optimizing code is the determination of function call order. For instance, consider the following statement: x = f() + g() * h(). Which function gets called first?\nThe answer is we do not know. Do not rely on function order to calculate something! Each function should be completely independant. There should not be a global variable manipulated in f() which will then be needed in g() or h(). You can inspect disassembled code for different compilers on https://godbolt.org/. It will differ from platform to platform, and from compiler to compiler (and even from option flag to flag).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/memory/",
	"title": "9.1: Memory management",
	"tags": [],
	"description": "",
	"content": "In the previous chapters we have many times talked about (RAM) memory and (RAM) memory addresses, as well as specialized memory regions such as the stack and the heap. Up until now, we have pretended as if these concepts mapped directly onto the underlying hardware, but reality is of course a bit more complex. In this chapter, we will consider how physical memory is actually managed by the OS and the CPU.\nMemory addresses and address lengths In modern computers and embedded systems there is typically a large amount of memory available. To be able to read and write to this memory, we need a way to access individual units of storage. This is done using memory addresses, numbers that uniquely identify memory locations. In theory, we could assign an address to each individual bit of physical memory, but in practice we address groups of 8 bits (or 1 byte). This means we always read/write 8 bits at a time. This is firstly because we don\u0026rsquo;t often work with individual bits and secondly because more addresses means more overhead, as we will soon see.\nHow many individual bytes we can support depends on how long our address numbers are. For example, a relatively simple/embedded system like the ATMega 328p on the Arduino Uno, has an address length of 8 bits. Consequently, this chip is said to be an 8-bit processor. 8 bits worth of addresses gives us 28 = 256 individual memory locations of 8 bits each, so a total of 256 bytes (not exactly a lot).\nFor a long time, more complex chips and general purpose CPU\u0026rsquo;s were 32-bit. This allows for a maximum of 232 = 4.294.967.296 individual bytes to be addressed. This comes down to about 4 GigaBytes of memory. For a long time, this was sufficient, as (RAM) memory was almost always (much) smaller than 4GB. However, as we know, over time memory was made that could be much larger than that, meaning that 32-bit processors were no longer enough.\nThis is when the big switch to 64-bit processors happened. This allows for a maximum of 264, which is equivalent to 224+10+10+10+10, which means 224 terabyte or 16 exabyte. The expectation is that that will remain enough for quite a while ;)\nYou might notice that, especially for 64-bit, there won\u0026rsquo;t always be enough physical memory to back the entire addressable space. As we will see later, this is where virtual memory addresses come in. To get there however, we need to introduce a few concepts first.\nOption 1: Chunking On small systems-on-chip (SOCs) that do not run an operating system, the world of memory addresses looks flat (starting at 0x00 and ending at 0xFF). Your singular program has full and direct access to the entire memory. However, as we\u0026rsquo;ve seen many times, one of the key purposes of an OS is to provide the ability to run multiple different tasks on shared hardware. As such we somehow need to subdivide the available memory amongst these tasks.\nThe simplest approach would be to subdivide the physical memory into separate chunks, and to reserve a single chunk of contiguous memory per-process. This is the mental model that we've been using so far, where each process/thread has a base memory address where it starts, and a \"limit\" offset where it ends (highest address = base + limit). Within those addresses, it's free to do as it pleases.  The OS then mainly has to make sure that process A doesn't try to address memory of process B and vice versa. Often this is referred to as memory protection.    The complete memory space, divided to one chunk per process.     This approach can be easily implemented in hardware with two registers. When a process is scheduled, the OS loads the CPU base register and the limit register with the correct values. The CPU can then check that the addresses of all memory accesses fall within the interval of the base address PLUS the limit. If this condition does not hold, there might be an (un-)intentional error. The CPU propagates this error to the OS for handling.\n  Base address register and limit register in logical addressing.   Importantly in this design, the two CPU registers that store the base and limit values should only be written by processes from the OS\u0026rsquo;s kernel-space, for example by employing a special privileged instruction.\nAddress Binding With this simple approach, let\u0026rsquo;s consider how memory addresses can be assigned to programs/processes in practice. This is important, as each variable/function in a program needs to reside at a specific address so it can be used/called. For example, assembly doesn\u0026rsquo;t call functions by their name like C, but by the address at which that function\u0026rsquo;s instructions start. As we\u0026rsquo;ve seen earlier, in a modern CPU architecture, code is stored in the same way as data (the text/data/bss segments).\nConsider for example the following code, which includes some variables. These variables need to be somehow assigned one or more addresses in the memory for the running process:\n#include \u0026lt;stdio.h\u0026gt; int main(void) { short i; short my_array[4]; long a_long_value; ... }   The question is then for example: at which address does the value for `i` begin?      We can see that with chunking, the address space for a certain program can (and will) be different for each run of the program, as the chosen chunk won\u0026rsquo;t always be at the same place in memory. As such, we cannot assign these addresses at compile time for example, as the compiler won\u0026rsquo;t know where exactly the program will be loaded (and we cannot expect the OS to load a given program at the same place all the time). Additionally, the compiler also won\u0026rsquo;t have full information about everything the program needs: for example, a program can load external software libraries, whose details are not yet known at compile time (and are for example resolved (mostly) at link time).\nAs such, this memory mapping needs to be done at load time, when the OS takes a program and turns it into a process instance. However, for the compiler to generate assembly instructions, it still needs to know the addresses of the variables/functions, as we\u0026rsquo;ve said above! It seems we have a contradiction here\u0026hellip;\nThe way this is solved, is by having the compiler generate relative/logical/virtual addresses. The compiler for example pretends that the program will always be loaded at address 0x0 and places all variables/functions relative to 0x0. Then, when the program is executed, the addresses can just be incremented with the process chunk\u0026rsquo;s base address to get the real, physical address in RAM (so physical adress = compiler-generated/logical/virtual address + base address).\nAs such, we can see that we have two different address spaces: the logical/virtual address space (always starting at 0x0 for each process) and the physical address space (where each process starts at a different offset).\nIn practice, this can even be done in hardware outside of the CPU, where a separate chip, the so-called \u0026ldquo;Memory Management Unit\u0026rdquo; (MMU), takes care of the translation. Using virtual/logical addresses at the CPU then simply requires the use of a relocation register (which can be loaded with the chunk\u0026rsquo;s base address) to obtain the final physical address:\n  The MMU    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nProblems with Chunking Chunking and load-time address binding works, but it has some downsides. For example, we would need to know all the variables/functions that will be used at load time. While we know more about them than at compile time (as the link step is done now), we often still don\u0026rsquo;t know everything yet. This is for example the case when using dynamically loaded external software libraries (so-called \u0026ldquo;dynamically linked libraries\u0026rdquo; or DLLs), which is something you typically do want to enable broad code re-use. These DLLs can be loaded during the execution of a process as needed, which is later than the load time we\u0026rsquo;ve considered.\nA more important downside of chunking however is that we would have to assign a fixed-size chunk to each process. This isn\u0026rsquo;t so much a problem for text/data/bss/stack, but more for the heap. We don\u0026rsquo;t really know how much heap memory a program will want to use up front, and this often also depends on how the program is used (sometimes you\u0026rsquo;ll use less memory, sometimes more). You can\u0026rsquo;t also just give each program a huge amount of memory (say 1GB of heap), as that would severely limit the amount of processes you could run at the same time. As such, you would again have to rely on the programmer to somehow give an indication of program size, or for example learn it from previous executions of the program.\nFinally, there is the problem of external fragmentation. As more and more programs are loaded into memory, the available space is reduced. Luckily, as programs are stopped/done, they also release the space they took up. As such, after a while, there will be gaps in the physical memory where older programs were removed. The thing is: those gaps are not always large enough to entirely fit a newly started program! As such, even though conceptually there is enough memory free when we regard the entire memory space, there is not enough -contiguous- memory to fit a new process. This is called \u0026ldquo;external\u0026rdquo; fragmentation, as the fragmentation happens outside of the space allocated to processes. We will discuss internal fragmentation later.\n  External fragmentation with chunking   It is clear we need a more advanced setup in practice.\nOption 2: Segmentation Segmentation is a more flexible way than chunking of loading processes into memory. The concept of segmentation is pretty simple: instead of reserving a single, large chunk for each process, we will instead reserve several, smaller chunks (called segments). For example, we can have a chunk for the text section, for data, bss, and the stack. And then we can have one (or more!) segments for the heap.\nThis nicely solves the problems of chunking: we can now appropriately size segments when loading new libraries/DLLs at runtime and we don\u0026rsquo;t need to reserve huge amounts of memory for each process up-front: we can just add (and remove!) segments as needed!\nAdditionally, it provides us with extra benefits, such as the fact that the segments for a single process no longer need to be in contiguous memory blocks!. This is illustrated by the images below:\n   Continuous image: the logical address space of a process. Source: G.I.       Segmented image: the physical address space in which the process\u0026#39; chunks can be located in different areas.     This is a very important characteristic, as it helps deal with the aforementioned external fragmentation (at least up to a certain point). Even though the full process cannot fit in any given gap, its individual segments can be distributed across several gaps as needed.\n  External fragmentation is partially solved with segmentation   Address Binding Additionally, segmentation allows us to do the memory mapping not at load time (as with chunking), but also much more dynamically at execution time: segments can be created (or removed) and placed as-needed by the OS.\nHowever, placing multiple segments at various locations in the physical memory does mean it now becomes more difficult to find the correct physical address given a logical/virtual address. A single relocation register is no longer enough (as we also no longer have a simple base address). Instead, we need a segment table that tracks the different segments and where exactly they can be found in physical memory. So, instead of having a single base address and a chunk limit, each individual segment has its own base address and limit offset, all of which are stored in the segment table. Again, for performance reasons this is often done in the separate MMU chip, which might look a bit like this:\n  Segmentation hardware    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nThis also means that we no longer have a single contiguous logical/virtual address space, but instead multiple smaller ones (one for each segment). In practice, we can still represent addresses with a single number however, which secretly consists of two parts: a segment number (s) and an offset (d). The segment number (s) will be used as an index into the segment table. The value at that index indicates the base address of that segment. The offset number (d) will be used to define an offset within this segment and is simply added to the base address to get the final, physical address.\nAs with chunking, if the sum of the base address and the offset falls within the segment limit, everything is fine. Otherwise the operating system will catch this error.\nWhat, oh what, could be the name of the error that the OS produces if the sum of the base address and the offset does not fall within the segment limit ?\n  Problems with Segmentation While segmentation already improves a lot on chunking, it still suffers from a few problems.\nIn practice, external fragmentation can still easily occur in modern systems with hundreds of running processes. This is because the individual segments are not equally sized, and gaps (created by removed/stopped segments) can thus easily be smaller than what is needed for a given segment. As such, the OS needs to keep track of a list of open gaps and their size. Every time a process needs memory for a segment, the OS would need to loop over the entire gap list to determine the most appropriate one, a list which can get very long quite quickly. This overhead can get quite substantial in practice. The OS could instead optimize and look for a \u0026ldquo;good enough\u0026rdquo; gap, but this in practice only worsens external fragmentation. Other techniques can be considered, such as compaction: this would \u0026ldquo;shift\u0026rdquo; segments of running processes together in memory to fill smaller gaps, making larger gaps available. While possible, this can get highly complex, as the logical addresses would also have to be re-generated.\nAs such, even segmentation isn\u0026rsquo;t our \u0026ldquo;final solution\u0026rdquo; to this problem. But before we discuss the real deal, let\u0026rsquo;s do some exercises.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/c-integrated-ex/",
	"title": "1. Integrated C exercise",
	"tags": [],
	"description": "",
	"content": "Download the start project here: c-integrated-start.c.\nThe program represents a book, consisting out of multiple pages, of which each page contains (random) text, represented as a char[].\nThe following functions are given:\n printBook() prints all pages of the book (no need to modify this) clearBook() clears pages and frees memory (no need to modify this) createRandomBook() creates a new book consisting out of amount of pages createRandompage() creates a new page with random text main() bootstraps everything.  1. Commandline compiling Create a simplified Makefile which does the following when executing the command make:\n Clean up any left object files Compile the exercise Run the exercise  This file should also be submitted.\nPlease submit a zip file with your .c and Makefile. Make sure your make commando runs flawlessly and the file names are in order. Submitting files separately will cost you points!\n 2. Create two \u0026ldquo;books\u0026rdquo; This program makes a book of 10 pages, instances of the page struct, represented as a linked list. Each object has a random bit of text as it\u0026rsquo;s text value, stolen from the Lorem Ipsum generator (thank you, Cicero). In this exercise, you have to extend the program such that the book will be split up into two books: one with text on pages that start with a vowel, and another with text that starts with a consonant. The pages have to continue to exist, but have to be relinked. It is of paramount importance not to make copies of objects! (Remember pointers?)\nSo, output without modifications:\nprinting the list: faucibus quam. eget, elit. dui ipsum faucibus neque dui, velit. end of the list the list is empty Output with modifications:\nprinting the list: faucibus quam. dui faucibus neque dui, velit. end of the list printing the list: eget, elit. ipsum end of the list 3. Refer to previous objects Extend the structure in such a way that not only a pointer to the next page is available, but also to the previous one. Currently, the structure looks like this:\nstruct page { char text[100]; struct page* next; }; A third variable should be added called previous. Think about which of the above functions you need to modify in order to set the correct values. This is essentially creating a double-linked list instead of a single-linked one.\nChange the main() and printBook() functions such that they keep track of and print the TAIL of the list instead of the HEAD, using your newly created variable.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-introos/filesystems/",
	"title": "1.2: File systems",
	"tags": [],
	"description": "",
	"content": "Introduction in Unix filesystems Everything is a file On Linux, almost everything is a file. A file is a file. A directory is a file. An entire harddrive is a file. A UART port is a file. Even a network connection is a file! Every file is owned by one user. All other users' privileges for that file (read, write, execute) are based on the access permissions (see below).\nWorking daily with files in Unix operating systems will no doubt increase your awareness of the specific ins and outs of the Unix filesystem. A very important aspect is that everything is a file in Unix (even things that aren\u0026rsquo;t really files, like devices, network sockets and even processes!). We will for example use this to our advantage to inspect the inner workings of a process in the /proc directory later.\nA real file is the smallest possible unit of storage in a unix system. However, a filesystem is more than mere data (the file itself): it also contains relations to other files, and attributes or metadata of the file itself, such as:\n name size timestamp owner, protection parameters  A directory is a special kind of file, in which other files can be stored. It simply references to other files it contains and does not duplicate the data itself.\n  src: http://www.rhyshaden.com/unix.htm   The image above gives an overview of common Unix directory structures, starting from the root / (forward slash). Depending on the Unix flavor, there will be slight variations. For example, Linux usually has /home as user root, while OSX has /Users (capital).\nThere are also special device files, mapped at /dev, that form the bridge between the virtual OS system and the physical machine system. These device files are char or block systems that enable the OS to read data from an external device. For instance, your USB devices, listed through lsusb, can be read directly using cat /dev/ttyXX.\nHard drives are mapped using the same technique, in /dev/sdx, listed through cat /etc/fstab (mount). lsblk (list block devices) allows you to see which block devices are currently linked through files to the OS. On OSX, my mount command outputs:\n Wouters-MacBook-Air:~ wgroeneveld$ mount /dev/disk1 on / (hfs, local, journaled) devfs on /dev (devfs, local, nobrowse) map -hosts on /net (autofs, nosuid, automounted, nobrowse) map auto_home on /home (autofs, automounted, nobrowse)  Depending on the Unix flavor, the location of the device files will be different.\nFile attributes When you list all files using ls -la (list all), you see something like this:\n  src: https://homepages.uc.edu/~thomam/   You can see that each file has a large number of associated attributes. Some of which you\u0026rsquo;ll know (size, date, name), others are somewhat more linux-specific (users, groups and permissions).\nYou will notice two special/strange entries at the top of the ls -al output, named \u0026ldquo;.\u0026rdquo; and \u0026ldquo;..\u0026rdquo;. These are not real files on the disk, but rather virtual files that help navigation in the filesystem and the execution of commands. Firstly, the \u0026ldquo;..\u0026rdquo; always means \u0026ldquo;the parent of this directory\u0026rdquo;. So if you are at the path \u0026ldquo;/home/user/test\u0026rdquo; and you do cd .., you will automatically go to \u0026ldquo;/home/user\u0026rdquo;. Doing cd ../.. will go to \u0026ldquo;/home\u0026rdquo;. Secondly, the single dot \u0026ldquo;.\u0026rdquo; means \u0026ldquo;the current directory\u0026rdquo;. This comes in handy if you want to search for something in files in the current directory or copy something to where you are at that moment without having to type the entire path.\n An important attribute is listed at the start: the permission modes of a given file or directory. As we\u0026rsquo;ve discussed with sudo before, not all users or user groups have equal capabilities in a UNIX system. Each file is \u0026ldquo;owned\u0026rdquo; by a single user (typically the user that created it) and single user group (the group of the user that created it). The permission modes allow us to very finegrainedly control what each a user can do with each (group of) files (and, because everything is a file, also with each device etc.).\nThere are five different permission mode characters displayed:\n r (readable), w (writable), x (executable). - means this specific permission is disabled or this flag is not set, and d means it\u0026rsquo;s a directory. If this is -, it means it\u0026rsquo;s a normal file.  The first character is always the directory indicator, but after that we see 3 collections of 3 characters. These are a repetition of rwx with potentially also -. Each group of 3 indicates permissions for a (group of) user(s) (so 9 in total).\n The first collection indicates the permissions for the owning user. The second collection indicates the permissions for the owning user group. The thid collection indicates the permissions for all users.  For example, -rwxrw\u0026mdash;- can be divided into - rwx rw- \u0026mdash; and would mean that:\n The owner can read, write and execute the file. All users in the owner user group can read and write but NOT execute the file. No other users can do anything with the file.  For example, if a file was not created by you, and you\u0026rsquo;re not in the same user group as the creator, you\u0026rsquo;d get the permissions of the third collection.\nThe default file permission modes of new files can be set using umask, while the the permission mode of existing files can be changed using the chmod (change mode) command. For example:\n chmod +x [filepath] adds the execute permission for all three collections. chmod -x [filepath] removes the execute permission for all three collections. chmod 640 [filepath] uses \u0026ldquo;bitflags\u0026rdquo; to change all the 9 different permissions at once.  Especially the last one deserves some further explanation. Each collection of rwx can be seen as 3 bits, where a value of 0 means the permission is OFF, and 1 means the permission is ON. So 101 would mean r-x for example. Now, we don\u0026rsquo;t always want to write chmod 101111010 for example, so we use a shorter version by representing each collection of 3 bits as a number from 0-7 (the octal number system). This is what you get when you interpret the 3 binary digits as decimal numbers (for example, 110 is 6, 101 is 5, while 011 is 3). Some examples:\n chmod 640 [filepath] maps to chmod 110 010 000 which means permissions rw- -w- --- for that particular file. Thus, the current user can read and write, the user group can only write and no one can execute. chmod 777 [filepath] maps to chmod 111 111 111 which means permissions rwx rwx rwx. Put differently: everyone can do everything. This is often the \u0026ldquo;lazy option\u0026rdquo; because it\u0026rsquo;s sure to work, but also very insecure!  Of course the root user (for example, when using sudo) can always write to files, even if they just have permission 400, as it has full permission on the entire system.\nFor this course, you will primarily use chmod +x to make a script or compiled file executable so it can be run as a program (since that doesn\u0026rsquo;t always happen automatically).\n Changing the owners of the file can be done using chown user:group [filepath].\nA complete overview of the permission system, neatly summarized by Julia Evans:\n  source: drawings.jvns.ca   File system (FS) flavors EXT (Extended File System) The EXT filesystem is originally developed for Unix-like Operating Systems. Its first variant came into market in 1992. Variant by variant this has overcome the limitations like size of single file, size of volume, number of files in a folder or directory. Journaling was introduced in ext3, and extended features in ext4, such as persistent pre-allocation, delayed allocation, an unlimited number of subdirectories, checksums, transparent encryption, online defrag.\nExt4 is backwards compatible with Ext2, meaning you can safely mount an older disk formatted with the older file system using ext4 drivers. Using Ext filesystems in Windows usually requires some driver that enables mounting of these file systems (Ext2Fsd for example).\nFAT (File Allocation Table) FAT stands for File Allocation Table and this is called so because it allocates different file and folders using lookup tables. This was originally designed to handle small file systems and disks. This system majorly has three variant FAT12, FAT16 and FAT32 which were introduced in 1980, 1984 and 1996 respectively.\n  Remember this? src: http://www.youtube.com/watch?v=EHRc-QMoUE4   (Magnetic) Floppy disks that could store up to 1.2 MB and later on 1.44 MB, were ideally for this FS. FAT32 is still mostly used in pen drive and micro SDs. It can store or copy a file with a max size of 4GB (size of a single file to be stored). If the size of file exceeds 4GB it wonât copy on storage media, but its partition size was up to 8TB (size of partition on which FAT could be applied).\nNTFS (New Tech. File System) Microsoft too has moved on from FAT to NTFS with the introduction of Windows NT (nice intro video for the curious student) (New Technology - what an original name!) in 1993. This was an enhanced and more advanced version of FAT systems. All modenr Windows installations are done on NTFS. NTFS has no file size limit and no partition or volume limit.\nModern features of this brand New Technology:\n Journaling Transactions File Compression Security  Since NTFS is a proprietary technology, is has long been difficult to mount NTFS partitions using Unix systems. Read-only is no problem, but writing is another matter. Even on OSX, it still requires the use of third-party software such as Mounty.\nNFS (Network File System) File systems usually enable the storage of files on local disks. The NFS FS, Network File System, does enable us to manage files in a distributed manner, where multiple client machines can connect to one or a few servers. To be able to do that, there are two different File Systems:\n A client-side one, connected to a network layer A server-side one, connected to a network layer - with underlying effective FS    src: http://bigdata-guide.blogspot.com/2014/02/network-file-system-nfs.html   Client system calls such as read(), write() and others pass through these. That means the Virtual FS is completely transparent: no special API is required. A network protocol (TCP/UDP) through a network layer enables communication between both client and server. The server is also called the \u0026lsquo;file server\u0026rsquo;. NFS was originally developed by Sun Microsystems, and it contains much needed server crash recovery modules.\nNFS version 2 migrated from a stateful to a stateless network protocol. Statelessness means the server is unaware of what the client is doing (what blocks are being cached, for instance) - it simply delivers al information that is required to service a client request. If a server crash happens, the client would simply have to retry the request.\nCommon File System techniques Fragmentation A hard disk drive has a number of sectors on it, each of which can contain a small piece of data. Files, particularly large ones, must be stored across a number of different sectors (in file fragments). As files grow in size (for example, by altering them), these chunks drift apart, depending on the file system. in the old FAT FS, file parts are simply saved as close to the physical start of the disk as possible, and so on, becoming fragmented over time.\n  src: https://7datarecovery.com/best-registry-cleaners/   NTFS is a bit smarter, allocating free buffer space around the files on the drive. However, after a set amount of time, NTFS systems still become fragmented. The Defragmentation process then is the process of rearranging file fragments from different sectors closer to each other, to minimize disk IO, especially without solid state.\nThe EXT file system handles sectors differently. The FS scatters different files all over the disk, leaving a large amount of free space between them. When a file is edited and needs to grow, thereâs usually plenty of free space for the file to grow into. This means fragmentation will only occur if the disk space is used up from 80% and onwards.\nTransactions When programs need to make multiple changes at FS level, if one or two changes fail for some reason, no change at all should occur. That is the main premise of a transactional system, in which rollback behavior will occur when something goes wrong.\nIt\u0026rsquo;s the same principle applied to most database transactions: a transaction should be an atomic operation, meaning no change or influence can occur in-between different changes. This usually involves some kind of locking mechanism.\nWindows Vista introduced Transactions in NTFS, but the use is now discouraged. The transaction system has largely been superseded by the Journaling system.\nJournaling A definition from Wikipedia:\n A journaling file system is a file system that keeps track of changes not yet committed to the file system\u0026rsquo;s main part by recording the intentions of such changes in a data structure known as a \u0026ldquo;journal\u0026rdquo;, which is usually a circular log. In the event of a system crash or power failure, such file systems can be brought back online more quickly with a lower likelihood of becoming corrupted.\n Journaling minimizes the loss of data when hard disks crash by recording additional metadata in a circular log. EXT4 and NTFS both implement this technique.\nCompressing Some file systems, such as the open-source Btrfs, introduce transparent file compression. In NTFS, it is not completely transparent: you can enable NTFS compression by changing advanced attributes of directories in Windows Explorer. It does come at a cost: CPU power. Therefore, it should never be applied to OS-specific and frequently-modified files.\nExt4 does not have transparent file compression, but it can be enabled in conjunction with ZFS. The APFS (Apple File System), a proprietary system by Apple for High Sierra (10.13) and later, does come with auto-compression systems based on Zlib and LZVN. HFS+, Apple\u0026rsquo;s older FS, comes without it.\nMore Resources  https://kerneltalks.com/disk-management/difference-between-ext2-ext3-and-ext4/ https://www.freebsd.org/doc/handbook/network-nfs.html https://en.wikipedia.org/wiki/Ext4 https://en.wikipedia.org/wiki/Comparison_of_file_systems https://www.linux-magazine.com/Online/Features/Filesystems-Benchmarked https://selvamvasu.wordpress.com/2014/08/01/inode-vs-ext4/  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/intro-labs/",
	"title": "2.2: String Manipulation",
	"tags": [],
	"description": "",
	"content": "We are at War! We\u0026rsquo;re at war! The orcs are attacking and are looking very hungry! Look at them!\n  Orcs attacking! source: G.I.   1. String manipulation However, instead of simply killing you, these not so friendly looking beasts target vowels instead of bowels. So when speaking to, they munch and munch, stripping your carefully chosen words of all vowels. How rude. Implement a function called char* munch(char* sentence) that obscures all vowels with an \u0026lsquo;X\u0026rsquo;, and then prints the results. You will also need a int main() function.\nAssume a maximum character length of 100 for the input sentence.\nTips:\n Re-read chapter 2. How do you start writing a program in C? Create one file, create a main function, print something and compile/run to test if it works. Then expand. Will you be using scanf() or fgets() for user input? What is the difference? Look up how to use either functions. You can safely ignore the *. A char array gets converted to a pointer if returned or given as an argument. Remember, in Java, the function signature would simply be char[] munch(char[] sentence) Go through the GNU Coding standards. Methods in C are snake-cased: my_nice_method instead of Java\u0026rsquo;s camelcasing myNiceMethod.   INPUT: 'hello friendly green guys' OUTPUT: 'hXllX frXXndly grXXn gXys'  Start from this blueprint:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; char* munch(char* sentence) { char* response = malloc(sizeof(char) * 100); // TODO eat those vowels!  return response; } int main() { char sentence[100]; // TODO read input  printf(\u0026#34;INPUT: %s\\n\u0026#34;, sentence); printf(\u0026#34;OUTPUT: %s\\n\u0026#34;, munch(sentence)); } The correct use of malloc() will be explained in the coming labs.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/structs/",
	"title": "2.3: Creating order: Structures",
	"tags": [],
	"description": "",
	"content": "Structs The only way to structure data in C is using the struct keyword:\nstruct Person { int age; int gender; // no bool, remember?  char name[100]; // do not forget to add a size } We can use this structure to assign values like this:\nstruct Person jaak; // do not forget \u0026#34;struct\u0026#34; // jaak.name = \u0026#34;Jaak Trekhaak\u0026#34;; - this is too easy - won\u0026#39;t work strcpy(jaak.name, \u0026#34;Jaak Trekhaak\u0026#34;); // include \u0026lt;string.h\u0026gt; for this jaak.age = 80; jaak.gender = 1; Another way of assigning values is defining the values inline using the {}brackets:\nstruct Person jaak = { \u0026#34;Jaak Trekhaak\u0026#34;, // need to be in order of property definition  80, 1 }; The next question is, can we also define functions in a struct? Yes and no. A function pointer makes this possible, but it is not the same such as a member variable of a class in Java. C function pointers, however, can be very usefully used as callback methods:\n#include \u0026lt;stdio.h\u0026gt; struct Person { int age; int (*is_old)(); }; int is_old(struct Person this) { printf(\u0026#34;checking age of person: %d\\n\u0026#34;, this.age); return this.age \u0026gt; 60; } int main() { struct Person jaak; jaak.age = 40; jaak.is_old = \u0026amp;is_old; printf(\u0026#34;is jaak old? %d\\n\u0026#34;, jaak.is_old(jaak)); } It looks a bit weird because there is no such thing as a magical variable named this - that\u0026rsquo;s one argument you have to provide yourself. You can emulate functions as members of a data structure, but as you can see it\u0026rsquo;s going to cost you. The function pointer, or callback method, will be further explained in chapter 4: pointers.\nCompile and execute the above code. what happens when you comment out jaak.is_old = \u0026amp;is_old;? Implement another function with signature int scoff_at(struct Person p) that calls people old when they are called \u0026ldquo;Jaak\u0026rdquo;. Look at the previous example on how to expand the struct to add a name property.\n  Extra definitions Creating a person looks awkward: struct Person jaak; - why can\u0026rsquo;t we simply use Person jaak;? That is possible if you define your own types using the keyword typedef. It\u0026rsquo;s also useful to emulate your own string implementation:\ntypedef struct Person Person; typedef char* string; Magic numbers are usually defined on top, in header files, using #define. With some tricks we can emulate booleans in C:\n#define TRUE 1 #define FALSE 0  typedef unsigned short int bool; bool male = TRUE; These #define statements are preprocessor flags. These can be as simple as this example or as complex as switching on different CPU architectures and executing another set of rules depending on the outcome. Macros are expanded just before expanding, see the \u0026ldquo;compiling\u0026rdquo; section below. Intricate examples are visible at Wikipedia.\nTypical C code that you may encounter due to lack of a bool: if (result) {...} where result is an int. This is in no case the same as JavaScripts Truthy / Falsey construction! The number 0 is false. EOF, NULL or \\ 0 all evaluate to a number to use this.\nC\u0026rsquo;s By-Value VS Java\u0026rsquo;s By-Ref In C, everything you pass to functions is passed by value, meaning a copy of the value is created to pass to the called function. This is very important to grap because mistakes are easily made. For instance, by passing the Person struct, we copy it. Any changes made to the struct in the function are done ON THAT COPY:\nvoid happy_birthday(Person person) { person.age++; } int main() { Person jaak; jaak.age = 13; happy_birthday(jaak); printf(\u0026#34;%d\\n\u0026#34;, jaak.age); // HUH? Still 13? } The copy of jaak gets to celebrate, but jaak himself stays 13.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; main{main} happy{happy_birthday} jaak[Person jaak] copy[copy of jaak] jaak -.-|create copy| copy main --|push to main stack| jaak happy --|push to local fn stack| copy  To fix this, we need the use of pointers, as explained in chapter 4. In Java, every object is passed by reference, meaning it points to the same value and changes will be persistent. As expected, In Java (and in pretty much any other programming langauge) this is not the case for primitives:\npublic static void increase(int i) { i++; } public static void main(String[] args) { int i = 5; increase(i); System.out.println(\u0026#34;i is \u0026#34; + i); // still 5 } Use of header Files The #include statements ensure the correct inclusion of functions in your program. Large programs consist of multiple C (source) and H (header) files that are glued together with compiling and linking. A header file contains function definitions, the declarations are in the source files:\n// person.h  struct Person { int age; } int is_old(struct Person p); With the following source file:\n// person.c  #include \u0026lt;stdio.h\u0026gt;#include \u0026#34;person.h\u0026#34; int is_old(struct Person p) { return p.age \u0026gt; 60; } int main() { struct Person jaak; jaak.age = 10; return 0; } The  main function works as a bootstrapper and is never placed in a header file. Note the difference between brackets \u0026lt;\u0026gt; and brackets \u0026quot;\u0026quot; at include: that is the difference between system includes and own includes (use relative paths!).\nThe reason for splitting this up is that other source files also provide access to is_old() and Person and thus the ability to reuse things.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{person.h} --|source| B[person.c] B -- E[definition is_old] A{person.h} --|source| C[facebook.c] C -- F[use struct] A{person.h} --|source| D[twitter.c] D -- G[use struct]  If you create a separate header file and include them into the source files, there is no need to compile or link it separately. That is, gcc code.c still suffices. Only when you split up source code into separate source files, multiple output files will need to be compiled - and linked together (with one main() function present somewhere).\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/",
	"title": "2: Introduction in C",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Introduction to C Chapter 2 handles the following subjects:\n Intro to C: primitives, differences between C/Java Structures, char* as strings, arrays Building C source files C Ecosystems Compiling \u0026amp; cross-compiling Building header/source files Makefiles  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-introcpu/1_lab_assembly/",
	"title": "3.2: Assembly basics",
	"tags": [],
	"description": "",
	"content": "Assembly basics Exercise 1 How would you implement a while loop in Assembly using BEQi and JMPi?\nYou don\u0026rsquo;t need to write out the full RAM contents nor variable addresses, just the instruction addresses and Assembly instructions (as in exercise 4 below).\nint accumulator = 0; int n = 10; while( n != 0 ) { accumulator += 10; n--; } int a = accumulator + 5; Exercise 2 How would you implement a for loop in Assembly using BEQi and JMPi? How is this different from the while in Exercise 1 above?\nYou don\u0026rsquo;t need to write out the full RAM contents nor variable addresses, just the instruction addresses and Assembly instructions (as in exercise 4 below).\nint accumulator = 0; for( int n = 10; n != 0; n-- ) { accumulator += 10; } int a = accumulator + 5; Exercise 3 Show what the full RAM contents might look like after executing the following pseudo-C program. Show both variables (A, B, C, \u0026hellip;) and encoded instructions. You can choose yourself at which addresses you place the variables and where you start placing the instructions.\nUse the following opcodes if necessary: MUL = 6, MULd = 7, MULi = 8, DIV = 9, DIVd = 10, DIVi = 11, BEQ = 12, BEQi = 13, BNE = 14, BNEi = 15, BLT = 16, BLTi = 17, BLE = 18, BLEi = 19, JMP = 20, JMPi = 21\nA = 5 B = 10 D = 20 if ( A \u0026gt; 15 ) { B = D * 10 } else { A += 15 B = D * 10 } C = 5 + (B / A); Tip: a single line of C code might need multiple lines of Assembly code!\nExercise 4 Below, you can see a (psuedo) C program on the left. It has been translated to the Assembly on the right, but the programmer has made some major errors!\nCan you find an fix the errors in the Assembly?\n\rif ( A == 15 ) { B = 10; if ( C \u0026lt; X ) { Y = 20; } else { Z = 30; } } C = X; \r\r0x08: BEQi A 15 0x20\r0x0C: ADDd B B 10\r0x10: BLE X C 0x18\r0x14: ADDi Y 20 Y\r0x18: JMP 0x20\r0x1C: ADDi Z 30 Z\r0x20: ADDi X C\r\r\rExercise 5 Did you know that X++ and ++X have subtly different meanings in most programming languages?\nSpecifically:\n\rint A = 1; int B = ++A; // A is now 2 // B is now 2 D = B + 1; // D is now 3 \r\rint A = 1; int B = A++; // A is now 2 // B is now 1 !!!!!!! (not 2 !!!) D = B + 1; // D is now 2 !!!!!!! (not 3 !!!) \r\rAs you can see, if X++ is used in combination with an assignment (Y = X), the assignment will happen first, and only then does the increment (++) happen.\nThe opposite is true for ++X: there the increment happens first, and then the assignment is done.\nWrite both options in Assembly (using ADDi and ADDd instructions) showing clearly how they\u0026rsquo;re different.\nNote: this shows that a single statement in C can often compile to 2 or even more assembly instructions, and the implications are often subtle!\nNote: when used in while/for loops (see Exercise 1 and 2), it typically doesn\u0026rsquo;t matter if you do \u0026ndash;X or X\u0026ndash;, they both end up the same in those cases\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab1_processmgmt/",
	"title": "6.2: Processes (lab)",
	"tags": [],
	"description": "",
	"content": "   p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nCreating and inspecting processes TIP: the unistd.h header has some useful functions. You may also want to check out the wait() function in sys/wait.h and exit() in stdlib.h.\n Write a C-program that prints its own PID on the screen.    An example output    Write a C-program that spawns another process using fork(). Both parent and child processes announce their existence (through a printf) and their PIDs.    An example output    Write a C-program that creates 4 child processes using fork()  each of the childeren checks which numbers below 10000 are prime every child reports only numbers that are prime (using printf), together with its own PID before exiting, a child must announce how many prime numbers it has found in total TIP: don\u0026rsquo;t worry too much about the efficienty of calculating whether a number is prime or not. The calculation is meant to be time-consuming. Answer this question: how are the processes scheduled? Do they all execute one after the other or at the same time? How can you tell?      An example output   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/lab1_algorithms/",
	"title": "7.2: Scheduling algorithms (lab)",
	"tags": [],
	"description": "",
	"content": "  image source: unsplash.com   Let\u0026rsquo;s compare ! We have discussed a number of algorithms the scheduler can use to do its job. Let\u0026rsquo;s compare them. We assume the following tasks:\n T1: arrives @ 0s, takes 10s, and has priority low T2: arrives @ 1s, takes 2s, and has priority high T3: arrives @ 4s, takes 5s, and has priority high T4: arrives @ 7s, takes 1s, and has priority medium  Compare Average Throughput, AJWT, and AJCT of the 4 algorithms we\u0026rsquo;ve seen up until:\n cooperative (non preemptive) FCFS, cooperative (non preemptive) SJF, preemptive priority based, and preemptive round robin (with a 1s time slice)  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/inspection-labs/",
	"title": "8.2: Inspecting memory regions",
	"tags": [],
	"description": "",
	"content": "Accompanying Screencast:\n  1. No malloc, no heap Let\u0026rsquo;s look at memory regions of a process that does not call malloc(). This means we will not use the heap just yet. Compile the following code:\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main() { getchar(); // breaks process until char input  return 0; } To explore the above example, we will introduce getchar() before ending the main() loop, so the program pauses and gives us a change to look under the hood.\nInspect the memory regions of the above program while running it. Look up the process ID using ps aux and browse through the files using cat in /proc/[procid].\n  Locate the following:\n 7f8122192000-7f8122193000 rw-p 00000000 00:00 0 7fffc566a000-7fffc568b000 rw-p 00000000 00:00 0 [stack] 7fffc577d000-7fffc5780000 r--p 00000000 00:00 0 [vvar] 7fffc5780000-7fffc5782000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0 [vsyscall]  No [heap] region allocated yet. Let\u0026rsquo;s do the same, but use malloc() to allocate a random block of memory. The return value of the method is a void* that can be printed to show the address of the heap region. Use printf(\u0026quot;%p\u0026quot;, p).\nPeeking into the files in /proc, we can sometimes see the heap block not moving at all. It turns out that this is completely OS-dependant. Try to use calloc() instead and see what happens!\n 1.1 Inspecting the stack See if there\u0026rsquo;s a way to more thoroughly inspect the stack region.\n Write a small program that fills the stack with a few variables and function calls, then a blocking call to getchar() which gives you time to inspect things. Check the /proc/{process_id} folder to scour for stack information. Use the gdb GCC debugger: gdb -batch -ex bt -p {process_id}. For older 32-bit computers only: use the pstack program to print the stack trace of the running process.  Does the output make sense? Are there major differences?\nA few tips:\n Operation not permitted problems are usually solved with using sudo! If you don\u0026rsquo;t know what the arguments of a cmdline program do, use man {cmd} to find out. These are also available online in various mirrors such as linux.die.net for gdb.  Another fun exercise: write a never-ending recursive loop and increase a global int variable that gets printed. How far can you get without segfaulting? Re-run the program. Is the maximum number the same? Is this the same on your neighbor\u0026rsquo;s computer?\n2. malloc: use heap Extend the above example and allocate dynamic memory. What happens in the /proc files? Can you see how the returned address is inside the heap region?\n  Let us use the program strace to figure out what malloc() exactly does. It should be a system call that asks the Operating System to allocate a certain amount of dynamic memory. But we have yet to figure out which system calls exactly are called. The program strace is used to track system calls and signals. Don\u0026rsquo;t be surprised by the amount of calls a simple program like ours makes! Let us try to find the calls for memory allocation.\nIt is a good idea to printf(\u0026quot;before malloc! \u0026quot;) to make sense of the strace output.\nuse strace ./program. It will output a lot of text, ending with read(0, (that is our getchar() break). Can you find the syscalls we are after?\n   munmap(0x7f88a38dc000, 114791) = 0 brk(NULL) = 0x5644fb34d000 brk(0x5644fb36e000) = 0x5644fb36e000 fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0 write(1, \"0x5644fb34d260\\n\", 150x5644fb34d260 ) = 15 fstat(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 1), ...}) = 0 read(0,  Aha, brk! What is that? Use man brk to find out more.\nThe data segment gets expanded by moving the program break pointer up, so the heap is actually an extension of the data segment of the program.\n3. multiple mallocs What happens when we call malloc() multiple times in a row?\nExtend the program by allocating 1024 bytes four times in a row. Inspect the program again using strace. What do you notice?\n  Memory allocation is optimized by avoiding a system call each time. It firsts allocates more than needed that will (hopefully) suffice.\nInstead of calling malloc four times, let\u0026rsquo;s see what happens when we use a loop to see how many times brk is called. More importantly, we will also see that he heap grows \u0026lsquo;upward\u0026rsquo;!\nExtend the program by allocating 1024 bytes in a for loop (Use getchar() before and after the loop) that counts to a random high number. Print \u0026ldquo;loop\u0026rdquo; and \u0026ldquo;end\u0026rdquo; before and after the loop. Inspect again using the trace tool, and also take a look at the maps file in the process folder. After entering a key you can re-inspect everything.\n   Before loop: 556a1a947000-556a1a968000 rw-p 00000000 00:00 0 [heap] After loop: 556a1a947000-556a1aa4f000 rw-p 00000000 00:00 0 [heap]  Converted to decimal: 93914201911296 - 93914200965120 = 946176 bytes used.\nIndeed, the Figure from chapter 7.1 and our findings confirm that the heap grows upwards.\n4. Free Until now the above examples have never taken into consideration the fact that one has to get rid unused space using free(). We will leave it up to you to inspect what happens in /proc.\nExtra: Using Valgrind to inspect the memory heap    Valgrind is a useful commandline tool that makes it easy for C programmers to inspect how much dynamic (heap) memory a program actually consumed, and how much of it was freed. It is a lot easier to use than dabbling in different disassemble commands, but it does NOT come with the GNU toolchain. Use apt-get install valgrind to install it onto your virtual machine.\nLet\u0026rsquo;s assume a simple program that reserves some memory, fills in the blanks, and then frees up some space: (also available in the osc-exercises repository)\n#include \u0026lt;stdlib.h\u0026gt; int main() { int* ptr; ptr = malloc(sizeof(int) * 1000); // we allocated 4000 bytes (since an int is usually 4 bytes)  free(ptr); return 0; } After compiling this, we can let the tool figure out how much space we took up, how many leaks there were, and much more:\n Wouters-MacBook-Air:ch8-stack wgroeneveld$ valgrind ./a.out ==87742== Memcheck, a memory error detector ==87742== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al. ==87742== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info ==87742== Command: ./a.out ==87742== --87742-- run: /usr/bin/dsymutil \"./a.out\" warning: no debug symbols in executable (-arch x86_64) ==87742== ==87742== HEAP SUMMARY: ==87742== in use at exit: 22,529 bytes in 188 blocks ==87742== total heap usage: 268 allocs, 80 frees, 32,649 bytes allocated ==87742== ==87742== LEAK SUMMARY: ==87742== definitely lost: 3,472 bytes in 55 blocks ==87742== indirectly lost: 2,832 bytes in 9 blocks ==87742== possibly lost: 0 bytes in 0 blocks ==87742== still reachable: 0 bytes in 0 blocks ==87742== suppressed: 16,225 bytes in 124 blocks ==87742== Rerun with --leak-check=full to see details of leaked memory ==87742== ==87742== For lists of detected and suppressed errors, rerun with: -s ==87742== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)  Try to let the above program produce a few memory leaks. Does valgrind notice you did not clean up your mess? The definitely lost amount should skyrocket after a few uncleaned malloc() calls. Are amount of reported bytes correct? Recalculate this manually.\n  Remember that malloc() calls that aren\u0026rsquo;t assigned could be optimized and removed by the compiler, resulting in no heap reservation at all. Make sure to actually \u0026ldquo;do\u0026rdquo; something with it (e.g. allocate a struct and pass in a few values). See also 8.1: Optimizing code.\nFurther Reading  Hack the Virtual Memory: malloc, the heap \u0026amp; the program break Valgrind quickstart  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/lab1_segfault/",
	"title": "9.2: Deliberate Segfaulting",
	"tags": [],
	"description": "",
	"content": "  source: xkcd.com   Segfault this For each of the programs below:\n Guess first if they will give a segmentation fault and where exactly and why Compile and execute them Were your guesses in the first step correct or not? Why (not)?  #include \u0026lt;stdio.h\u0026gt; int main(void) { int* i = NULL; *(i) = 666; return 0; } #include \u0026lt;stdio.h\u0026gt; int main(void) { int i = 666; *(\u0026amp;i - 10000) = 777; return 0; } #include \u0026lt;stdio.h\u0026gt; int main(void) { int i = 666; *(\u0026amp;i + 10000) = 777; return 0; }  Try and change the number \u0026ldquo;10000\u0026rdquo; to higher and lower numbers in the previous two code samples to help make sense of what\u0026rsquo;s happening.  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; int main(void) { int *i = (int *) malloc(26 * sizeof(int)); *i = 20; *(i + 27) = 30; free(i); *i = 40; return 0; }  Try and change the number \u0026ldquo;100\u0026rdquo; to \u0026ldquo;26\u0026rdquo; and the number \u0026ldquo;101\u0026rdquo; to \u0026ldquo;27\u0026rdquo; and run the program again. Do you get the same results?  A to the Z As we\u0026rsquo;ve seen in the previous exercises, segmentation faults don\u0026rsquo;t always happen when we expect them to. Let\u0026rsquo;s explore this a bit deeper here.\n Again, first guess if the program below will give a segmentation fault or not (and why!) Compile and run the program and verify your guess  #include \u0026lt;stdio.h\u0026gt; #define WRITE_SIZE 26 #define READ_SIZE 27  int main(void) { int i; unsigned char alphabet[26]; for(i=0;i\u0026lt;WRITE_SIZE;i++) { alphabet[i] = 65 + i; } for(i=0;i\u0026lt;READ_SIZE;i++) { printf(\u0026#34;%2d -\u0026gt; %c (%02x)\\n\u0026#34;, i, alphabet[i], alphabet[i]); } return 0; }   Now, recompile the code above, but verify that you have the -0s option added for gcc. This compiler flag sets the optimisation towards size, but also adds a few more interesting things. Does that make a difference?\n  Now increase the WRITE_SIZE to 27. Compile and run again, with and without the -0s option. Any errors? If so, what type of error could this be?\n  Compare this to the last code sample of the first exercise above. What is different about which memory we are trying to access here?\n  Do some research online until you can explain what a \u0026ldquo;stack canary\u0026rdquo; is and why it\u0026rsquo;s useful.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/rtos/",
	"title": "X.2: Real-time Operating Systems",
	"tags": [],
	"description": "",
	"content": "Operating Systems In the previous chapters, many aspects are handled that form the Operating System. The image below, linked from Wikipedia, shows the most important components in the Linux kernel.\n  source: wikipedia.com   The image above shows:\n Task management Memory management the IO subsystem    The desired situation.      The real situation.     The main goal of the OS is still the same: allow multiple programs and/or users use the same hardware. On the left-hand side of the image above a visual representation is given of what we want to experience. Everything is running truly in parallel.\tWe have touched on a number of reasons why reality is more like is shown on the right-hand side of the image. The OS will do its best to achieve the desired situation as good as possible. However, sometimes doing your best is not good enough.\nTry to imagine these situations (and the possible explanations from the developers):\n A pacemaker missed an arrhythmia  Sorry, at that moment the tiny OS was switching tasks.   A surveillance drone crashed against a construction crane  Sorry, at that moment the drone should have detected the crane, the flight computer was in the middle of a TCP time-out while sending telemetry.   The motor of a high-end sports car burned out due to a faulty-timed gear shift  Sorry, at the moment the timer ended, the microcontroller was handling a switch in radio stations.    Sometimes, doing your best is not good enough.\nReal-time Operating Systems While a general OS tries its best to meet all constraints, a sporadic failure is not a vital problem. If there is a dip in network handling resulting in a short lag of a youtube-clip the computer will not crash-and-burn. Operating systems like this are called soft real-time operating systems and examples are: Linux, Windows, iOS, \u0026hellip;\nLike illustrated above, some situations have tasks which have to meet their constraints. Failing to do so results in a system failure. For these applications Real-Time Operating Systems (RTOS) can be used. These operating systems are called hard real-time systems. They have very specific deterministic constraints and ALL of them should be MET, at ALL TIME. Examples of RTOSes are:\n FreeRTOS mbedOS Contiki Xenomai (there are many more)  For the sake of completeness it is pointed out that tweaks are available to turn the Linux kernel into a real-time kernel.\nFreeRTOS    FreeRTOS is an open source real-time OS that is tailored for embedded systems. You can even run this on an Arduino.\nThis OS essentially consists out of merely 5 files:\n tasks.c: handles task management queue.c: handles queues \u0026amp; synchronisation list.c: handles lists port.c: details for porting to a specific processor heap_x.c: handles the heap  FreeRTOS is a trade-off between bare-metal programming and the luxury of an OS. With everything you\u0026rsquo;ve seen in this course you should be able, after some studying perhaps, to understand how a task is described.\nvoid runClock(void* pvParameters) { short i, j, k, l; for/*ever*/(;;) { for(i=0;i\u0026lt;24;i++){ for(j=0;j\u0026lt;60;j++){ d[0]= j % 10; d[1]= (j-d[0]) / 10; d[2]= i % 10; d[3]= (i-d[2]) / 10; for(k=0;k\u0026lt;10;k++) { for(l=0;l\u0026lt;4;l++) { _delay_ms(5); }\t} }} } /* end of for/*ever*/ // no return statement } The function above is the program/tasks that simulates time. The time is written in shared memory that is accessible by other programs/tasks.\nThe main function could like this:\nint main(void) { /* Perform any hardware setup necessary. */ // define the outputs to be the 4 digit-selectors and the 7 segement-selectors  DDRB |= 0b00111111; DDRD |= 0b11111000; /* APPLICATION TASKS CAN BE CREATED HERE * eg. xTaskCreate(TaskBlinkGreenLED, (const portCHAR*) \u0026#34;GreenLED\u0026#34;, 256, NULL, 3, NULL); * with 1st argument: name of the function * 2nd argument: human readable name (only for debugging purposes) * 3rd argument: stacksize (in \u0026#34;words\u0026#34; (words*stackwidth = memory)) * 4th argument: function parameters * 5th argument: priority (0 ... (configMAX_PRIORITIES â 1)) * 6th argument: pxCreatedTask can be used to pass out a handle to the task being created. This handle can then * be used to reference the task in API calls that, for example, change the task priority or * delete the task. **/ xTaskCreate(runClock, (const portCHAR*) \u0026#34;runclock\u0026#34;, 256, NULL, 3, NULL); xTaskCreate(showOnDisplay, (const portCHAR*) \u0026#34;displayer\u0026#34;, 256, NULL, 3, NULL); /* start the scheduler */ vTaskStartScheduler(); /* Execution will only reach here if there was insufficient heap to start the scheduler. */ for/*ever*/ (;;); return 0; } Is a RTOS outdated? One could state that RTOS-es are outdated. Processors and OS-es, anno 2023, are so powerful that no additional measures should be taken to guarantee specific constraints. While that might be true, there is also scalability.\nIt might be feasible to write all the desired software (networking stacks, logging, sensor reading, \u0026hellip;) in such a way all constraints are met, BUT it requires a state-of-the-art system. For example: a processor with 2 GB of RAM memory and 160 GB of solid state storage. Installing this in every gearbox of a car, for example, simply is too expensive. If the same constraints and performance requirements can be met for a fraction of the price, industry dictates the latter option should be chosen.\nThat\u0026rsquo;s good news for whoever is studying this course, as skill-full programmers and engineers are a necessity to make this work!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch1-introos/cli/",
	"title": "1.3: Getting your CLI-feet wet",
	"tags": [],
	"description": "",
	"content": " Source: Neo Getting your feet wet This \u0026lsquo;lab\u0026rsquo; consists of a list of small tasks that introduce you to the command-line interface. None of these tasks should require exhaustive manual labour. Most can be accomplished by using the commands you already know (and that are listed at the cheat sheet), and for others a short Google session should give you the answer.\nNavigation  figure out in which folder you are at the moment navigate to the root folder of the OS navigate to your home folder (/home/username), without actually typing that path make an alias (man alias) that navigates to your home folder from everywhere within the system  File manipulation  make a folder \u0026ldquo;myVeryOwnFolder\u0026rdquo; navigate into that folder create a file: hello.txt that contains the text hello world. Do this in a single step (hint: you can \u0026ldquo;pipe\u0026rdquo; commands together) move up on folder and remove the entire directory myVeryOwnFolder, including the .txt file  Access permissions  create a new file with any content make this file read-only for everyone remove the file make a folder ToBeDeleted remove the execute rights from this folder for ALL users remove this folder  Various  navigate to course-files/ch1_os (from your cloned git repository)   all these files contain random text one file contains the word \u0026ldquo;bamboozle\u0026rdquo; find out which file contains this word find out the line number on which the word occurs  search the man page for the meaning of \u0026ldquo;-x\u0026rdquo; in the command ls display the current date and time (is this correct? Why wouldn\u0026rsquo;t it be?) execute three commands, using the enter key only once display the current date and time sleep for 10 seconds display the current date and time  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-introcpu/2_functions/",
	"title": "3.3 Functions and The Stack",
	"tags": [],
	"description": "",
	"content": "In the previous part, we\u0026rsquo;ve discussed how we can use special branching and jumping instructions (like BEQ and JMP) to skip over certain parts of code to implement if-else logic. In the lab, we\u0026rsquo;ve seen how we can use the same building blocks to build while and for loops.\nIn this part we will discuss how we can use jumping to implement functions, and some extra challenges that come with that.\nFunction calls Let\u0026rsquo;s consider a very simple example of one C function calling another on the left, and the known assembly on the right (using line numbers instead of hexadecimal addresses for simplicity from now on):\n\r1. void f1() { 2. f2(); 3. int C = 10; 4. } 5. void f2() { 6. int D = 20; 7. return; 8. } \r\r1. 2. JMPi line6 3. ADDd 0 10 C\r4. 5. 6. ADDd 0 20 D 7. JMPi line3 8. \r\rFor the coming examples, we\u0026rsquo;ll always assume f1() is called automatically at the start of the program.\n Logically, we know what we want the f2() function call to do: if it\u0026rsquo;s encountered, we want to stop executing the current function f1, and start executing the other function f2 from its start (line 6). Only when the other function is done (indicated by the return statement), do we want to return to f1. However, we don\u0026rsquo;t want to start f1 all over again (on line 2), we want to continue right after the function call (line 3).\nAs we can see above on the right, we can quite easily do this with JMPi, having the f2 jump \u0026ldquo;back\u0026rdquo; to f1 once it\u0026rsquo;s done. However, you there\u0026rsquo;s a problem with this particular approach? Can you see what it is?\nImagine a third function f3() that also calls f2().\n\r9. void f3() { 10. f2(); 11. int E = 50; 12. } \r\r9. 10. JMPi line6 11. ADDd 0 50 E\r12. \r\rAs a programmer, you expect the same logic as above: f2 gets called, and after that the execution of f3 continues on line 11. However, this is not what happens! This is because above, we\u0026rsquo;ve hardcoded f2 to \u0026ldquo;return\u0026rdquo; to the hardcoded address \u0026ldquo;line3\u0026rdquo; (which is in f1). As such, we would never go back into f3 to execute line 11, but instead execute part of the unrelated f1. That\u0026rsquo;s of course not good!\nWe can see that, in order to allow f2 to be called from anywhere, the address it JMPs to when it\u0026rsquo;s done needs to be dynamic. As hinted in the previous part, we will use JMP instead of JMPi for that. Instead of a direct address value to jump to like JMPi, JMP will instead take an address which it needs to read to get the real address to jump to. As we\u0026rsquo;re working with functions and we want to know where to jump to after \u0026ldquo;returning\u0026rdquo; from a function, this address is typically called the return adress.\nLet\u0026rsquo;s change the example above to incorporate this idea. We choose a single location in memory (at address 0x07) to store the return address for both f1 and f3.\n\r1. void f1() { 2. f2(); 3. int C = 10; 4. } 5. void f2() { 6. int D = 20; 7. return; 8. } 9. void f3() { 10. f2(); 11. int E = 50; 12. } \r\r1. ADDd 0 line3 0x07 // store return address line3\r2. JMPi line6 3. ADDd 0 10 C\r4. 5. 6. ADDd 0 20 D 7. JMP 0x07 // jump to whatever address is stored in 0x07 8. 9. ADDd 0 line11 0x07 // store return address line11\r10. JMPi line6\r11. ADDd 0 50 E\r12.\r\r\rAs you can see, we can now indeed correctly return to either f1 or f3 from f2, depending on which return address values they store in 0x07 before calling f2!\nHowever, this is still not ideal\u0026hellip; can you see the problem when f2 itself would want to call another function (for example f4)?\n\r1. void f1() { 2. f2(); 3. int C = 10; 4. } 5. void f2() { 6. f4(); 7. return; 8. } 9. void f3() { 10. f2(); 11. int E = 50; 12. } 13. void f4() { 14. int X = 29; 15. return; 16. } \r\r1. ADDd 0 line3 0x07 // store return address line3\r2. JMPi line6 3. ADDd 0 10 C\r4. 5. ADDd 0 line7 0x07 // store return address line7 6. JMP line14 // call f4 7. JMP 0x07 // jump to whatever address is stored in 0x07 (this exact line...) 8. 9. ADDd 0 line6 0x07 // store return address line6\r10. JMPi line6\r11. ADDd 0 50 E\r12.\r13. 14. ADDd 0 29 X 15. JMP 0x07 // jump to whatever address is stored in 0x07 16. \r\rIn that case, f2 of course also needs to store a return address for f4 to return to. However, it can\u0026rsquo;t re-use 0x07, because that memory location already stores the return address for either f1 or f3, which would be overwritten and lost forever\u0026hellip; This is what happens in the example above: f1 is called, stores line3 in 0x07, then calls f2. f2 then stores its own return address line7 in 0x07, overwriting the line3 value forever. It then calls f4, which is indeed able to correctly return to f2. However, then we have another problem: f2 tries to \u0026ldquo;return\u0026rdquo; to f1 via 0x07, but erroneously returns to itself, causing an infinite loop!\nA naive solution to this problem of overwriting 0x07 would be to just choose a different storage location for f2\u0026rsquo;s return address, say 0x06. f4 can then just do JMP 0x06 and stuff works, right?\nSadly no\u0026hellip; imagine what happens if f3 wants to call f4 as well. It would have to somehow know that it needs to use 0x07 when calling f2, but 0x06 when calling f4, or things won\u0026rsquo;t work. This is doable for a handful of functions, but you can imagine that this is not feasible for larger programs with hundreds of functions that could each call one another. It would also consume quite some of memory, as we basically would need a separate return address storage location for each function. It also wouldn\u0026rsquo;t support so-called \u0026ldquo;recursive functions\u0026rdquo; (see below).\nIn short, we cannot simply choose a single return address location (we want to call functions from inside functions), nor can we choose a separate one for each function (impractical). We clearly need something else\u0026hellip; but before we figure out what the real solution might be, let\u0026rsquo;s first think about what functions need besides return addresses. Maybe we can end up designing a system that works for other things as well?\nPassing parameters and returning values As you probably know, the functions we had above were really the simplest possible ones. Real functions are typically more complex, as they also allow the use of function parameters and return values.\nFor example, below fY accepts two input parameters (A and B) and has a single output/return value result:\n\r1. void fX() { 2. int A = 1; 3. int B = 2; 4. int C = fY(A, B); 5. 6. int D = C + 5; 7. } 8. int fY(int A, int B) { 9. int result = A + B; 10. return result; 11. } \r\r1. 2. ADDd 0 1 A\r3. ADDd 0 2 B\r4. JMPi line7 // call fY\r5. ADDi result 0 C // C = result\r6. ADDi C 5 D\r7.\r8. 9. ADD A B result 10. JMPi line5 // return to fX 11. \r\rNote: line 4 was split into two instructions: one JMP to call the function and one ADDi to assign the return value to C\nAs we can see above, we can both pass input parameters and receive outputs (return values) by using the same variable names across functions (A, B and result). However, this again is suboptimal. In essence we\u0026rsquo;re now using global variables! Imagine again fY would call another function fZ that also uses variables named A, B or result\u0026hellip; then fZ would end up overwriting their values with new contents! With this approach, each variable and parameter in the entire codebase would need a unique name, which is of course almost impossible (and very impractical!).\nTo make this a bit better, we could again think of something similar to the return address, where fX places its parameters values in certain known memory locations (say 0x60 and 0x61) and fY places its return value in 0x80. However, this is really just the exact same problem again\u0026hellip; whether we call the location A or 0x60, the concept remains exactly the same! We are still assuming that the same memory location is re-used across functions, which has the danger of it being overwritten.\nSo, let\u0026rsquo;s try to rephrase our problem in more general terms, now that we see that our issues with the return address, input parameters and output values are in essence the same single problem.\nThe core problem is thus that we want to pass one or more pieces of data from one function to another without functions overwriting each other\u0026rsquo;s data. The only way we can do this, is to place the data in certain memory locations.\nThat leaves us with two options:\n  Re-use the same memory locations.\nFor example:\n 0x07 is the return address for each function 0x60 - 0x79 are the addresses of the input parameters for each function (allowing up to 20 parameters) 0x80 holds the return value for each function  Positive: each function knows where to look for its data, how to return after execution, and where to put data it wants to pass to another\nNegative: only allow us to call a single function at a time, otherwise we would overwrite the data\n  Have separate memory locations for each individual function.\nFor example:\n f1() would use 0x100 as return address, 0x101 - 0x120 for parameters, 0x121 for return value f2() would use 0x200 as return address, 0x201 - 0x220 for parameters, 0x221 for return value etc.  Positive: no risk over overwriting (except when using recursive functions, see exercises)\nNegative: huge memory requirement. Each function needs to know which memory locations other functions will use before being able to call them.\n  As we\u0026rsquo;ve said above, option 2 is impractical. But option 1 would only allow us to call a single function at a time\u0026hellip; or would it?\nAs we defined the problem above, the real problem is NOT re-using memory locations; it is potentially overwriting data in them.\nAs such, if we can find a way to re-use the same memory locations without overwriting them, we\u0026rsquo;ve found our solution!\n How can we possibly re-use memory locations without overwriting the data contained within?\n\rBy first copying out the data before the memory locations are re-used, then afterwards copying the original data back in!\r\r  The Stack The epiphany we had at the end of the last section should be quite easy to grasp: to prevent overwriting some data, we first copy that data to another location. Then, after the function call is done, we copy the data back to the original location.\nLet\u0026rsquo;s illustrate this with the simple example where we only need to worry about the return address (stored at 0x07) being overwritten by f1 calling f2, which then calls f4:\n\r1. void f1() { 2. f2(); 3. int C = 10; 4. } 5. void f2() { 6. f4(); 7. 8. 9. return; 10.} 11. void f4() { 12. int E = 50; 13. return; 14. } \r\r // we start in f1() so 0x07 is empty at this time\r1. ADDd 0 line3 0x07 // store return address line3\r2. JMPi line5 // call f2\r3. ADDd 0 10 C\r4. 5. ADDi 0x07 0 0x500 // copy whatever is in 0x07 (return addres from caller f1) to 0x500\r6. ADDd 0 line8 0x07 // put our own f2 return address (line8) in 0x07\r7. JMPi line11 // call f4 8. ADDi 0x500 0 0x07 // copy 0x500 back to 0x07 (restore previous value)\r9. JMP 0x07 // return from this function to caller f1\r10.\r11. // f4 doesn't call another function, so we don't need to put anything in 0x07\r12. ADDd 50 0 E\r13. JMP 0x07\r14.\r\r\rAs we can see, all the magic happens in f2. Here, we first protect f1\u0026rsquo;s return address (which is in 0x07) by copying it to 0x500, before putting f2\u0026rsquo;s own return address into 0x07. As we can see, we are now indeed re-using the single memory location in each function, but without overwriting/losing data!\nYou might think that\u0026rsquo;s it, we\u0026rsquo;ve done it! However, let\u0026rsquo;s contemplate what happens if I have more than 3 functions. Let\u0026rsquo;s say f1 calls f2, which calls f3, which calls f4, which calls f5. f2 can use 0x500 as temporary storage, but of course f3 cannot, or it would overwrite things again! So f3 would maybe use 0x600, f4 might use 0x700, etc.\nWe can see this is again not ideal\u0026hellip; it\u0026rsquo;s better than our previous discussion, because here the calling function can choose which memory to use as temporary storage (as opposed to being hardcoded in the called function). However, this still has issues: how does f4 know that f2 is using 0x500 and that it itself should be using 0x700 instead? In other words:\nHow do we ensure functions don\u0026rsquo;t re-use each other\u0026rsquo;s temporary storage?\n\rBy having each function store its data directly after that of the previous function!\r\r  Instead of having each function decide its own temporary storage locations, we will just ensure they put their copied data after that of the previous function. As such, if f2 would use 0x500, then f3 would use the next free location after that, being 0x501. f4 then uses 0x502, etc. Conceptually, you can say that each function adds more temporary data on top of a large pile of memory. You can compare it to a stack of papers or books: you start with one on the bottom, then keep putting others on top whenever you need place to store a page/a book.\nThe opposite is also true: you can\u0026rsquo;t just grab the book at the bottom of the stack; you first need to remove other books from the top. Here, this means f4 can\u0026rsquo;t just immediately return to f2 (0x500), we of course first need to return to f3 (0x501) before we can do that.\nAs such, this approach of putting per-function data directly after each other in memory is typically called just that: the stack!\nThe Stack Pointer Again, we of course don\u0026rsquo;t want to hardcode the addresses in the functions! We want to dynamically know which location the previous function used last, and then dynamically store our data after that.\nWe can do this by using just a single global variable (shared across all functions), which is called the stack pointer.\nLet\u0026rsquo;s say this stack pointer lives at address 0x82. Its initial value is 0x500. f1 starts and places it return address (line3) into 0x07. This is the (partial) state of the RAM at this point:\n\r   Address Value Comment     0x07 line3 shared return address location   0x82 0x500 stack pointer    \r\r   Address Value Comment     0x500     0x501     0x502     0x503      \r\rWhen f2 is called, it loads the value that\u0026rsquo;s in 0x82 and finds it\u0026rsquo;s 0x500, which is the address where it can start storing its temporary data. It puts the return address of f1 (value in 0x07) in 0x500 and then, very importantly, increases the value of the stack pointer by one (one, because it has stored a single byte). The value of the stack pointer at 0x82 is now 0x501. f2 can now set its own return address (line8) in 0x07 and then call f3 as shown above. The RAM now looks like this:\n\r   Address Value Comment     0x07 line8 shared return address location   0x82 0x501 stack pointer    \r\r   Address Value Comment     0x500 line3 return address of f1, stored here by f2   0x501     0x502     0x503      \r\rf3 then wants to call f4, so it needs to store f2\u0026rsquo;s return address (currently in 0x07) on the stack. It loads the stack pointer at 0x82, finding that it has safe room to temporarily copy data at 0x501 and does so. f3 then, again very importantly, increases the value of the stack pointer by one (to 0x502, preparing it for use by f4), and stores its own return address (say line14) before actually calling f4. The RAM is now:\n\r   Address Value Comment     0x07 line14 shared return address location   0x82 0x502 stack pointer    \r\r   Address Value Comment     0x500 line3 return address of f1, stored here by f2   0x501 line8 return address of f2, stored here by f3   0x502     0x503      \r\rNote: because of how we\u0026rsquo;re drawing the diagrams above, the stack (starting at 0x500) is \u0026ldquo;upside down\u0026rdquo;, growing downward instead of upwards!\nNow, let\u0026rsquo;s say it ends there: f4 does not call any other functions, just does its work and then returns back into f3. Now we\u0026rsquo;re really going to start to see the power of using the stack!\nf4 did not do anything to 0x07, so it knows it can just use its current value (line14) as return address to get back into f3. After that, the RAM still looks the same as the last diagram from above.\nNow, f3 needs to do some more work of course. It needs to figure out how to return to its caller (f2). It knows it has previously stored f2\u0026rsquo;s return address on the stack (via the stack pointer), and after that it had increased the stack pointer by one. As such, to figure out where it had stored f2\u0026rsquo;s return address, it needs to undo what it did. First, it reads the value of the stack pointer in 0x82, getting back 0x502. It then decrements that value by one, making it 0x501. It now knows that the return address of its caller (f2) is at 0x501 and can read that value, finding line8. It places that value back into 0x07. It now first cleans up after itself by clearing the value of 0x501, and then returns to f2 by JMPing to 0x07. The RAM now looks like this:\n\r   Address Value Comment     0x07 line8 shared return address location   0x82 0x501 stack pointer    \r\r   Address Value Comment     0x500 line3 return address of f1, stored here by f2   0x501     0x502     0x503      \r\rYou will see this state of the RAM is exactly the same as that from before we called f3 in the first place. This is of course exactly as intended! The stack is only used as temporary storage, and as such should no longer be needed once we\u0026rsquo;re done executing all functions!\n The same thing happens in f2: it reads the stack pointer, decrements it by 1, and finds 0x500. It places the value at 0x500 at 0x07, clears 0x500, and then returns to f1 with JMP 0x07. The RAM state is again the same as the one we started with above.\nThe Stack in Assembly To be able to write the above sequence in Assembly, we actually need a new syntax. This is because we need to read the stack pointer\u0026rsquo;s value (at 0x82) and then use the value itself (example 0x501) as an address! We weren\u0026rsquo;t able to do this before, except when it was implied in the instruction\u0026rsquo;s way of working (for example, JMP does work like this implicitly!). If we want to do this for other instructions as well however (for example ADDi), we need some way to indicate that the read value should be treated as an address instead of an immediate. For this, we\u0026rsquo;ll take inspiration from C\u0026rsquo;s pointers:\nIf we write * before an address, it means we want to use the value at that address as a new address instead of as a value.\nFor example:\nADDd 3 5 0x10 : this means we store the value 8 at address 0x10.\rADDd 3 5 *0x10 : this means we read the value currently at address 0x10, interpret it as another address (say the value was 0x29), and write 8 to that address. This is needed to be able to write to the free temporary memory address pointed to by the stack pointer:\nADDi 0x07 0 *0x82 : this adds 0 to the value of 0x07 (so it stays the same), and then stores the result not in 0x82, but in the address value stored in 0x82 (which is 0x500 at the start of our examples).\r If we thus write the above sequence in Assembly, we get:\n\r0. 1. void f1() { 2. f2(); 3. int C = 10; 4. } 5. void f2() { 6. 7. 8. f3(); 9. 10. 11. return; 12.} 13. void f3() { 14. 15. 16. f4(); 17. 18. 19. return; 20. } 21. void f4() { 22. int E = 50; 23. return; 24. } \r\r // we start in f1() so 0x07 is empty at this time\r0. ADDd 0 0x500 0x82 // initialize the stack pointer (done by the OS)\r1. ADDd 0 line3 0x07 // store return address line3\r2. JMPi line5 // call f2\r3. ADDd 0 10 C\r4. 5. ADDi 0x07 0 *0x82 // store the contents of 0x07 (line3) on the stack (at 0x500)\r6. ADDi 0x82 1 0x82 // increment the stack pointer value by one (is now 0x501)\r7. ADDd 0 line9 0x07 // put our own f2 return address in 0x07\r8. JMPi line13 // call f3 9. SUBi 0x82 1 0x82 // decrement the stack pointer value by one (is now 0x500)\r10. ADDi *0x82 0 0x07 // store the value at the stack pointer address (at address 0x500) in 0x07\r11. JMP 0x07 // return from this function to caller f1\r12.\r13. ADDi 0x07 0 *0x82 // store the contents of 0x07 on the stack (at 0x501)\r14. ADDi 0x82 1 0x82 // increment the stack pointer value by one (is now 0x502)\r15. ADDd 0 line17 0x07 // put our own f3 return address (line17) in 0x07\r16. JMPi line22 // call f4 17. SUBi 0x82 1 0x82 // decrement the stack pointer value by one (is now 0x501)\r18. ADDi *0x82 0 0x07 // store the value at the stack pointer address (at address 0x501) in 0x07\r19. JMP 0x07 // return from this function to caller f2\r20.\r21. // f4 doesn't call another function, so we don't need to put anything new in 0x07\r22. ADDd 50 0 E 23. JMP 0x07 // return from this function to caller f3\r24.\r\r\rOne crucial thing you will notice is that the stack-specific code is exactly the same for both f2 and f3. This is the beauty of using a dynamic setup like the stack pointer: it doesn\u0026rsquo;t rely on hardcoded addresses or logic, and so the exact same code can be re-used!\nNote also that the stack instructions are each other\u0026rsquo;s opposites: ADDi 0x07 0 *0x82 is undone by ADDi *0x82 0 0x07 and ADDi 0x82 1 0x82 is inverted by SUBi 0x82 1 0x82, creating a nice symmetry!\nPush and Pop You might also notice that this re-use of code makes it quite annoying to read/write, especially since each stack operation requires two instructions: one to actually move a value on/off the stack and one to increment/decrement the stack pointer.\nFor this reason, these operations are often represented by their own specific instructions, called PUSH and POP. PUSH adds a new value on top of the stack and increments the stack pointer by the size of that value (in our examples this has been 1, but soon we\u0026rsquo;ll see this can be any size). POP is the opposite: it removes the top value from the stack and decrements the stack pointer by the size of that value.\nAs such, the previous Assembly on the left becomes the new on the right, making it more readable and easier to write:\n\rADDi 0x07 0 *0x82 ADDi 0x82 1 0x82 ...\rSUBi 0x82 1 0x82 ADDi *0x82 0 0x07\r\r\r\rPUSH 0x07 // push value in 0x07 on top of the stack\r...\rPOP 0x07 // pop value from top of the stack and put it in 0x07\r\r\rNote: for simplicity, we will assume PUSH and POP know where the stack pointer is stored (at 0x82 previously). In \u0026ldquo;real\u0026rdquo; Assembly, this is often a special address/memory location reserved for this purpose by convention and is always the same for all programs (we\u0026rsquo;ll see later how that\u0026rsquo;s possible with virtual memory without causing collisions).\nThe stack for parameters, return values, and local values If you look back at how we introduced the need for the stack above, you\u0026rsquo;ll remember we talked not just about return addresses, but also function parameters and return values (remember function fY(int M, int N)?). We then said that these function features had exactly the same problems as the return addresses (risk of overwriting data across functions).\nNow that we\u0026rsquo;ve solved the problem for return addresses, you\u0026rsquo;ll be happy to hear the same solution also works for parameters and return values! To reprise the example from before (but changing the variable names slightly to enforce our point), we would now write it as follows using the stack:\n\r1. void fX() { 2. int A = 1; 3. int B = 2; 4. 5. 6. 7. int C = fY(A, B); 8. 9. int D = C + A; 10.} 11. int fY(int M, int N) { 12. 13. 14. int Q = M + N; 15. 16. return Q; 17. } \r\r1. 2. ADDd 0 1 A\r3. ADDd 0 2 B\r4. ADDd 0 line8 0x07 // return address in 0x07\r5. PUSH A // parameter 1\r6. PUSH B // parameter 2\r7. JMPi line12 // call fY\r8. POP C // C = return value from fY\r9. ADD C A D\r10.\r11. 12. POP N // B was pushed last, is now on top, corresponds to N\r13. POP M // A was pushed first, was below B, corresponds to M\r14. ADD M N Q\r15. PUSH Q // put Q on top of the stack, so fX can POP it later to access the return value 16. JMP 0x07 // return to fX 17. \r\rAs you can see in the example, we no longer need to have global variable names/addresses! Variables that are passed via the stack don\u0026rsquo;t have a name, just a specific order on that stack. As such, we have no more danger of overwriting the parameters/return values if we were to call other functions, as PUSH/POP would change the stack pointer accordingly. Do note that this order is reversed in the called function (we first need to POP N and then M, while we PUSHed A and then B).\nNote: we\u0026rsquo;re currently not storing the return address on the stack. Instead, we use 0x07 directly because we know there\u0026rsquo;s just two functions. With more functions, you of course do have to store it as well and think about the correct order. You\u0026rsquo;ll have to do this in one of the lab exercises!\nLocal values As we can see in the examples above, the stack is a self-cleaning data structure. As long as you properly POP everything you\u0026rsquo;ve pushed, the stack is nicely reset to its previous state once a function returns! This also makes it a very powerful way to track memory usage and prevent memory leaks!\nThis is one of the main reasons why most programming languages don\u0026rsquo;t just store return addresses, input parameters and return values on the stack, but also most local variables!\nIn the example above, A, B, C, D and Q (variables that are declared without being direct function parameters), would in reality also be stored on the stack!\nConsider again that this makes sense: say fX had chosen address 0x700 for A and 0x701 for B. How would fY know this? How would you prevent it from choosing 0x700 as well to store Q? You would have to keep track of all used memory and check if something was available for each variable you created!\nInstead it\u0026rsquo;s much easier to just PUSH new local variables on the stack, as you know the stack is auto-increment/decremented with PUSH/POP and so you\u0026rsquo;ll never overwrite. Additionally, as we\u0026rsquo;ve said here, this automatically cleans up the local variables. After all, outside the function they are no longer needed. If they were, they would have been passed as input parameters or returned as a return value (also via the stack!)\nWe (intentionally) don\u0026rsquo;t really have the correct Assembly syntax here to easily represent storing local variables on the stack as well. After all, you\u0026rsquo;d have to be able to reference values on the stack by name/position without POPing them for the examples above. In real Assembly, this is typically done by using memory offsets within the stack to re-use values. That is however too advanced for this class (see later courses for that). For now, just assume all variables you declare somehow end up on the stack and are also automatically cleared from it when the function returns. This will be important when talking about stack memory usage and stack overflows in later classes!\n Of course not everything is stored on the stack. There is of course a way to use memory outside of the stack and do its management yourself. As you\u0026rsquo;ll see later in this course, we call this the heap, and you will have to use special C functions (for example malloc/free) to use that memory, while you get the stack \u0026ldquo;for free\u0026rdquo; (the necessary Assembly is added by the C compiler so you don\u0026rsquo;t have to worry about it).\nFor the heap, the OS indeed tracks which memory is used and which is free, and it has to check this every time you try to reserve new memory. This is why most variables live on the stack (easier, less overhead) and why you will usually reserve larger chunks of heap memory at once (not just 1 byte, but tens or even thousands at once).\n Cache and Registers Finally, it should be said that our Von Neumann architecture was really a bit too high-level to (still) be realistic. Up until now, we\u0026rsquo;ve been pretending that everything is stored directly in RAM memory (both data and instructions) and that all instructions read/write directly from/to RAM. This is not true in practice, as that would be quite slow\u0026hellip;\nYou see, RAM memory is very large nowadays, and we mean that in a physical sense. The physical area needed to store say 16 GB RAM (say 20 square centimeters) is several times that of your CPU chip. Additionally, we want to be able to easily replace/add RAM, so it\u0026rsquo;s usually stored separately from the CPU chip itself (e.g., separate memory banks on your motherboard), otherwise changing RAM would also mean changing the CPU!\nWhile the physical distance between your RAM and CPU seems small (just a few centimeters), at the speeds your CPU is running (nano-second scale for each clock tick!), that distance is actually very large, as we\u0026rsquo;re limited by the theoretical speed of light, and practical metal conduction properties. As such, reading/writing data from RAM can take several hundreds to thousands of nano-scale CPU cycles, which is of course too slow to be practical.\nAs such, over time, smaller but faster pieces of memory have been added closer to the CPU. You probably know these as the cache memory and its several layers. L1 is close to the CPU and takes 5-10 cycles. L2 is a bit further away and takes tens of cycles. L3 is further still (and usually shared between several CPU cores) and can take +100 cycles. In the image below, 1 cycle is about 0.25 nano seconds (ns), so 50ns is +200 cycles for RAM (and a mechanical hard disk can take over 20 million cycles (50ms+)!!!!).\n  Image Source: http://www.cs.cornell.edu/courses/cs3410/2019sp/schedule/slides/12-caches-pre.pdf   But maybe you have not heard about the smallest but also fastest type of memory a CPU has, called registers. Registers are a very limited amount of memory locations (usually there are 32 to 256 registers, each of 32 to 64 bit) that are located very close to the ALU itself. Registers typically cost no (extra) CPU cycles to read/write and thus should be used as much as possible. However, since we have so few of them available, we have to be very clever which registers to use for what at what time, and also not to overwrite them with new values!\nIn practice, this is done by the compiler (and sometimes partly by the advanced CPU itself!). The compiler will generate assembly that properly loads data from the RAM into the registers (and caches), but that also writes data back to RAM/cache from the registers when it knows it won\u0026rsquo;t be needed soon. Deciding what should be kept in registers and what can be \u0026ldquo;spilled\u0026rdquo; to RAM is often a complex problem (somewhat understandably explained in this short video). This all means that real assembly is usually even more complex than what we\u0026rsquo;ve seen, with extra LOAD/STORE instructions in between.\nMost real assembly instructions also don\u0026rsquo;t work on memory addresses directly, but instead on register numbers. For example, ADD would not take 3 RAM addresses, but rather 3 register numbers. If we wanted to ADD data from the RAM, we would first need LOAD instructions to get that into registers, then do the ADD on the registers, and then STORE the result back into RAM.\nThis might seem like a lot of overhead (didn\u0026rsquo;t we just say RAM access is slow?) until you recognize that most programs do lots of calculations with intermediate results (say A = B + C; D = A + E; F = D + G;), where we can keep all the intermediate values (A, D, F) just in registers; they often don\u0026rsquo;t need to be stored in the RAM until all the way at the end of the calculations/function! This in combination with the faster cache memories means this \u0026ldquo;layered memory architecture\u0026rdquo; actually gives a massive speedboost (compared to using RAM directly) in practice!\nYou will learn much more about real assembly, registers and cache memory in later courses. But for now, this will do ;)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-introcpu/",
	"title": "3: Introduction in CPU",
	"tags": [],
	"description": "",
	"content": "Chapter 3 Introduction to CPU basics 3.1: Von Neumann Architecture\n The ALU Instructions Branching and the Program Counter Jumps Von Neumann Architecture  3.2 (lab): (pseudo)assembly programming\n3.3: Functions and the Stack\n Function calls and the Return Address Function parameters The Stack Recursive Functions Registers  3.4: (lab) The Stack Machine\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-pointers/pointers/",
	"title": "4.1: Pointers and arrays",
	"tags": [],
	"description": "",
	"content": "In the schematic examples below, the following concepts can be explicitly distinguished:\n The name of a variable The value of a variable  And the following implicitly:\n The type of a variable The adres of a variable  The type of a variable determines the amount of memory that is freed up to be able to save the value. The value is either a current value or an address that refers to a different value. Each variable has a unique address. Variables can therefore refer to each other.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; A[varnamevalue] B{varnameaddress}  This example visualizes the instruction int a = 5:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; A[a5]  Where the type, int, reserves a few bytes depending on the target platform (sizeof(int)), on a specific address. We can manipulate the address itself - this is essential when working with arrays.\n\u0026ldquo;Compound\u0026rdquo; types C uses a pass-by-value system to pass variables to functions. This means that the value is copied, and that function cannot make changes to the original value. That is something positive: separation of concerns.\nWhen we think of our person example of chapter 2, that struct is therefore always copied. That can be very inefficient, depending on the size of the data! To avoid this, we use a \u0026ldquo;pointer\u0026rdquo;: a reference to the current data. Objects are passed by-reference by default in Java - so in C we have to do something extra for this.\nInstead of is_old(struct Person person) the signature becomes is_old(struct Person * person) (note the added asterik *). We have two options for reading a value here:\n \u0026ldquo;dereferencing\u0026rdquo; the pointer: asking for the real value, behind the reference. Following the arrow where it points towards, so to speak. Ask for members of the pointer using \u0026ldquo;.\u0026rdquo;.  Because in C, the . operator takes precedence over *, we have to add brackets to combine both: (*person).age. It is annoying to constantly have to use brackets, so the creators came up with an alternative, the -\u0026gt; operator: person-\u0026gt;age.\n(*pointervariable).property equals to pointervariable-\u0026gt;property.\n In Java properties are accessed using the dot operator ..\nPointer types A pointer is a \u0026ldquo;changeable\u0026rdquo; reference to a variable. Pointers have their own memory address on the stack and can refer to something else at any time: they are not constant. They are recognizable by * after variable type.\n#include \u0026lt;stdio.h\u0026gt; int main() { int young = 10; int old = 80; int *age = \u0026amp;young; age = \u0026amp;old; printf(\u0026#34;%d\\n\u0026#34;, *age); printf(\u0026#34;%d\\n\u0026#34;, age); }  What will be printed in the above example? The first line should be obvious, but the second one\u0026hellip;\n  mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{*age} --|after first assignment| B[young10] A -.- |after second assignment| C[old80]  Notice the use of the \u0026amp; operator, it is the address-of operator to fetch the address of a variable. A pointer points to an address, not to a value (of a variable).\nLook at it this way: I live in streetname, city. When I give you my card, you have a reference to my address. I can hand out cards to more people. The card does not represent my house, but points towards it. If you wish to do so, you can write a different address on the card, eliminating my previous address. From that point on, your card points to a different address, while other cards I dealt out still point to my original address.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; C{my card} --|contains directions to| A[my house] B[your house]  If I want to get the address of your house, I\u0026rsquo;d have to use the address-of operator: \u0026amp;your_house. If I want to get the house itself (physically impossible\u0026hellip;), I\u0026rsquo;d use the dereference operator on the card: *card. This simply follows the arrow where card currently points to.\nSo, what was the output of printf(\u0026quot;%d\\n\u0026quot;, age);? 1389434244! Huh? We are printing the address of the pointer, not the actual value (by following where it points to). Remember, to do that, you have to use the dereference * operator: printf(\u0026quot;%d\u0026quot;, *age);. The compiler hints at this with the following warning:\n warning: format specifies type \u0026lsquo;int\u0026rsquo; but the argument has type \u0026lsquo;int *\u0026rsquo; [-Wformat]\n C\u0026rsquo;s By-Value VS Java\u0026rsquo;s By-Ref - redux Pointers can point to pointers which can point to pointers which can \u0026hellip; Add enough * symbols!\nint val = 10; int *ptr = \u0026amp;val; int **ptr_to_ptr = ptr; int **ptr_to_ptr = \u0026amp;ptr;  Why does int **ptr_to_ptr = ptr; generate a compiler error?\n  mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; A{\"**ptr_to_ptr\"} --|ref| B{\"*ptr\"} B -- |ref| C[val10]  Practical use of the double ** notation would be to relink a pointer to another location. As you know from chapter two, variables in C are passed along by value: even pointer values. This means a copy of a pointer is created whenever calling a function with a pointer. Chaining the actual value is possible by following the address using the dereference operator. But chaining the address itself is only possible with double pointers:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;void increase(int* nr) { (*nr)++; } void reassign(int** nr, int* dest) { *nr = dest; } int main() { int* nr = malloc(sizeof(int)), *nr2 = malloc(sizeof(int)); *nr = 10, *nr2 = 5; increase(nr); printf(\u0026#34;%d\\n\u0026#34;, *nr); // prints 11  reassign(\u0026amp;nr, nr2); printf(\u0026#34;%d\\n\u0026#34;, *nr); // prints 5 } Pointer instantiation Where does a new pointer point to that is not yet instantiated?:\nint *ptr; printf(\u0026#34;%d\u0026#34;, *ptr); // prints -12599900072 Whoops. Always assign \u0026lsquo;nothing\u0026rsquo; to a pointer, using int *ptr = NULL. Note that depending on the C implementation (such as VC++, clang, GNU C), an uninitialized pointer might contain the value 0.\nNULL is a platform dependent (!!) macro that in C refers to zero (0), usually in the form of a void pointer. A void * pointer can refer to any type and is usually used to address low-level memory, as we will see using embedded hardware equipment.\nWhat gets printed in the above example if we assign NULL to *ptr?\n  The definition of a pointer does not prescribe the exact location of the *: int* age is the same as int *age (notice the placement of the stars). Be careful with things like \u0026lsquo;int *age, old_age\u0026rsquo;! The last variable here is an ordinary int, and not a pointer!\nFunction pointers Now things are getting interesting. A pointer can also point to a function. (Remember the datastructure from chapter 2?). You will need the same signature definition to do that:\n#include \u0026lt;stdio.h\u0026gt; int increase(int nr) { return nr + 1; } int doublenr(int nr) { return nr * 2; } int main() { int (*op)(int) = \u0026amp;increase; printf(\u0026#34;increase 5: %d\\n\u0026#34;, op(5)); op = \u0026amp;doublenr; printf(\u0026#34;double 5: %d\\n\u0026#34;, op(5)); return 0; } The definition of the op pointer looks a bit strange, but the signature predicts that we will return an int (far left), and that one parameter is needed, also in the form of an int (in brackets). If you fail to do so (for instance, by creating a double doublenr(int nr) function), weird things happen, but the program does not crash:\n Wouters-Air:development jefklak$ gcc test.c \u0026\u0026 ./a.out test.c:15:8: warning: incompatible pointer types assigning to 'int (*)(int)' from 'double (*)(int)' [-Wincompatible-pointer-types] op = \u0026doublenr; ^ ~~~~~~~~~ 1 warning generated. increase 5: 6 double 5: 14  Function pointers can also be given as a parameter, for example with void exec (int (* op) (int)) {. A function can return a function (pointer), for example with int (* choose_op (int mod)) (int) {. The function \u0026ldquo;choose_op\u0026rdquo; expects 1 int parameter and returns a function pointer that refers to a function with 1 int parameter and return value int. To simplify that mess, typedef is usually used:\n#include \u0026lt;stdio.h\u0026gt; typedef int(*func_type)(int); int increase(int nr) { return nr + 1; } int doublenr(int nr) { return nr * 2; } func_type choose_op(int mod) { return mod == 0 ? \u0026amp;increase : \u0026amp;doublenr; } void exec(int (*op)(int)) { printf(\u0026#34;exec: %d\\n\u0026#34;, op(5)); } int main() { exec(choose_op(0)); // print 6  exec(choose_op(1)); // print 10  return 0; } Now you understand how we used the \u0026lsquo;callback function\u0026rsquo; is_old() in the Person struct in chapter 2.\nWatch out for syntax! Remember that symbols such as * en \u0026amp; have different meanings.\n int *p; - * after a type: it\u0026rsquo;s a pointer. p = \u0026amp;i - \u0026amp; used in an expression: address-of operation *p = i - * used in an expression: dereference operation  Ponder on this  What is the difference between char msg[] = \u0026quot;heykes\u0026quot; and char *msg = \u0026quot;heykes\u0026quot;? Clarify your answer with a drawing. Wat is the difference between int a[10][20] and int *b[10]? Can you also say something about memory usage? In which case would you definitely use pointers in C, and in which case would you not? Explain your choice. What happens when I get the address of a stack variable, like \u0026amp;x in section \u0026lsquo;changing values around\u0026rsquo;, but the stack got cleared because the method call was finished?  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-debugging/testing/",
	"title": "5.3: The Middle Way: TDD",
	"tags": [],
	"description": "",
	"content": "Test-Driven Development: Google Test A concept you learned to love in the Software Engineering Skills course.\nIt\u0026rsquo;s concepts and definitions will not be repeated here, but we will introduce Google Test, a unit testing framework for C/C++ that enables us to write tests to track down bugs and reduce the amount of time needed dabbling in gdb. That is one of the major advantages of using automated test frameworks.\nGoogle Test is a C++ (11) framework, not a C framework! We will be using g++ instead of gcc to compile everything. C++ files are suffixed with .cpp instead of .c. Major differences between both languages exist but will not be needed to know all about in order to write a few simple tests. Since g++ and the tool we need to build it, cmake, are not installed on the image by default, use apt install g++ cmake to download and install the toolchains.\n A. Installation Most open source libraries require you to download the source code and compile it yourself. For Google Test, we will do exactly that, since we are learning how to work with compiling and making things anyway. We want to only compile googletest, and not googlemock - both are part of the same repository.\n Clone the github repository: https://github.com/google/googletest/. We want to build branch v1.12.x - the master branch is too unstable. Remember how to switch to that branch? Use git branch -a to see all branches, and git checkout -b [name] remotes/origin/[name] to check it out locally. Verify with git branch. Create a builddir and navigate into it: mkdir build, cd build Build Makefiles using Cmake: cmake ./../ Build binaries using make: make.  More information about CMake can be found in chapter 2.5: C Ecosystems.\nIf all goes according to plan, four libraries will have been created:\n libgtest.a ligbtest_main.a libgmock.a (we won\u0026rsquo;t use this) libgmock_main.a (we won\u0026rsquo;t use this)  In the subfolder googletest/build/lib.\nB. Usage Using the library is a matter of doing two things:\n1. Adding include folders You will need a main() function to bootstrap the framework:\n// main.cpp #include \u0026#34;gtest/gtest.h\u0026#34; int main(int argc, char *argv[]) { ::testing::InitGoogleTest(\u0026amp;argc, argv); return RUN_ALL_TESTS(); } And another file where our tests reside:\n// test.cpp #include \u0026#34;gtest/gtest.h\u0026#34; int add(int one, int two) { return one + two; } TEST(AddTest, ShouldAddOneAndTo) { EXPECT_EQ(add(1, 2), 5); } TEST(AddTest, ShouldAlsoBeAbleToAddNegativeValues) { EXPECT_EQ(add(-1, -1), -2); } What\u0026rsquo;s important here is the include that refers to a gtest/gtest.h file. The gtest directory resides in the include folder of your google test installation directory. That means somehow we have to educate the compiler on where to look for the includes!\nThe -I[directory] (I = include) flag is used to tell g++ where to look for includes.\n 2. Linking with the compiled libraries When running the binary main() method, Google Test will output a report of which test passed and which test failed:\n Wouters-MacBook-Air:unittest wgroenev$ ./cmake-build-debug/unittest [==========] Running 2 tests from 2 test cases. [----------] Global test environment set-up. [----------] 1 test from SuiteName [ RUN ] SuiteName.TrueIsTrue [ OK ] SuiteName.TrueIsTrue (0 ms) [----------] 1 test from SuiteName (0 ms total) [----------] 1 test from AddTest [ RUN ] AddTest.ShouldAddOneAndTo /Users/wgroenev/CLionProjects/unittest/test.cpp:18: Failure Expected: add(1, 2) Which is: 3 To be equal to: 5 [ FAILED ] AddTest.ShouldAddOneAndTo (0 ms) [----------] 1 test from AddTest (0 ms total) [----------] Global test environment tear-down [==========] 2 tests from 2 test cases ran. (0 ms total) [ PASSED ] 1 test. [ FAILED ] 1 test, listed below: [ FAILED ] AddTest.ShouldAddOneAndTo 1 FAILED TEST  However, before being able to run everything, InitGoogleTest() is implemented somewhere in the libraries we just compiled. That means we need to tell the compiler to link the Google Test libraries to our own application.\nAdd libraries as arguments to the compiler while linking. Remember to first use the -c flag, and afterwards link everything together.\n Bringing everything together:\n Wouters-MacBook-Air:debugging wgroeneveld$ g++ -I$GTEST_DIR/googletest/include -c gtest-main.cpp Wouters-MacBook-Air:debugging wgroeneveld$ g++ -I$GTEST_DIR/googletest/include -c gtest-tests.cpp Wouters-MacBook-Air:debugging wgroeneveld$ g++ gtest-main.o gtest-tests.o $GTEST_DIR/build/lib/libgtest.a $GTEST_DIR/build/lib/libgtest_main.a -lpthread Wouters-MacBook-Air:debugging wgroeneveld$ ./a.out [==========] Running 2 tests from 1 test case. [----------] Global test environment set-up. [----------] 2 tests from AddTest [ RUN ] AddTest.ShouldAddOneAndTo  As you can see, it can be handy to create a shell variable $GTEST_DIR that points to your own Google Test directory. To do that, edit the .bashrc file in your ~ (home) folder. Remember that files starting with a dot are hidden by default, so use the -a flag of the ls command. Add the line:\nexport GTEST_DIR=/home/[user]/googletest/googletest\nAnd reopen all terminals. Verify the above using echo $GTEST_DIR, it should print out the path.\nIf you are using a different shell, edit your shell\u0026rsquo;s config file. If you have no idea which shell you\u0026rsquo;re using, you\u0026rsquo;re probably using Bash. Verify with echo $SHELL which will likely output /bin/bash.\nThe -lpthread linking flag tells the compiler to link the standard threading libraries along with anything else, that are needed by GTest internally. We will get back on these in chapter 6.  Without this flag, you will get the following errors: \u0026ldquo;ld returned 1 exit status, undefined reference to pthread_[fn]\u0026rdquo;\n C. \u0026lsquo;Debugging\u0026rsquo; with GTest Going back to the crackme implementation, a simplified method that verifies input is the following:\nint verify(char* pwd) { // return 1 if verified against a pre-determined password, 0 otherwise. }  Write a set of tests for the above method - BEFORE implementing it yourself! Time to hone your TDD skills acquired from the course \u0026lsquo;Software Engineering Skills\u0026rsquo;. Simply copy it into the test file, or include it from somewhere else. You should at least have the following edge cases:\n right password entered wrong password entered empty password (what about NULL or \u0026quot;\u0026quot;?)  Use the GTest macro EXPECT_TRUE and EXPECT_FALSE. These correspond to JUnit\u0026rsquo;s AssertTrue() and AssertFalse().\nAgain, watch out with the order in which parameters should be passed (expected/actual)! See Google Test Primer.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/interprocess/",
	"title": "6.3: Inter Process communication",
	"tags": [],
	"description": "",
	"content": "Having multiple processes running is all good-and-well. Hey, it is one of the main reasons why the concept of an OS was introduced, remember ? Right, good job !!\nIt would make sense, though, if different processes were able to communicate with each other. That\u0026rsquo;s what this Section is about.\nThere are two main techniques to facilitate communication between multiple processes. These two techniques are shown in image below.\n Shared memory Message passing    The two main techniques for inter process communication    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nShared memory Shared memory is \u0026hellip; memory that is shared. Normally multiple processes are not allowed to read/write to each other\u0026rsquo;s memory space. This is enforced by the OS and we\u0026rsquo;ll later discuss some details on how it does this. Errors, similar to the one in the example below, are generated by the OS if a processes try to access areas that it is not allowed to access.\n#include \u0026lt;stdio.h\u0026gt; int main(void) { int i, my_array[8]; for(i=0;i\u0026lt;=20;i++) { my_array[i] = i+1; } for(i=0;i\u0026lt;8;i++) { printf(\u0026#34;%d\\n\u0026#34;, my_array[i]); } return 0; }   An example of memory protection that given by the OS   Note that, depending on the OS, the error message might vary: on MacOS, it\u0026rsquo;s simply [1] 28507 abort ./example.bin.\nThe code above exceeds the allowed stack space. Try to find out why this happens.\n  The technique of using shared memory allows other processes to gain access certain regions of the address space. Both processes have to be aware that the memory is not protected by the OS. A programming API for using shared memory is provided by POSIX (Portable Operating System Interface, a set of standards implemented by most UNIX OSes). The example below shows a producer on the left (a process which puts data inside of the shared memory) and a consumer on the right (which uses the data it gets from the producer).\n// PRODUCER  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/shm.h\u0026gt;#include \u0026lt;sys/mman.h\u0026gt; int main() { const int SIZE = 4096; /* buffersize (bytes) */ const char *name = \u0026#34;OS\u0026#34;; /* shared memory object name */ const char *data_0 = \u0026#34;Hello\u0026#34;; const char *data_1 = \u0026#34;World!\u0026#34;; int shm_fd; /* shared memory file descriptor */ void *ptr; /* create the shared memory file descriptor */ shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666); /* configure the size of the shared memory file */ ftruncate(shm_fd, SIZE); /* memory map the shared memory file */ ptr = mmap(0, SIZE, PROT_WRITE, MAP_SHARED, shm_fd, 0); /* write to the shared memory file */ sprintf(ptr,\u0026#34;%s\u0026#34;,data_0); ptr += strlen(data_0); sprintf(ptr,\u0026#34;%s\u0026#34;,data_1); ptr += strlen(data_1); return 0; } \r\r// CONSUMER  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/shm.h\u0026gt;#include \u0026lt;sys/mman.h\u0026gt; int main() { const int SIZE = 4096; /* buffersize (bytes) */ const char *name = \u0026#34;OS\u0026#34;; /* shared memory object name */ int shm_fd; /* file descriptor */ void *ptr; /* open the shared memory file */ shm_fd = shm_open(name, O_RDONLY, 0666); /* memory map the shared memory file */ ptr = mmap(0, SIZE, PROT_READ, MAP_SHARED, shm_fd, 0); /* read from the shared memory file */ printf(\u0026#34;%s\u0026#34;,(char *)ptr); /* remove the shared memory file */ shm_unlink(name); return 0; } \r\r\r p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nNote: to compile these on Linux, you need to pass the -lrt flag to gcc (\u0026ldquo;link with library rt\u0026rdquo;) like so: gcc -o producer producer.c -lrt. MacOS relies less on auxiliary external libraries; the flag can safely be omitted there.\nThis very simple example uses shared memory. Try to find answers to the questions below:\n Try to find out what these programs do What is the size of the memory that is shared ? Can a producer read from the shared memory ? Can a consumer write to the shared memory ? How do both processes know which data is shared ? In other words, how does the consumer decide which memory it connects to? Do both processes have to be active at the same time for the memory sharing to work? Why (not)?    Using shared memory is handy, but also not ideal in terms of security. For example, can you see any way in the API above to make sure only authorized programs can read/write to the shared memory? In this setup, any program which knows the name of the shared memory block and runs with sufficient permissions (remember chmod?) can also access the shared memory.\nMessage passing The second technique for for InterProcess Communication (IPC) comes in the form of message passing. This method is a bit more restricted than using raw shared memory, but also easier to use and safer because of that. Here we touch on 2 different mechanisms for achieving this: signals and pipes.\nSignals Signals are the cheapest form of IPC. They literally allow one process to send a signal to another process, through the use of the function kill(). Although due to historical reasons the name might be a bit misleading (originally, the only defined signal was used to stop a process, other uses only came later), it can be used to send different signals. Signals here are very simply numbers with a predefined meaning (an \u0026ldquo;enum\u0026rdquo; if you will) that you can send to another process. Depending on the number received, a different action is expected to be taken. For example, if you use the Ctrl+c keyboard shortcut to stop a running program, the shell sends a SIGINT (signal interrupt) to the current program. Similarly, Ctrl+z will send a SIGTSTP signal.\nThe snippet below shows the different types of signals that can be sent. Many of them have to do with stopping other processes, but with subtly different effects (for example, stop a process -immediately- versus allowing it to safely exit itself).\njvliegen@localhost:~/$ kill -L 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX There is also has CLI-compatible command kill that can send these signals to any running process (addressed by their PID). For more information on the kill command, add the --help argument, read the man-page (man kill), or ask the Internet.\nLet\u0026rsquo;s illustrate this with an example:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; #define DURATION_IN_MINUTES 10  int main(void) { int i = DURATION_IN_MINUTES * 60; for(;i\u0026gt;=0;i--) { printf(\u0026#34;TIMER: 00:%02d:%02d\\n\u0026#34;, (int)((i-i%60)/60), i%60); sleep(1); } return 0; } This program will emulate an egg timer. Every second it displays how much time is left. Once the process starts running, it takes 10 minutes to complete. This process can be stopped by just pressing CTRL+C. Note that you don\u0026rsquo;t have to manually do anything for this to work: your program automatically listens for this signal and exits the program when it is received. This is achieved through a combination of the gcc compiler (which adds code for signal handling) and the OS executing that code when needed.\njvliegen@localhost:~/$ ./egg_timer.bin TIMER: 00:10:00 TIMER: 00:09:59 TIMER: 00:09:58 TIMER: 00:09:57 TIMER: 00:09:56 ^C Another way to kill the process would be to explicitly send the signal through the kill command. To use this command, the PID is needed as an argument. Through a new CLI-window, this PID has to be searched for first. Note that the type of signal is an argument in the command.\njvliegen@localhost:~/$ ps ux | grep timer jvliegen 5041 0.0 0.0 4504 772 pts/1 S+ 06:04 0:00 ./egg_timer.bin jvliegen 5066 0.0 0.0 21996 1080 pts/2 S+ 06:05 0:00 grep --color=auto timer jvliegen@localhost:~/$ kill -KILL 5041  Try this for yourself. You can also use the ./longhello program from Section 6.1. Run this program and try to kill it using both approaches that were explained above. (Of course this means you shoud run it again, after you killed it the first time ð )\n  Although there are numerous uses for sending signals between signals, one more example is interesting to have a closer look at. Above there was already some hinting to CTRL+Z.\nThe CLI is running a shell, as you already know by now. This offers just a single interface. If you were to start a program, that CLI is occupied (you cannot type or execute any commands). Imagine you are working remotely on a server (e.g., through ssh): this would require you to open up a new connection to the server and have a second shell at your disposal every time you executed a longer running command (e.g., starting a web server). A more convenient solution would be to send the running program to the background.\n  An example of a program that needs to be killed with CTRL-C   Before you can send processes to the background, the process has to be halted first. This can be done through the CTRL+Z shortcut. With a halted process, the command bg sends the halted process to the background. If you do not send it to the background, the process will freeze. Once it is in the background it unfreezes and continues running. Additionally, this gives you back your shell.\njvliegen@localhost:~/$ xeyes ^Z [1]+ Stopped xeyes jvliegen@localhost:~/$ bg [1]+ xeyes \u0026amp; jvliegen@localhost:~/$ For the sake of completeness we enumerate a few more usefull aspects about this:\n a process can be started in the background as well. This can be achieved by adding an ampersand after the command (e.g., xeyes \u0026amp;) the command jobs gives you an overview of which jobs are running in the background through the command fg \u0026lt;#\u0026gt; the job with index number \u0026lt;#\u0026gt; will pulled to foreground.  Try this for yourself. If the xeyes program is not installed, install it first or use the longhello program from before.\n  Pipes Another option to achieve message sending is through pipes. There are two different types of pipes available:\n anonymous pipes named pipes  Anonymous pipes are like waterslides. You can put some data on it on one end (the top of the slide) and it comes out the other (the bottom), but it\u0026rsquo;s not possible to go up the waterslide from the bottom. Put differently: communication is half-duplex (single direction). One process can write into the pipe, while the other can read from of the pipe. This type of pipe can only be create between two processes that have parent-child relationship. What happens internally is that the stdout of the first process is mapped to the stdin of the second process. For this, we use the | (pipe) character.\nWhen using the CLI, anonymous pipes are a very powerful tool for chaining different commands. The output of the first command will be the input for the next command. This can be chained multiple times.\njvliegen@localhost:~/$ xeyes \u0026amp; jvliegen@localhost:~/$ ps -ux | grep xeyes | head -1 | cut -d \u0026#34; \u0026#34; -f 3 5526 jvliegen@localhost:~/$ The example above chains the following:\n give a list of all my processes (ps = process status) only filter the lines that contain the word xeyes (grep stands for Global Regular Expression Print) filter only the first line (head) split the input on a space (\u0026quot; \u0026ldquo;) and report only the third field (which is the process ID)  Let\u0026rsquo;s try something similar for yourselves:\n Use anonymous pipes to display all the processes of which you are the owner. From these processes only display the PID and the first 10 characters of the process\u0026rsquo;s name (the COMMAND column). From this list, only show the first 10 processes. Then, add another command to sort the output by descending PID (so the largest PID is on top, the smallest on the bottom)    An example output     Do you remember the Process Control Block ? This has one field called list of open files. We\u0026rsquo;ve already touched upon stdin, stdout and stderr. Using anonymous pipes will add an entry to this list.\nWe can also relink the 3 default open files to other targets. For example, instead of writing output and errors to the command line, we can redirect them to a file. Similarly, we can read input from a file instead of from the keyboard:\n  Redirection of the standard output    The syntax for this is a bit weird though: 1\u0026gt; is meant to redirect data that normally goes to stdout, while 2\u0026gt; is used to relink stderr. You can also point directly to the existing stdout/stderr by using \u0026amp;1 or \u0026amp;2 respectively:\n process 1\u0026gt;{STDOUT} 2\u0026gt;{STDERR} process 1\u0026gt;{STDOUT} 2\u0026gt;\u0026amp;1 process \u0026lt; {STDIN} (read from a file at location {STDIN} rather than from the keyboard)  Note that here we\u0026rsquo;re using the \u0026gt; pipe here instead of | as above. The difference is subtle, but a simple explanation is that \u0026gt; deals with mapping a command to a file (or something that pretends to be a file, like stdout/stderr), while | maps a command to another command.\nSince STDIN is \u0026ldquo;just a file\u0026rdquo; in Linux, we can also read from that file to get input into our program.\nLook up yourself online how to read data from STDIN and write a small C program that reads (some) data from STDIN and prints it to STDOUT using printf().\nThen use a command like this to have data from 1 file pass through your program and into another:\ncat input_file.txt | ./my_program 1\u0026gt; output_file.txt\nAlso try to do input_file.txt \u0026gt; ./my_program 1\u0026gt; output_file.txt and explain why that doesn\u0026rsquo;t (seem to) work.\n  Named pipes are the other type of pipes that can be created. The main differences with anonymous pipes are the lifetime of this mechanism and their presence in the file system.\nThe anonymous pipes above only live for as long as the processes live. Named pipes instead persist and have to be closed explicitly (or are closes automatically at system-shutdown).\nNamed pipes also have an actual presence in the file system. That is, they show up as files. But unlike most files, they never appear to have contents. Even if you write a lot of data to a named pipe, the file appears to be empty. Making named pipes can be done through the mkfifo command.\nAs they are not frequently used, we direct the interested reader to man pages.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/towards/",
	"title": "7.3: Towards real-world schedulers",
	"tags": [],
	"description": "",
	"content": "The previously discussed scheduling algorithms are but a select number of a huge amount of imaginable approaches that can be thought of. We have seen that all individual algorithms come with certain challenges/downsides that make them difficult for direct use in real-world scenarios. And we haven\u0026rsquo;t even taken into account all variables that are in play in a typical OS!\nAs such, in this Section, we first look at a few factors that come into play in real systems. We then look at how the naive schedulers we\u0026rsquo;ve already seen can be adapted to deal with these new problems. Finally, we discuss how all of this has been combined in practice in the Linux OS scheduler over time.\nReponsiveness vs Efficiency In theory, we could say that using a Round-Robin (RR) scheduler would be a good starting point. After all, this is a very fair scheduler that ensures that all processes get at least some time on the CPU (so there is no starvation as with a pure priority-based scheduler).\nIn this section, let\u0026rsquo;s start from a simple RR scheduler and discuss why it\u0026rsquo;s difficult to make it work properly in a real-world setting. This is mainly due to three factors:\n Context switching overhead: this limits how often we can switch between tasks I/O vs CPU-bound processes: this means we can\u0026rsquo;t use a single time slice length for all tasks Need for priorities: while pure priorities aren\u0026rsquo;t optimal, we still want to use their concept to be able to speed up certain key tasks  Context Switching Overhead As said previously, when a new task is scheduled for execution by the OS, a number of operations need to happen to swap the old task with the new one. In the previous examples, we\u0026rsquo;ve pretended these operations happen instantly (the context switching overhead was 0), but that\u0026rsquo;s of course not the case.\nSay we have 2 user tasks: X and Y. X has already been running for a while on the processor, while Y is in the ready state, waiting for CPU-time. After X\u0026rsquo;s time slice is up, the OS uses the RR scheduler to select Y as the next task to run. Switching between the tasks involves the following steps:\n  Step 1\nX has be stopped in such a way that it can continue from where it left off the next time it is scheduled. Therefore, a snapshot has to be made: what is the value of the program counter, what are the values in the registers, which address is the stack pointer pointing to, what are the open files, which parts of the heap are filled, \u0026hellip; ? All these values need to be stored. As was discussed in Chapter 6, the OS keeps a PCB (Process Control Block) for every process, which contains the fields necessary to store all these required parameters: The PCB of X needs to be updated to the current CPU. The kernel then puts this PCB in a list of paused tasks.\nNote: something similar happens for Threads (remember there\u0026rsquo;s also a conceptual TCB), but it\u0026rsquo;s more lightweight, since there is more shared state between threads in the same program and so fewer aspects need to be updated.\n  Step 2\nX is removed from the processor. For example, the register contents and program counter are cleared.\n  Step 3\nThe scheduler uses the RR scheduling algorithm to determine which process is next. Since Y is the only other process, it is selected. The processor searches Y\u0026rsquo;s PCB in its list of paused tasks.\n  Step 4\nEverything that happened to the PCB of X, now needs to be done in the opposite direction with the PCB of Y: the PCB of Y needs to be restored. The program counter is read and the next instruction is loaded. The values of the registers are restored. The stack pointer is updated.\n  Step 5\nY starts to run on the processor.\n  In the previous Section, we have called this series of actions \u0026ldquo;Dispatching\u0026rdquo;. As such, the time it takes for the dispatcher to stop one process and start another running is known as the dispatch latency, or the scheduling latency. The actions of updating (and restoring) individual PCB\u0026rsquo;s is typically referred to as a context switch (though some sources also call the entire process together a context switch).\nAs you can see, during the dispatch period, the CPU is not actually doing any useful work: it is waiting and/or updating its state so the new task can start running. As such, the act of dispatching a new process is considered 100% overhead, and it should be avoided as much as possible. Note: there are also other aspects that make context switches slower, such as the fact that it often means that data in the cache memory is no longer useful. As the cached data belongs to the previous task, the cache needs to be (partially) flushed and updated with data from the new task as well, which again takes time.\nHowever, before you get the wrong idea, it\u0026rsquo;s not all that bad. In practice, the dispatch latency is typically in the order of (10) microseconds (say about 1/100th of a millisecond). Still, if we were to switch processes for example each millisecond, we would have a full 1% overhead, which over time definitely adds up (we don\u0026rsquo;t only spend more time context switching, we also do a larger amount of context switches over time). If you recall from the previous section, we introduced the metric CPU efficiency (Î·CPU), which helps make concrete how much overhead actually was introduced.\nWe can see that we somehow need to strike a balance between being CPU efficiency (less overhead) and keeping the system responsive (switching between tasks often enough). This is easy enough in our simple examples with just 3-10 tasks, but modern systems often run hundreds of tasks at the same time.\nThe length of the RR time slice thus plays a large part in this: shorter time slices make things more responsive, but cause more context switches, and vice versa. As such, we want to determine an ideal time slice length, but it\u0026rsquo;s not easy to see how this can be accomplished. In general, we can really only say that the time slice should always be quite a bit larger than the dispatch latency, but that we don\u0026rsquo;t really have an easy way to determine an upper bound.\nI/O-bound vs CPU-bound tasks A second aspect that\u0026rsquo;s highly relevant in modern systems is that there are typically two large classes of tasks: I/O-bound and CPU-bound tasks.\n  The I/O-bound tasks typically run for only short amounts of time (a few milliseconds) before they already have to wait for an I/O operation. Put differently, these tasks often pause themselves (go into the \u0026ldquo;waiting\u0026rdquo; state) often. A good example is a program that\u0026rsquo;s listening for user input (keyboard/mouse). These tasks are thus sometimes also referred to as interactive tasks. However, waiting for packets to come in from the network is also an I/O operation, as is for example waiting for a large file to be loaded from hard disk into RAM memory so it can be used. More generally, an I/O bound task is a process that can\u0026rsquo;t execute (many) useful instructions on the CPU (at this time) because it doesn\u0026rsquo;t have the necessary data/input available (yet).\n  The CPU-bound tasks typically do not require much outside input and/or mainly have to run a lot of calculations on the data. As such, if the data is available, these tasks can keep issueing instructions to the CPU for a long time without pause. They run for tens of milliseconds (or much more) without ever yielding/waiting themselves. These jobs typically process data in large chunks, and are sometimes called batch tasks.\n  The fact that there are typically few processes that do \u0026ldquo;something in between\u0026rdquo; (few programs can do a medium amount of calculations with a medium amount of data) again makes it difficult to determine a good time slice length:\n  If there are many I/O-bound tasks, shorter timeslices are probably better, as most tasks will pause themselves frequently anyway, and we don\u0026rsquo;t loose much (extra) efficiency for higher responsiveness.\n  On the other hand, if there are many CPU-bound tasks, longer timeslices are probably better, as processes will typically fill their slices with useful work and we reduce the amount of context switches (and those tasks typically don\u0026rsquo;t need to be very responsive).\n  Note that many processes switch between being I/O-bound and CPU-bound over the course of their execution. Take for example Photoshop: here, you often want to first load a (very) large image file into RAM memory from disk (or network nowadays) to then use advanced image processing tools on it (e.g., apply a sepia filter).\nAs such, while waiting for the image data to become available on the CPU, Photoshop is I/O bound and can\u0026rsquo;t do much. However, once the data is available, it will want to execute heavy calculations on it, causing it to become CPU-bound for the duration of the calculations.\n Simply using an average time slice that\u0026rsquo;s \u0026ldquo;somewhere in between\u0026rdquo; can produce the worst of both worlds: it lowers the responsiveness in interactive systems (as batch processes delay interactive processes), while (needlessly) increasing the amount of context switches during batch processing.\nAs in the previous subsection, it\u0026rsquo;s unclear how long a time slice should ideally be to deal with both I/O and CPU-bound tasks, and especially with tasks (like Photoshop) that can switch from one category to the other, depending on what they\u0026rsquo;re doing at a given time.\nnot correct. Concluding that a system with a lot of IO-intensive tasks is better of with a smaller time slice, and a system with a lot of CPU-intensive tasks is better of with a larger time slice, would be more correct. If the latter is not the case, the CPU will not only spend a large percentage of time context switching, it will also do a larger amount of those context switches over time. As a rule of thumb it can be assumed that the time for a context switch is (a little) less than 10% of the time slice. --  Images like the one above, we've seen a number of times up until now. A question that arises is: **what happens on the dotted line ?** As was mentioned before the scheduler has two main jobs: 0. Choose the next task task that is allowed on the processor 0. Dispatching: switching tasks that are running on the processor The algorithms provide the scheduler with an approach to **choose the next task**. The second function a scheduler has is the **dispatching** of the newly chosen task. Let's break it down. ## Dispatching There are 2 user jobs: X and Y. X is running on the processor while Y is in the ready state, waiting for CPU-time. The scheduler decides that X's time is over an it's Y's turn on the processor. -- Singular time slice length example Let\u0026rsquo;s illustrate this with an example:\n There are two I/O-bound tasks, T1 and T2. Both run for 1ms, in which they update state, and then wait/yield for more input.  Input becomes available after 4ms of wait time (starts when the task yields). The task then becomes \u0026ldquo;ready\u0026rdquo; to process this input. Both tasks do three rounds of this (wait for input 3 times in total)   There is one CPU-bound task, T3, that runs a total of 10ms without yielding All three tasks start/arrive at 0s and input is available for T1 and T2 at 0s The two I/O-bound tasks are higher priority than the CPU-bound task. Each task gets to complete its full time slice unless it yields by itself. In this very unrealistic system, the dispatch latency is a full 1ms  Draw schemas of how these tasks would be scheduled in two scenarios:\n with a time slice of 2ms with a time slice of 5ms.  Indicate clearly each time a task goes into a ready state and don\u0026rsquo;t forget to take into account the high dispatch latency!\nFor each scenario, calculate the CPU efficiency Î·CPU (the percentage of time that the processor performs actual work: total runtime - dispatch latency overhead). Note that calculating AJWT is less useful here to compare both scenarios, as we have multiple waiting periods! As such, focus on the AJCT and calculate that for the three tasks as well.\nAnswer these questions:\n How many context switches are there in each scenario? Which scenario is more efficient? Why?    Comparison between two time slices     Dynamic Time Slice Length As we can see from the example above and the discussion before that, it\u0026rsquo;s indeed difficult to find an optimal, singular time slice length for all processes in a system over time.\nHowever, there is a relatively simple insight that we should get by now: if the optimal time slice depends on how often the processes pauses itself (which is often for I/O-bound, and more rare for CPU-bound), we can start assigning dynamic time slice lengths to processes, depending on their past behaviour!\nFor example, if task X got a time slice of 20ms, but paused itself after just 5ms, we might decide to give it just a 10ms time slice the next time it\u0026rsquo;s scheduled, as we expect it to be I/O-bound. Inversely, if task Y does run for its complete 20ms time slice, we might increase that to 30ms the next time, as it\u0026rsquo;s probably CPU-bound.\nNote that, even if a task evolves from I/O bound to CPU-bound (or back), the logic still works! The time slice will keep adjusting itself dynamically over time to accomodate whatever a task needs at a given time.\nThis is a very elegant yet powerful solution that is indeed used in practice. However, it also has some challenges. Two of the main ones are:\n We still need lower and upper bounds for the time slice lengths, which are not trivial to determine CPU-bound tasks can still end up blocking I/O-bound or interactive tasks, especially if the CPU-bound tasks get a relatively large time slice!  This latter point isn\u0026rsquo;t a problem if the CPU-bound task is something the user is actually waiting for (e.g., using Photoshop). However, if it\u0026rsquo;s a background process that\u0026rsquo;s processing data (for example updating a search index, scanning files for viruses), we would rather not have that process interupting more important tasks (e.g., while the user is using a Web browser).\nAs such, even with dynamic time slice lengths, we still need to add the concept of priorities of processes, to make sure we can manually (using C APIs, see the lab in 7.4) or automatically (e.g., which window is the user currently interacting with) ensure important processes are given more CPU time.\nTry to understand the load that is put on the CPU. There is a periodic pattern.\nTry to draw the repeating pattern in this timing diagram How many context switches are there ? What is the efficiency of the CPU ? (Reminder: CPU efficiency (ï¿½CPU): the percentage that the processor performs actual work.) Tip: read the title of this section !!    Time slice = 10 ms  Answer:  2. There are 11 context switches. 3. \u0026#0951;CPU = tuseful / ttotal = 110 ms / 121 ms = 0.90909  \u0026nbsp;tuseful = 10 x tIO + 1 x tCPU = 10 x 10ms + 1 x 10ms = 110ms \u0026nbsp;ttotal= tuseful + toverhead = 110 ms + 11 x 1 ms = 121 ms    ### The time slice is 100 ms Try to understand the load that is put on the CPU. There is a periodic pattern.\nTry to draw the repeating pattern in this timing diagram How many context switches are there ? What is the efficiency of the CPU ? (Reminder: CPU efficiency (ï¿½CPU): the percentage that the processor performs actual work.) Tip: read the title of this section !!    Time slice = 100 ms  Answer:  2. There are 11 context switches. 3. \u0026#0951;CPU = tuseful / ttotal = 200 ms / 211 ms = 0.94787  \u0026nbsp;tuseful = 10 x tIO + 1 x tCPU = 10 x 10ms + 1 x 100ms = 200ms \u0026nbsp;ttotal= tuseful + toverhead = 200 ms + 11 x 1 ms = 211 ms    -- Priorities As discussed in the previous section, we often want to explicitly indicate a given task is more important than another. This is usually done using priorities, whereby each task is assigned a number so they can be fully ordered to determine which is most important.\nIn the simple Priority-based scheduler we\u0026rsquo;ve considered, the priority was mainly used to determine when to start which process, as higher priority processes are selected earlier. However, we\u0026rsquo;ve also seen that this could lead to starvation for low-priority tasks, needing some ageing mechanism (whereby the priority is slowly increased for low-priority tasks over time) to correct this.\nApplying this idea that priorities influence when a task is scheduled to our preemptive RR scheduler with dynamice time slices, we see it becomes more like an how often is a task to be scheduled! For example, higher priority tasks can be scheduled more often than/before lower priority tasks, independent of their (dynamic) time slice length. While that works conceptually, it does not really help prevent starvation.\nHowever, can we not think of another, quite different, way to interpret priorities rather than when a task is scheduled?\nImagine for a second that, rather than controlling the timing of when a task is scheduled, instead we use the priority (rather than how often a task pauses itself) to determine the per-process time slice length.\nFor example, high priority jobs could get a longer time slice (say 10ms) to make sure they get to do as much work as possible, while lower priority tasks could get less time (say 2ms per burst). We can then use the simple RR scheduler between the different tasks, as the priorities are enforced by the time slice length, rather than by strict execution order/scheduling time. Lower priority processes would get time on the CPU more often than with a direct priority-based scheduler, but in shorter bursts, solving ageing while keeping relative priorities intact.\nThis seems like a good idea, but we can again question if this will work well in practice. For example, say the high priority tasks in a system are I/O-bound and the low priority tasks are CPU-bound, the proposed system seems to do the exact opposite of what we want (as I/O-bound tasks don\u0026rsquo;t need long time slices, but batch jobs do).\nAs such, we now have two competing ways to determine our dynamic time slice length:\n Based on how often a task pauses itself / how often it uses its entire current time slice Based on the task\u0026rsquo;s priority  It is difficult to know which of both is optimal for any given use case, and as you might predict, in practice a combination of both is used (see the Linux schedulers below). This also still doesn\u0026rsquo;t solve our lower/upper bound for time slice lengths.\nA dynamic solution To summarize our discussion so far: at this point it\u0026rsquo;s clear that we have multiple different requirements of a real world scheduler: it needs to be both responsive and CPU efficient, it needs to support both I/O-bound and CPU-bound tasks in a decent way, and it needs to have support for per-task priorities to allow further tweaking of scheduling logic.\nAs we\u0026rsquo;ve seen, the concept of a Round-Robin scheduler using dynamic time slice lengths is a promising solution, but we\u0026rsquo;re still not sure how to properly choose the time slice lengths\u0026hellip; If we were to think about this further, we would end up at the conclusion that there is no single optimal answer and we will have to use combinations of different options to reach good results.\nThe general concept of such a solution that combines multiple options is the multi-level feedback queue scheduler. In this setup, we no longer have a single long list of processes, but instead distribute them across multiple, independent \u0026ldquo;run queues\u0026rdquo;. Each of these queues can then employ their own scheduling logic (for example use FCFS or RR or even priority-based) and determine other parameters such as if the queue is processed cooperatively or preemptively (in which case, the time slice length can also vary). That\u0026rsquo;s the \u0026ldquo;multi-level\u0026rdquo; part.\nThe \u0026ldquo;feedback\u0026rdquo; part indicates that tasks can move between these separate queues over time (for example as they become more or less important (change priority), as they run for longer or shorter bursts, etc.).\nWe can then see that we also need a sort of top-level scheduler, that determines how the different run queues are processed (for example, queue 2 can only start if queue 1 is empty if we use FCFS between the queues).\nOne of the first examples of this approach was given by Fernando J. CorbatÃ³ et al. in 1962. Their setup has three specific goals:\n Give preference to short jobs. Give preference to I/O-bound tasks. Separate processes into categories based on their need for the processor.  To achieve these goals, they employ three differen run queues:\n  When a newly created process is added to the scheduler, it arrives at the back of the top queue (8ms). When it is scheduled, there are two options: (a) either it runs the full 8ms or (b) it yields before that. In the case of (b), it\u0026rsquo;s likely that we have a short and/or I/O bound task. As such, when it is done waiting, it is appended at the back of the top queue again.\nIn the case of (a) however, it\u0026rsquo;s more likely that we have a CPU-bound task. As such, after the 8ms, it is pre-empted and we move it down to the middle queue (16ms), where it should get a longer time slice next time it is run. We can see this improves efficiency, as we can assume the task will remain CPU-bound and thus we have only half the context switches for these processes!\nIf the processes in the middle queue keep running to their full time slice of 16ms multiple times, this is an indication they are very heavily CPU-bound. In response, we move them down to the bottom queue. Here, processes are run in FCFS fashion until completion.\nFinally, processes can move up to the previous queue if they yield to an I/O operation. This allows for example mostly batch tasks to still get a bit more execution time if they have phases in their programming that requires some I/O work.\nAcross the three different run queues, a simple FCFS logic is applied: the top queue is processed until it is empty and only then are tasks from the middle queue scheduled. Note: if I/O bound tasks are waiting, they are of course no longer in the top queue, otherwise the bottom queues would never get any time! Only tasks ready to execute are in the queues.\nAs said in the previous Section, it is difficult to do Shortest Job First (SJF) scheduling, since it\u0026rsquo;s difficult to know the total duration of a job. This type of setup however tries to approximate this logic by looking not at the total duration of a job, but at the duration of individual \u0026ldquo;bursts\u0026rdquo;. Longer jobs automatically move down to the lower queues, leaving more room for jobs with shorter bursts at the top.\n The setup described above is of course highly specific to those three goals and needs of a particular system. The concepts of the multi-level feedback queue are however much more flexible, as we can also envision other ways of partioning queues to model other advanced scheduling setups. For example:\nEach level can represent a separate priority (doing for example RR within each level gives us the simple Priority-based scheduler from the previous Section) Each level can represent a separate scheduler (the first level can for example do RR, the next FCFS, the next priority-based, etc.) Each level can represent a different time slice length (the first has slices of 8ms, the next 16ms, etc.)  Between the levels, we can then also employ other schedulers than FCFS of course (e.g., a RR scheduler, a priority-based scheduler etc.) to improve the responsiveness of tasks in the lower levels.\nIn practice, these aspects are often combined in specific ways to get a desired outcome (as with the example above). This outcome depends on the system and intended usage. We will see several options for this in the next Section on Linux schedulers. In some way, most modern OS schedulers are variations on the general multi-level feedback queue scheduling concept.\n #### Multiple FIFOs A priority based system might use the exact same scheduling algorithm, with the exception of priorities. A solution could be to use multiple FIFOs: one FIFO for each priority level. **When a process is created**, it is simply added to the back of the queue that matches the process's priority level.   #### Tree When a more complex algorithms are used in the scheduler, a tree might suit the needs better. Depending on the strategy a tree might be ordered in a certain way. For example, in a **shortest-job-first** algorithm, jobs may be ordered (from **short** to **long**) in the tree from **left** to **right**. **When a process is created**, calculations have to be done to determine the position in the three of the new process.   ### Multi-level feedback queue -- Linux Schedulers Now that we\u0026rsquo;ve explored some of the practical issues with real-world scheduling and introduced a basic solution framework, it\u0026rsquo;s time to look at how things are practically done in the Linux OS. This again goes one step beyond the scheduling logic, as we now need to also take into account performance of the implementations and datastructures, as well as the provided API for programmers (for example, how to actually manipulate priorities in practice).\nOver time, the Linux kernel has used different schedulers, of which we will discuss three here. Linux kernels with version 2.4 - 2.6 (before 2003) were using the O(n) scheduler, in 2.6 - 2.6.11 (2003-2007) the used scheduler was O(1), and from 2.6.12 (after 2007) onward the Completely Fair Scheduler (CFS) is mainly used. These schedulers are briefly touched upon here. All of these are preemptive schedulers that incorporate priorities, but as we will see, they do this in various different ways.\nYou might be confused by O(n) and O(1). This \u0026ldquo;Big Oh\u0026rdquo; notation is an often used concept in computer science to indicate the worst-case performance (called \u0026ldquo;time complexity\u0026rdquo;) of a program. In general, the factor inside of the O() function should be as small as possible. As such O(1) is optimal (\u0026ldquo;constant time\u0026rdquo;), while O(n) indicates that in the worst-case, the program scales linearly with (in this case) the amount of tasks (n). Exponential setups like O(n*n) and especially O(2^n) are to be avoided. In practice, O(log N) is often the best you can do.\n O(n) scheduler The O(n) scheduler got his name from the fact that choosing a new task has linear complexity. This is because this scheduler uses a single linked list to store all the tasks. Upon each context switch, the scheduler iterates over all the ready tasks in the list, (re-)calculating what is called a \u0026ldquo;goodness\u0026rdquo; value. This value is a combination of various factors, such as task priority and whether the task fully used its allotted time slice in its previous burst. The task with the highest goodness value is chosen to run next.\n  This setup combines some of the aspects of the multi-level feedback queue concept, but in a single datastructure. For example, if a task didn\u0026rsquo;t use its entire alotted time slice, it gets half of the remaining time alotted for its next run (somewhat bumping its priority, as the alotted timeslice is taken into account with the goodness value as well). As such, while each task is typically assigned the same time slice length initially, this starts to vary over time.\nIn practice, this scheduler works, but it has severe issues. Firstly, it is somewhat unpredictable (e.g., the time slice could grow unbounded for very short processes, meaning we need additional logic to deal with this). Secondly, and most importantly in practice, the performance was too low. Because each task\u0026rsquo;s goodness needs to be caclculated/checked on every context switch (the O(n)), this adds large amounts of overhead if there are many concurrent processes. Thirdly, it also does not scale well to multiple processors: each processor is typically scheduled independently, meaning that for each CPU a mutex lock had to be obtained on the single task list to fetch the next candidate.\nO(1) scheduler Given the problems of the previous O(n) scheduler, a new, much more advanced version was introduced. One of its main goals was to reduce the time it takes to identify the next task to run, which can now be done in constant time, expressed as O(1).\nTo understand how exactly this works, we first need to understand how Linux practically deals with priorities, since the O(1) scheduler is tightly integrated with this.\nLinux defines task priority as a value between 0 and 139 (so a total of 140 different priorities). 0 is the highest priority, 139 the lowest (somewhat unintuitively\u0026hellip;). The range 0-99 is reserved for so-called \u0026ldquo;real time\u0026rdquo; tasks. In practice, these are kernel-level tasks (as the kernel of course also has internal things to do). These also for example include the concrete I/O operations (for example reading from disk), as other (user-space) (I/O-bound) tasks might be waiting for that. The range between 100 and 139 then is reserved for user-space processes, sometimes called time sharing or interactive processes.\nHowever, programmers don\u0026rsquo;t manually assign priorities between 100 and 139. Instead, Linux APIs add an additional abstraction on top called the \u0026ldquo;nice value\u0026rdquo;. These nice values are from the range [-20,19], which maps directly onto the \u0026ldquo;real\u0026rdquo; priorities in [100,139]. When a process is nicer to other processes (a higher nice value), it means it doesn\u0026rsquo;t mind giving some of its time to other processes. As such, a higher nice value means a lower priority (just like a higher priority number also indicates a lower priority conceptually).\nWhat should be the default nice value (or priority) that is given to a user process?\n  The clip below tries to illustrate the effect of the overall priority.   (The code for the examples in the video can be found here).\nThe O(1) scheduler creates a new queue (linked list) for each of these 140 different priority values. For the real-time tasks (queues 0-99), processes within each priority list are scheduled either FCFS or RR, which can be toggled by the user (look for SCHED_FCFS and SCHED_RR). The user-space tasks (100-139) are typically scheduled RR per priority (SCHED_NORMAL) but they can also be scheduled based on remaining runtime to improve batch processing (SCHED_BATCH). Each priority list is emptied in full before the next priority list is considered. At every context switch (at every time slice), the highest priority list with a runnable task is selected.\n  If we were to use this setup directly, the lower-priority tasks would very often by interrupted by higher-priority ones and we again get the problem of starvation. To prevent the need for manual priority adjustment with ageing, the O(1) scheduler instead uses a clever trick, by introducing a second, parallel datastructure. As such, there are two groups of 140 queues. The first is called the \u0026ldquo;active\u0026rdquo; queue, the second the \u0026ldquo;expired\u0026rdquo; queue. When a task has consumed its time slice completely (either in 1 run, or by yielding multiple times), it is moved to the corresponding \u0026ldquo;expired\u0026rdquo; queue. This allows all processes in the active queue to get some time. When the active processes are all done, the expired and active lists are swapped and the scheduler can again start with the highest priority processes.\nThis setup is efficient, because we no longer need to loop through all tasks to find the next one: we just need the first task in the highest priority list! As long as processes are added to the correct priority queue, this can be done in constant time. Some psuedocode to illustrate these aspects can be found below:\n// pseudocode!!! // in reality, the data structures and functions look different!  struct PriorityTask { struct Task *task; // for example the PCB  struct PriorityTask *next; } struct PriorityList { struct PriorityTask *first; struct PriorityTask *last; } struct RunQueue { struct PriorityList *tasks[139]; // 140 linked lists, 1 for each priority } void appendToList(struct PriorityList *list, struct Task *newTask) { // TODO: make sure list-\u0026gt;last exists etc.  list-\u0026gt;last-\u0026gt;next = newTask; list-\u0026gt;last = newTask; } struct RunQueue *active; struct RunQueue *expired; // new task is started with priority x appendToList( active-\u0026gt;tasks[x], newTask ); // scheduler wants to start a new task // loops over \u0026#34;active-\u0026gt;tasks\u0026#34; from 0 to 139, looking for the first non-empty list, with index y // Note: in reality, a bitmap is used to prevent the need to loop (see below), keeping things O(1) struct PriorityTask *runTask = popFirstFromList( active-\u0026gt;tasks[y] ); execute( runTask-\u0026gt;task, runTask-\u0026gt;task.timeslice ) // task is done running and has consumed its timeslice completely appendToList( expired-\u0026gt;tasks[x], runTask ); // OR: task is done running and hasn\u0026#39;t consumed its timeslice yet (waiting state) runTask-\u0026gt;task.timeslice = leftoverTimeslice; appendToList( active-\u0026gt;tasks[x], runTask ); // if all lists in \u0026#34;active\u0026#34; are empty (no \u0026#34;y\u0026#34; found): swap both runqueues and start over struct RunQueue *temp = active; active = expired; expired = temp;  How would you implement appendToList and popFirstFromList in practice? What other properties should struct Task have besides \u0026ldquo;timeslice\u0026rdquo;?\n  As explained in the pseudocode, we also need a clever way of knowning which priority queue still has pending tasks without looping over all of them. This can be cleverly done by using a so-called bitmap, where each individual bit of an integer is used as a boolean to indicate if there are tasks for the priority corresponding to that bit. To represent 140 bits, we need about 5 32-bit integers (total of 160 bits). Checking which bits are set can be done very efficiently. See the image below for a schematic representation:\n  As such, we can see the O(1) scheduler is an excellent example of a complex multi-level feedback queue! It utilizes several queues for the different priorities, using different schedulers per-queue depending on the real-timeness of the task. On top, it has two higher-level queues (active and expired) for which it uses an FCFS scheduler. Conceptually a different time slice could also be employed (e.g., higher priorities get a longer time slice), though this was typically not employed.\nThis scheduler is however not perfect. In practice, it turns out that I/O-bound or interactive processes could get delayed considerably by longer-running processes, due to the active vs expired setup. This caused the need for a complex set of heuristics (basically: educated guesses) that the OS would use to estimate which processes were I/O-bound or interactive. These processes would then receive an internal priority boost (again a form of ageing), while non-interactive processes would get penalized. In practice however, like with the O(n) scheduler, this process was somewhat unstable and error-prone.\nThe Completely Fair Scheduler The current default scheduler was intended to take a bit of a step back from the relatively complex O(1) scheduler and to make things a bit simpler; as we\u0026rsquo;ll see however, that\u0026rsquo;s simpler for Linux kernel developers, not necessarily for us. The CFS is a relatively complex scheduler, and as such a thorough study on this algorithm falls out of the scope of this course. We will touch upon the main concepts however.\nThe main insight in the CFS is that the size of the time slices can be highly dynamic. Previously, we\u0026rsquo;ve seen that interactive processes for example can get 8ms, while CPU-bound processes could get 16ms. That\u0026rsquo;s already nice, but it doesn\u0026rsquo;t take into account the current load of the system: if there are many different processes waiting, each will still get 8 to 16ms, causing later ones to be significantly delayed.\nThe CFS solves this problem by calculting the per-task time slice length for a given time period based on the number of ready tasks. Say that N tasks are ready and we want to schedule each of them over the next 100ms (an \u0026ldquo;epoch\u0026rdquo;). Then each task is assigned a time slice of 100ms * 1/N (if we ignore the context switching overhead for a bit). In theory, this gives each task an equal share of the processor, hence the name. As such, if there are fewer tasks active in the system, N will be lower, and the time slices will get larger, and vice versa. Of course, the CFS puts a lower bound on the time slice length (typically 4ms) as otherwise the context switching overhead could become too large.\nTo determine which task executes first within the next epoch, the CFS keeps track of how much time each task has actually spent on the CPU so far. As such, for I/O-bound processes that yield frequently, this value will be lower than for CPU-bound tasks that always use their full time slice. The scheduler always selects the process that has so far spent the least amount of time on the CPU. This automatically makes sure that interactive processes are scheduled frequently enough, but also that CPU-bound processes age correctly.\nThis timekeeping is done in a quite complex datastructure called a (binary) (self-balancing) red-black tree. The details are not important here, but this mainly means that the next task (that has spent the last amount of time on the CPU) is always the most bottom left node in the tree. As such, it can easily be retrieved with low overhead. Similarly, adding new tasks (or moving tasks around) in the tree can be done in O(log N).\n  The complexity increases even more when we look at how this setup incorporates priorities. As there are no longer explicit per-priority lists like in the O(1) scheduler, the CFS simulates this by shrinking/expanding the time slices of low/high priority processes. This is similar to what we\u0026rsquo;ve discussed above, that time slice durations can be used to emulate priorities. As such, if a high priority process executes for 10ms on the CPU, the timekeeper might only record that it spent 5ms of \u0026ldquo;virtual time\u0026rdquo;. This gives the task a \u0026ldquo;priority boost\u0026rdquo; when the scheduler next goes looking for a new task. The opposite is done for low priority tasks (e.g., 10ms of runtime can become 20ms of \u0026ldquo;virtual time\u0026rdquo;). We can see this is no longer a \u0026ldquo;completely fair\u0026rdquo; scheduler in practice, but it\u0026rsquo;s quite elegant in how it combines interactivity, priorities and time slice lengths in practice.\nFor more information on CFS you can read the kernel documentation here. For the daredevils \u0026hellip; you can even read (or modify, at your own risk) the kernel C code here.\nOther schedulers As might be expected, these are not the only schedulers that exist, even within Linux. There a many schedulers available and, certainly, there will be many more to come. Just a small grasp of existing schedulers:\n Brain F*ck Scheduler (Linux) Noop Scheduler (Linux) Task Scheduler 1.0 (Windows) Task Scheduler 2.0 (Windows) JobScheduler (iOS)  Look up at least 1 other scheduler (for example one used in Windows) and grasp its main concepts and compare it to how Linux works.\n    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/scope/",
	"title": "8.3: Different Scopes in C",
	"tags": [],
	"description": "",
	"content": "Scoping issues Now that you have some basic knowledge on the stack and the heap, it is time to take a better look at the different scopes present in the C programming language. These concepts are very important because they in part determine whether variables live (or not) - and whether anything is pushed to the local stack (or not). Let\u0026rsquo;s start with a basic example:\n#include \u0026lt;stdio.h\u0026gt; void* whats_my_age() { int age = 30; // I\u0026#39;m being generous here! } int main() { int* my_age = (int*) whats_my_age(); printf(\u0026#34;%d\\n\u0026#34;, *my_age); }  What is the output of the above program? I\u0026rsquo;m sure you know the answer, but\u0026hellip; why?\n  If you forget how void* works, please re-read chapter 4: pointers and arrays.\nYou can accidentally get the handle of another pointer that way. Inspect the following code carefully:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt; int* p; void* whats_my_age() { p = malloc(sizeof(int)); *p = 5; int age = 30; // I\u0026#39;m being generous here! } int main() { printf(\u0026#34;hey\u0026#34;); int* my_age = (int*) whats_my_age(); printf(\u0026#34;ptr p is p(%p), x(%x) \\n\u0026#34;, p, p); printf(\u0026#34;ptr my_age is p(%p), x(%x) \\n\u0026#34;, my_age, my_age); } Formatting %p (pointer) or %x (hex values) result in the same, with or without the 0x prepend, printing the raw pointer address. Are these the same or different? How about printing the values of both variables by dereferencing them and using %d? Are those the same or different? Why?\nTry these out locally or on https://godbolt.org/ using different C compilers (Clang, GCC, MSVC). Sometimes these behave differently!\nLocal variables The local variabel, age, exists as soon as whats_my_age is pushed to the stack. That automatically includes all locally defined variables. When the method is done, after the } sign, things are popped from the stack to make room for future methods and their local variables. This means whats_my_age and age disappear. Forever.\nIf we were to try and change the age variable from inside the main() method (another method, so the variable is not local anymore) by adding age = 5; in the main method, we would get a compile error:\n scoping.c:9:5: error: use of undeclared identifier 'age' age = 5; ^ 1 warning and 1 error generated.  Nothing shocking there. Identifiers are undeclared when they are not present in the program stack.\nAs a side note referenced from here, there is a way to tell C to keep a stack variable around, even after its creator function exits, and that is to use the static keyword when declaring the variable. A variable declared with the static keyword thus becomes something like a global variable, but one that is only visible inside the function that created it.\nFor instance:\nstatic int i; int j; int main() { // do something!  return 0; } We say that i is not visible outside the module, while j is visible. Again: adding static reduces visibility, it does not enhance it!\n i has what is called internal linkage. You cannot use the name i again for variables or functions in any other source files. j has what is called external linkage. You can still use j from other compilesets using the keyword extern, like this: extern int j;  The keyword extern is used to declare a C variable without defining it. You\u0026rsquo;re telling the compiler \u0026ldquo;all right, I\u0026rsquo;ve got this variable x, but it\u0026rsquo;s defined somewhere else, ok?\u0026rdquo;. Thus, it extends the visibility of the variable (or function). It\u0026rsquo;s a strange construction, one that you probably won\u0026rsquo;t need except under very specific circumstances.\nGlobal variables Once we change the above program by moving age outside of the method scope, into the global scope, we get something like this:\n#include \u0026lt;stdio.h\u0026gt; int age; void whats_my_age() { age = 30; } int main() { whats_my_age(); printf(\u0026#34;%d\\n\u0026#34;, age); }  Compile the above program. It correctly prints the age. What is the biggest disadvantage of having a global variable?\n  Can you figure out where global variables resize in the memory of a program? Take another good look at the schematic in 8.1: program memory. It lives outside of the stack, and also outside of the heap, in a separate block called \u0026ldquo;data\u0026rdquo;. Note that we are not employing any kind of pointer system. Thus, we are not calling upon the heap to transfer data from one method to the other.\nWhen should I use global variables? The answer is never, if possible. In practice, in an iterative programming language such as C, that is very difficult to achieve. In essence, function and struct declarations are all part of the global scope. Methods can be accessed from any other method (provided you used forward declarations), because the method name itself is declared globally, instead of locally.\nConsider the following example in Javascript:\nfunction someMethod() { int age = 30; function someMethodInAMethod() { age = 5; // will work: the variable is part of this (closed) scope  } someMethodInAMethod(); } function otherMethod() { age = 5; // will not work: age is accessible in someMethod only  otherMethod(); // will work and cause an infinite loop  someMethodInAMethod(); // will not work: same reason as age } Calling functions within functions is only possible in C in the form of function pointers leveraging the power of void*, but there is no change in the functionality of the scopes. In C, everything in top level is global scope. Top level is declared as the lowermost level, the body of your source file where all delcarations are put. main() is globally scoped at top level. In the above JS example, so is someMethod, but not someMethodInAMethod: that method is not part of the global scope.\nIn practice, most C programs are constructed as a sequence of method calls, all residing in the global scope. You should try to avoid using global variables as any method can change the value of that variable, causing all kinds of wreckage. If you need to pass something, use return. If you need to pass multiple things, use a struct, or refactor it!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/paging/",
	"title": "9.3: Paging",
	"tags": [],
	"description": "",
	"content": "In the previous part, we have discussed both the benefits and problems with segmentation. One of the main issues was the fact that the segments could be of different sizes, making it difficult to find memory gaps of an appropriate size and adding overhead when doing so.\nAs such, the actual method used most commonly in practice is segmentation with all the segments being of the same size. This means that not only the segments, but also the gaps they leave, are always the same, predictable size. As such, any empty gap is immediately a good candidate for when a new segment is created, without having to actively look for a gap that is large enough (but not too large as to prevent external segmentation).\nIn practice, this approach is called paging, as each same-sized segment is called a page. In other sources, you might also find the term \u0026ldquo;frame\u0026rdquo;, as each logical page is mapped to a physical memory block of the same size, called a frame.\n  The pieces of memory with (logical) segmentation (left) and paging (right). Source: Wikipedia     Each page has an equally sized frame in physical memory   Address binding Paging of course retains most of the benefits of segmentation: it\u0026rsquo;s easy to perform address binding/memory mapping at execution time, making it highly flexible.\nThe way that logical addresses are transformed into physical addresses are also similar: the address' first part indicates the page number (p) (similar to the segment number from before) and the second pard is the page offset (d), that again indicates where exactly the address is in a given page. Instead of a segment table, we now have a page table that keeps track of where the pages actually start in memory.\nPaged addresses However, this is where things start to diverge between the two approaches, as the page table no longer needs to store the size of each page (the \u0026ldquo;limit\u0026rdquo;), as it\u0026rsquo;s always the same. However, the chosen page size still (implicitly) impacts how exactly the (p) and (d) parts of the single address are assigned. This is because the (p) part needs enough bits to identify each page individually, and the bits needed depends on the page size and the total amount of memory.\nPut differently, if the size of the logical address space is defined as 2m, and a page size as 2n bytes, then the m-n left-most bits of the logical address define the page number (p), and the n remaining bits form the page offset (d).\n      Paging hardware   \u0026lt;style\u0026gt;  p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; } \nsource: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\n  To make this more concrete, consider an example. In a hypothetical 4-bit system (each address is 4 bits long) the processor can address 24 different addresses. This gives us a maximum of 16 bytes of physical memory (physical addresses from 0x0 to 0xF). Say we choose a page size of 2 bytes. With the definitions as declared above, this comes down to:\n m = 4 n = 1 amount of pages = 16 / 2 = 8 pages (starting at physical addresses 0x0, 0x2, 0x4, 0x6, 0x8, 0xA, 0xC, 0xE) amount of bits needed to store page numbers (p): m - n = 3 (indeed, 23 is 8) amount of bits needed to indicate offset in page (d): n = 1 (indeed, 1 bit gives us two options: 0 and 1, enough to differentiate between the 2 bytes in each page) So, the first 3 bits of a logical address indicate (p), the last 1 (d)  Now let\u0026rsquo;s imagine that the (first half of the) page table would look like this: [5, 6, 1, 3]. This means that the page with nr/index 0 (first in the table) maps to physical \u0026ldquo;frame\u0026rdquo; 5. Frames are the same size as pages, so the page\u0026rsquo;s physical base address if frame 5 is: 5 x page size = 5 x 2 = 10 = 0xA.\n  The page table maps page numbers to frame numbers, which in turn represent base-addresses of physical memory blocks.   Now let\u0026rsquo;s convert some logical addresses. The simplest one is logical address 0, which we can write with 4 bits as 0b0000. The first 3 bits (0b000) are the page table index (p), which is 0. As said above, this maps to frame 5, and the page\u0026rsquo;s base address 0xA. Now we need to add the offset (d), the last bit, which is 0, so the phyiscal byte we address is simply 0xA (0b1010).\nSimilarly, if we were to look up logical address 1, we get 0b0001. We see this only changes the offset (d) within frame 5, so we get phyiscal address 0xB (0b1011).\nA bit more complex, consider logical address 5 (0b0101). The first 3 bits give 0b010 = page number 2. Page number/index 2 maps to physical frame 1 in the page table, which is physical address 0x2 (frame nr x page size = 1 x 2). The final bit indicates the offset, which is 1. So we need to do 0x2 (base) + 0x1 (offset) = physical address 0x3. We can see that logical address 5, which is higher than logical address 1, is stored \u0026ldquo;lower\u0026rdquo; in physical memory!\nGiven the hypothetical system as described above, what would be the corresponding physical addresses for the logical addresses 6, 3 and 15?  \nAnswer:  Logical address 6 = 0b0110 (page 3, offset 0) maps to frame 3 at physical address 0x6 [= (3 Ã 2) + 0]. By coincidence, the logical address is the same as the physical address!  Logical address 3 = 0b0011 (page 1, offset 1) maps to frame 6 at physical address 0xD [= (6 Ã 2) + 1]  Logical address 15 = 0b1111 (page 7, offset 1) doesn't map to a physical address, because page 7 is not yet mapped! We'll later see that this is a variation of what is called a \"page fault\". Can you already think of how/why this might happen and how to solve this problem?    Page table size However, choosing this singular page size in an optimal way is not simple. As we will discuss below, we cannot simply choose too large a page size (say 10MB per page), since that might lead to leftover memory (called internal fragmentation). However, we also cannot choose a very small page size (say 1KB), since then we would need to keep a large amount of page numbers in our page table!\nTo understand this second aspect a bit better, let\u0026rsquo;s assume a single segment of 5MB. If this is stored in pages of 1KB each, we would need 5000 indidivual pages to represent this segment. While the segment table can just contain a single entry (segment number 0 starting at address 0x1111, with size of 5MB), the page table needs to contain 5000 entries (page number 0 at 0x1111, page number 1 at 0x1511, page number 2 at 0x1911, etc.). While looking up the page addressess will be fast in the page table, it will take up a lot of memory!\nLookup the default Linux page size (in bytes) by using the getconf PAGESIZE (or PAGE_SIZE) command\n  Let\u0026rsquo;s calculate how much memory we would need in a practical scenario: take a 32-bit processor that can address 232 different locations. Let\u0026rsquo;s also assume a a page size of 4kB (=212, = 4096 bytes).\nIn this more realistich scenario, calculate how many bits we need for the page number (p) and offset (d) in our logical addresses. \nAnswer:  m = 32, n = 12. As such, we need 32-12 = 20 bits for the page number, and 12 for the offset.      The amount of pages is then: 232 / 212 = 232 - 12 = 220 entries (1048576 individual pages)\n  Since the page table maps page numbers to physical addresses, and physical addresses are 32-bits on this system, each page entry needs to be 32 bits in size (= 4 bytes, = 22).\n Note that in practice, this is a bit more complicated, as we\u0026rsquo;ve seen before that the page table stores frame numbers, not full 32-bit addresses. However, in practice, each page also stores a few additional status bits (see below), and the page table entries are rounded up to a power of 2, so we still get 4 bytes for each page entry.    As such, the total page table size = entries * size per entry = 220 * 22 = 222 bytes = 212 kB = 22 MB = 4 MB\n  Now, 4MB might not seem like a lot. However, consider that the page table is needed for each memory access! As such, you need to store it very close to the CPU executing instructions to keep the lookup latency overhead low. We cannot simply store it in RAM, since that is conceptually quite far from the CPU (remember that the speed decreases considerably from registers -\u0026gt; cache memory -\u0026gt; RAM -\u0026gt; disk). Typical register memory is at the order of a few (hundreds of)bytes, while cache memory is typically less than 8MB (and it needs to store much more than just the page table!). Also consider that for modern 64-bit setups, things are much, much worse. This requires 254 = 18.014.398.509.481.984 bytes, which can\u0026rsquo;t even fit on modern hard drives!\nThere are several options for dealing with this problem. Firstly, we can choose larger page sizes (say 2MB or even 1GB) to reduce the amount of entries needed. However, as we\u0026rsquo;ll see below, this is a trade-off with internal fragmentation. Secondly, we can use a multi-layered/hierarchical page-table, so that we don\u0026rsquo;t need to keep the full page table in memory all the time (as it\u0026rsquo;s unlikely all those 64-bit addresses are actually used!). However, this increases lookup time (as each layer needs to be traversed). Thirdly, we can have each process track its own page table individually and swap the page tables in/out when a process is scheduled/context switched. This means we only need memory for the pages that are actually used, but this increases context switch overhead.\nIn practice, all of these techniques are combined in clever ways on modern OSes to get the best of each. Full details of how this works would take us too far, but let\u0026rsquo;s consider one common approach.\nHierarchical page tables    A two-level page-table scheme   source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\n One way to overcome large page tables is to use a two-level paging algorithm. This technique uses paging for page tables. The page number in the example above is 20 bits. Using the same technique again, the page number gets split into two 10-bit addresses.\n   With this setting, p1 is the index into the outer page. Similarly, p2 is the index into the inner page. When the physical address is to be searched from a logical address, first the outer table needs to be examined using p1. With the inner table index found, the base address can be searched for in the inner table using p2. Finally the page offset is added to the base address to end up with the mapped physical address.\nThe translation from logical to physical address happens from the outer table, inward. Therefore this scheme is known as a forward-mapped page table. The are also \"inverted\" and \"hashed\" approaches that do things differently.\n  In practice, this approach can be repeated at will, adding more and more lookup tables in between. For example, for a typical 64-bit system, a hierarchy of 4 page tables is used with page sizes of 4KB, where each page number pN is stored in 9 bits. This approach is chosen so that each page table nicely subdivides into 4KB pages itself, which allows them to be \u0026ldquo;swapped\u0026rdquo; as well (see below).\nThis approach can then be combined with per-process page tables and make them hierarchical as well (which is useful for processes that use a large amount of memory, such as for example databases).\nThese approaches can then also be implemented in the hardware MMU, which can for example keep a few per-process page tables in its local memory at a time, replacing them with others as new process are scheduled.\nModern hardware typically supports a number of different page sizes (typically 4KB, 2MB and 1GB) that allow some level of tweaking if the programmer knows the expected memory usage up front.\nInternal fragmentation As mentioned several times above, next to the large page table size, another problem with paging is internal fragmentation. Before, we\u0026rsquo;ve discussed external fragmentation (with segmentation), which means that memory -outside- of the processes gets fragmented. With paging however, the memory -assigned to the processes- can get fragmented as well. Consider for example the case where we have pages of 4KB each, but we now need to allocate space for an array of 5KB. We can\u0026rsquo;t allocate memory smaller than a page, so we need to reserve at least 2 pages of 4KB, wasting 3KB. In theory, this 3KB can be filled with other data of the same process, but that requires the memory allocator to keep track of such things (which doesn\u0026rsquo;t always happen for various reasons).\n  Each process can have some leftover memory, leading to internal fragmentation.   It should be obvious that, the larger the page size, the more risk of internal fragmentation occurs. Again: this is why we don\u0026rsquo;t just use 1GB pages for each process.\nAdded benefits of paging As we\u0026rsquo;ve seen above, paging is a very flexible setup, but it requires quite a bit of work to tune/optimize for practical use. Once we\u0026rsquo;ve done that however, we get plenty of additional benefits from this approach. Let\u0026rsquo;s look at a few.\n(Side note: we will now discuss some of the extra \u0026ldquo;status bits\u0026rdquo; that are kept for each page table entry, which as we\u0026rsquo;ve seen above (search \u0026ldquo;status bits\u0026rdquo;), force the page table to store more bits than required by the frame number alone, causing the page table size to increase).\nAllocating more memory than is physically available As you can imagine, no machine today has enough physical (RAM) memory to allow the use of the entire 64-bit address space at once. In reality, you typically \u0026ldquo;only\u0026rdquo; have 8 - 32 GB of RAM in your home PC, while large servers have 512 GB to 2 TB of RAM nowadays.\nHowever, you can also easily imagine processes using up more than the available amount of memory, especially on your PC (have you ever run Google Chrome? Photoshop?). In that case, you wouldn\u0026rsquo;t be able to start new processes any more, until you\u0026rsquo;ve manually closed some others, even if you have a lot of processes that are idle/running in the background.\nTo prevent this from happening, modern OSes will use the harddisk storage as a semi-transparent extension to the RAM memory. If the main memory is full, pages that are not currently being used, can be stored on the hard disk for a while, making room for new pages in the RAM. If those stored pages are later again needed, the OS can again move other in-memory pages to disk, to make room to restore the previously \u0026ldquo;hibernated\u0026rdquo; pages. This process of storing pages to disk and restoring them to main memory at a later time is called swapping.\n  Pages can be swapped in/out to the harddisk to reduce main memory occupancy    p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nIn practice, this all happens transparently for the programmer. For each page in the per-process page table, the OS tracks whether this page is currently in the main memory or not. This is done using a so-called valid/invalid bit. If the program attempts to access a page with the invalid bit set, the OS generates a page fault. Other than with a segmentation fault, the OS will not actually regard this as an error. Instead, a new page location (free gap in main memory) is found and the contents of the page are again read from disk into that space. If there are no free pages in main memory, old pages are first swapped out to disk for both.\nSomething very similar happens if we try to access a page that has never actually been allocated before (say with a new malloc()). In this case, the OS will look for a new page in main memory and allow the program to start using it (this is what we saw above when trying to use logical address 15, which was not mapped in the page table yet). It is even possible to take this one step further, and to not actually allocate memory for a given page until it is actually used/written to. For example, if you allocate 2GB of memory with malloc() (which you can certainly do), the OS won\u0026rsquo;t actually start reserving that much space in the memory. It will just add the necessary placeholders in the page table until you actually start writing to this 2GB of memory. Something similar happens if you start allocating lots of smaller blocks: you\u0026rsquo;re only storing the pointers to those blocks; the actual block memory is only converted to pages when data is written to them (this is why in some exercises in the previous chapter you can use a \u0026ldquo;logical\u0026rdquo; amount of memory of many hundreds of gigabytes before you actually run out of \u0026ldquo;real\u0026rdquo; memory). This approach can also be used to reduce the amount of code each process needs to load. In most programs, large parts of the code are never actually executed in normal executions of the program. As such, the OS can decide not to load pages in the text/code segment of the process until it actually needs to execute them, an approach called load-on-use.\nAs such, we can see that this allows us to, in theory, extend the size of the physical RAM to encompass our entire hard disk as well (e.g., going from 16GB of RAM to 2TB \u0026ldquo;extended\u0026rdquo; RAM). In practice, modern OSes reserve a fixed amount of hard disk space for swapping purposes (linux typically has a separate swap partition, while windows typically has a large file of 1x or 2x the RAM size it uses for swap space). It\u0026rsquo;s only when this extra space also runs out, that the OS will have to start shutting down processes to make room for more. This is something you still see today in mobile OSes such as Android and iOS, which occasionally will auto-close (long-idle or heavy) apps to make room for new ones.\nHowever, there are of course also downsides to this approach. Hard disk memory (even if you\u0026rsquo;re using an SSD drive) is still much slower than using RAM directly. As such, every time a page needs to be swapped, this adds additional overhead, as memory needs to be copied to the RAM first before use. The OS tries to hide this overhead by scheduling other instructions while it\u0026rsquo;s waiting (up to even scheduling another process while waiting), and especially by being clever about which pages it keeps in memory and which it switches out. This can again be done with a variety of algorithms with descriptive names such as Least-Recently-Used (LRU), Not-Frequently-Used (NFU), First-In-First-Out (FIFO), Second Chance, etc. A final approach is by tracking whether a page that was previously stored to disk (and is currently still on disk), has actually been changed in main memory (this is done by using a modified bit (or \u0026ldquo;dirty\u0026rdquo; bit)). If the modified bit is not set for a page in main memory, we can simply overwrite it without first serializing it to disk, as we know the version already-on-disk is the same as the one in main memory, and we don\u0026rsquo;t loose any state by overwriting it. In general, the OS prevents the use of swapping as much as possible.\nOptimized data sharing Another common optimization is to allow multiple processes and threads to share individual pages amongst themselves. For example, we have seen that when fork()\u0026lsquo;ing processes, they each get a new copy of the PCB and all the program segments. This is conceptually true, but in practice this would of course be quite wasteful, as the child will probably never change the text/data/bss segments and will probably keep re-using much of the same heap and stack data as the parent (at least in the beginning).\nInstead, when fork()\u0026lsquo;ing a new process or starting a new thread, the new task typically still just points to the old pages. This only changes when the new task tries to write to one of the shared pages: at this point, the copy-on-write approach is used, and the to-be-changed page is copied for the new task before it is changed. This is conceptually similar to the allocate-on-write approach discussed in the previous subsection.\nSomething similar happens for the previously mentioned (dynamically) loaded libraries, where code can be shared between many processes. In this case, the libraries\u0026rsquo; pages can be marked with a special read-only bit, to make sure that processes don\u0026rsquo;t overwrite the library internals and mess up other processes that way.\nMemory-Mapped I/O As discussed before, no modern system actually has enough RAM/harddiks memory to back the full 64-bit address space, meaning that we always have quite a few addresses leftover, even under full load. Additionally, the concept of swapping has shown that we can view memory addresses as proxies to access -other- hardware (in that case, the harddisk) in an indirect way.\nIf we take this concept further, we can imagine also addressing other hardware in the same way, by exposing them as logical memory addresses that are mapped internally by the OS to something else than the RAM. This is generally called memory-mapped I/O and it is a very common technique.\nFor example, in Chapter 2, we\u0026rsquo;ve mentioned that Arduino CPU registers are accessible from C code in this way: they are assigned a logical address, which the OS maps not to the RAM/harddisk, but to the CPU registers for setting/reading them. Similarly, modern GPUs typically also have a large amount of \u0026ldquo;VRAM\u0026rdquo; (Video RAM), specifically for storing (graphical) data. Instead of addressing this VRAM in a completely separate way, the OS instead maps it to its own logical address space (pretending as if it\u0026rsquo;s \u0026ldquo;normal RAM\u0026rdquo;) which can be read/written using standard methods. Other examples include serial peripheral devices (for example mouse/keyboard/older modems/many sensor systems/\u0026hellip;) that can be accessed by pretending their output can just be read from a memory region. Finally of course, we can also do this for \u0026ldquo;normal\u0026rdquo; files on the harddisk (and not just swapped pages).\nWhile powerful, this technique can sometimes be tricky to get right, as in some cases the memory access is not immediately propagated directly to the other hardware, but instead the RAM is still used as intermediate storage. For example, have you ever wondered why you should eject a USB stick ? It might happen that you have written data to the USB stick, by writing to its memory-mapped addresses, but the data has not reached its destination yet. This is because the data is first written to RAM, and then transferred over time (by the OS) to the actual hardware. In these cases, the earlier mentioned dirty bit again comes into play. This is not just something that happens with memory-mapped I/O though: any intermediate memory layer has these issues, and this frequently shows up with CPU cache memory as well.\nSecurity Finally, let\u0026rsquo;s discuss security a bit more. As we\u0026rsquo;ve seen at the start of this Chapter, one of the main reasons for keeping track of chunks/segments/pages is to provide memory protection: to prevent process A from accessing process B\u0026rsquo;s memory. This is for example needed to prevent A from stealing passwords or sensitive data stored in B\u0026rsquo;s memory during execution.\nHowever, in the first lab, we\u0026rsquo;ve also seen that this protection is rarely enough, and that there are plenty of ways for hackers to still mess with your system, even if it\u0026rsquo;s just within a single process. Over time, OSes and compilers have added many advanced techniques to help deal with these issues.\nOne such technique is called Address Space Layout Randomization (ASLR). A common attack vector is for the hacker to \u0026ldquo;escape\u0026rdquo; the stack/heap (e.g., through an array overflow, as we\u0026rsquo;ve seen). With that, they then attempt to overwrite code/data for a known vulnerable function in the program that allows for example execution of new code. This requires the hacker to reliably know where exactly this vulnerable function is in the logical address space. While pages can be randomly distributed inside the physical memory, without ASLR the logical address space would always look the same (starts at 0x00, text ends at 0x?? every time, since it\u0026rsquo;s always the same length, so data segment starts at a predictable location as well etc.). As such, ASLR randomizes the logical base address of the program, as well as the logical starting locations of the other segments as well, to make it more difficult (though far from impossible) for hackers to guess where certain code/data is located between different runs of the same program.\nEven with all this, it\u0026rsquo;s still possible for attackers to steal data, albeit via very complex methods. For example, the Spectre and Meltdown make use of several advanced features of modern CPUs, such as out-of-order execution, speculative execution, caching and, -indeed-, also virtual addressing and paging. The details are far too complex to discuss here (that\u0026rsquo;s more 2nd master level), but one of the key parts in the attack is that the OS and CPU actually does not check the segment/page boundaries immediately when an access occurs. To speed things up, CPUs will allow out-of-bounds accesses to occur and they will load privileged data into cache memory. It is only when this memory is then being used/read that the check occurs and it is prevented. However, the fact that the data is loaded into (cache) memory allows the hacker to use other methods (\u0026ldquo;side-channel attacks\u0026rdquo;/\u0026ldquo;timing attacks\u0026rdquo;) to decipher the contents of the cache, thus stealing the protected memory, across process boundaries and even from inside the kernel! Spectre and Meltdown have been very impactful, often requiring changes at the OS and even hardware level, lowering performance by up to 20% in some cases!\n  Meltdown and Spectre - Two attacks that got access to other process\u0026#39;s memory.   Finally, even if hackers can just stay within a single process, things can still go terribly wrong. For example, in the cloudbleed attack, the large content distribution network Cloudflare suffered massive data leaks. This is because they used a single Web server process to serve requests/responses from many different clients. Using a simple buffer overflow, this allowed the attacker to read not just their intended response, but also the responses to request from other clients (which were located on the same heap), which often contained passwords, banking information, etc. This is because, within a single process as we\u0026rsquo;ve seen, memory checks are typically absent. Making matters much worse, this could be triggered by requesting a specially crafted HTML page, causing some search engines like Google to actually cache the leaked passwords in their search results! As such, fixing the bug at the Cloudflare servers was not enough: many search engine caches also had to be purged manually!\nAs such, it should be clear that memory access protection is a complex topic, going far beyond what we\u0026rsquo;ve discussed, and still an active area of research and development.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/devdrivers/",
	"title": "X.3: Device Drivers",
	"tags": [],
	"description": "",
	"content": "General The kernel of an operating system has a lot of different tasks. Some of them we visited in earlier chapters. One task of the OS is the management of the different hardware devices that are typically connected to a processor in a laptop, desktop, or server. The OS has to know how to talk to a certain device. Typically an Application Programming Interface (an API) is provided to the user space, so users can interact with the hardware.\nLet\u0026rsquo;s take the timer from chapter 3 as an example. Enabling the interrupt for when Timer1 overflows was done by writing a \u0026lsquo;1\u0026rsquo; to the LSB of address 0x6F. Although this is no rocket science, it is not realistic to assume that the user simply knows this. The manufacturer of the hardware, the timer in this example, is the one that knows and should make this knowledge available to the user. This is typically done through documentation, or through a driver.\n... #define TIMSK1 _SFR_MEM8(0x6F)  ... TIMSK1 |= 0x1; ...  The user could find out what is to be done to enable the interrupt from the documentation. When that is figured out, the user has to program/use the functionality correctly.\n  This needs no additional software  This requires a \"skilled\" user   ... #include \u0026#34;timer.h\u0026#34; ... enable_interrupt_timer1_overflow(); ...  The user could find out what is to be done and execute the dedicated function.\n  This requires a \"normal\" user  This needs additional software    The argument of needing additional software, e.g. the driver, is no longer a concern. The additional kilobyte of \u0026ldquo;firmware\u0026rdquo; can easily be stored. An additional benefit from using a driver is that it provides a more flexible solution. For example, version 2.0 from our timer has this bit moved to another position in the register. The \u0026ldquo;skilled\u0026rdquo; user that uses the timer, has to update his/her source code. With the solution of a driver, the vendor simply ships an updated driver with the hardware and no modifications needs to be made by the user. Scalability is improved!\nWhere do they live ? The driver is a piece of software that allows an operating system to interact with a certain hardware component. In the first lecture on OSes, we talked about the user-space and kernel-space. So, where do device drivers live ? Take a guess \u0026hellip; I\u0026rsquo;m sure you\u0026rsquo;ll be correct.\nThe correct answer is: both ð The driver can live in user-space and in kernel-space. Depending on the space, there is a benefit and drawback. Drivers in user-space will not crash the entire system is an error occurs in the driver itself. This provides improved stability. Drivers in kernel-space will run with much higher priority than user-space processes ever can. This provides improved performance.\nCharacter device drivers Character devices are the simplest types of devices to communicate with, on a Linux systems. The textbook example for this is a serial port. The serial port is the predecessor of the Universal Serial Bus (USB). Students might have been in contact with the serial port when using the RS232 protocol. This is how the UART communicates. Today, it is hard to find the original connector on a modern laptop/desktop, as they are all replaced with a USB-alternative.\n  The original serial port cable      Different types of USB connectors     As was stated before in the course, everything is a file on a Linux operating system. The serial port, which is driven by a character device driver, is represented by a file in /dev\n  The representation of a serial port on Linux system   The first character of permission modes is a c. We have seen that a d represents a directory. The c here shows that the device is a character device. A character device is a device that works with one character as a basic unit. The user can read one character at a time, or write one character at a time. As state above, the serial port is an excellent example for this.\nBlock device drivers Devices that support filesystems are referred to as block devices. The drivers, not surprisingly: block device drivers. The typical examples for these are hard drives, solid-state drives, and USB flash memories.\nSimilar to the letter c with the character devices in the permissions, the block devices show the letter b.\n  The representation of a solid-state drive on Linux system   Writing your own When you develop new hardware, or got in a situation where no drivers are around for existing hardware, you might decide it is time to write your own device driver.\nThis type of programming is very different from general application programming as you know it. If you make mistakes with general programming, you get a fault and that\u0026rsquo;s that. Maybe you need to reset the terminal, but that\u0026rsquo;s (roughly speaking) the worst that can happen.\nWhen you write bugs in your device drivers, it is a different story. Memory leaks in the kernel might crash your system periodically. Incorrectly handled exceptions lead to system failures and maybe even hardware failures.\nKernel modules Earlier we have seen that the kernel is not one impenetrable binary (monolithic). The kernel can be expanded with kernel modules. This is a small piece of code that can be added to the kernel, while the kernel is running. A number of kernel modules are present in the Virtual Machine image. You can list the kernel modules with the command: lsmod.\n  Result of lsmod in the Virtual Machine   Modules can be loaded and unloaded with insmod and rmmod, respectively.\nDriver While developing your custom driver, it is recommended you start of with a kernel module. After compiling, the kernel module can be inserted into the kernel (and hopefully, don\u0026rsquo;t crash it).\nBecause the driver is a kernel module it also has no main() function. \u0026ldquo;It\u0026rsquo;s just\u0026rdquo; a collection of functions that are attached to hooks of the kernel.\nint register_blkdev(unsigned int major, const char *name); For example: the hook above is a standard function that kernel provides to allow a driver for a block device to register itself with the kernel.\nDisplaying messages on the standard output device is also not possible. Beginning kernel hackers use the printk function:\nprintk(KERN_DEBUG \u0026#34;Debug message shown!\\n\u0026#34;); These printed message do not end up on the device\u0026rsquo;s monitor, but in the kernel ring buffer. A ring buffer is a buffer with a fixed amount of size. When overflowing, the oldest messages get deleted.\nThe ring buffer can be consulted using the dmesg command. This prints the current content of the ring buffer to the screen.\n  Example of dmesg output      Example of dmesg, after inserting a USB drive     This is also a good place to visit when you are trying to solve system problems !!!!\n "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/structs-labs/",
	"title": "2.4: Structs",
	"tags": [],
	"description": "",
	"content": "We are (still\u0026hellip;) at War! We\u0026rsquo;re still at war! The orcs are attacking and are looking very hungry! Look at them!\n  Orcs attacking! source: G.I.   2. Modelling the orcs Let us try to model these filthy beasts. Ask the user for a number. That number is the amount of orcs in the army. Create as many struct instances with random property values and print these on the screen. An orc has the following properties (both simple numbers, between 1 and 10, use rand() from stdlib):\n attack life   INPUT: 3 OUTPUT: orc 1: attack 3, life 5. orc 2: attack 5, life 6. orc 3: attack 1, life 1.  Tips:\n Do not forget to generate a new seed for the random value, see the rand() docs. Creating a separate function generate_orcs() will keep your main() function short and clean. The function will return a list of orcs, the \u0026ldquo;army\u0026rdquo;, so to speak. Remember that returning an array is of type Orc*.  The generate method will look like this:\nOrc* generate_orcs(int amount) { Orc* army = malloc(sizeof(Orc) * amount); // add stuff to army  return army; } Details on how the malloc() function works will be explained later.\n3. Orcs eating each other?? Vowels did not seem to fully satisfy them, now they are turning on each other!? All the better for us. Expand the program such that the first orc fights the next one. (life minus attack). Create a function Orc fight(Orc attacker, Orc defender). Is the defender still alive after the attack? Then he is victorious (and will be returned). Print the last man stending. Input stays the same.\n INPUT: 3 OUTPUT: orc 1: attack 3, life 5. orc 2: attack 5, life 6. orc 3: attack 1, life 1. orc 1 VS 2: 2 wins (6 - 3 = 3 life left) orc 2 VS 3: 2 wins (1 - 5 = dead) orc 2 is victorious!  Tips:\n You will need to loop through all orcs and take two elements out of the array to pit them against each other. Reassign Orc winner = army[0] with the result of the fight() function, within the loop. If both orcs survive, the first one wins.  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch3-introcpu/2_lab_stack/",
	"title": "3.4: The Stack",
	"tags": [],
	"description": "",
	"content": "Exercise 1 Write the (psuedo)Assembly code (using PUSH/POP) for the following C code (note: you can re-use a lot of the example code in the chapter to start, but you\u0026rsquo;ll need to add/change a lot too!).\nSince we\u0026rsquo;re now calling additional functions, also make sure you store the return addresses on the stack correctly (using 0x07 as temporary return address storage).\nYou can use 0x60, 0x61 and 0x62 as addresses for the input parameters (M, N, R, S, and T), and 0x80 to store any return values. Alternatively/additionally, you can also pass around values using the stack. Both approaches will have pros and cons and situations where you have to work around them.\nTo keep things simple, you don\u0026rsquo;t have to store local variables (like C, D, Q, etc.) on the stack and can pretend they just reside at an appropriate address, like in the examples in the chapter.\nvoid fX() { int A = 1; int B = 2; int C = fY(A, B); int D = C + A; } int fY(int M, int N) { int Q = M + N; int R = fZ(Q, M, N); R = R + M; return R; } int fZ(int R, int S, int T) { int U = R * S * T; return U; } Exercise 2 A recursive function is a function that calls itself. This might seem weird, but it is actually quite useful in many situations, as it makes code easier to understand. You will learn more about practical use cases for recursive functions in later classes.\nFor now, consider the following simple recursive function that counts to 10 and the function f1 that calls it:\nvoid f1() { int Y = countToTenRecursive(0); int Z = Y + 10; // should be 20! } void countToTenRecursive(int currentNumber) { if ( currentNumber \u0026gt;= 10 ) { return currentNumber; } int nextNumber = currentNumber + 1; return countToTenRecursive( nextNumber ); } Your goal is to write Assembly (using the stack of course!) that properly implements this function. You can assume that a function retrieves its first parameter (in this case currentNumber) from the address 0x60 and stores its return parameter in 0x80.\nBefore you start, consider what exactly it means to return a function call as we do here at the end of countToTenRecursive\nTake care to correctly store the return addresses (and potentially also the return values and function parameters) on the stack so that, even 10 calls deep, things still work as expected!\nExercise 3 There are many types of Assembly. One of the most recent ones is called \u0026ldquo;WebAssembly\u0026rdquo;, a somewhat high-level Assembly language (if that\u0026rsquo;s not a \u0026ldquo;contradictio in terminis\u0026rdquo;, I don\u0026rsquo;t know what is) that can be used as an alternative for JavaScript.\nWebAssembly is special in that it works purely by means of a \u0026ldquo;stack machine\u0026rdquo;. Put differently: all of its instructions/operations work by pop\u0026rsquo;ing and push\u0026rsquo;ing things from/to the stack. If it\u0026rsquo;s not on the stack, you can\u0026rsquo;t perform calculations with it.\nFor example: while C = 10 + 5 would look like this in our pseudo-assembly: ADDd 10 5 C, in (conceptual) WebAssembly it would instead be this:\nPUSHi 10 // put value 10 on the stack\rPUSHi 5 // put value 5 on the stack\rADDd // pops the top two values from the stack, add them together, then push the result back onto the stack\rPOP C // store the top value of the stack in address C\rUsing this approach, write (conceptual) WebAssembly code for the following functionality:\n Given a value on the stack (put there by some previous operation), subtract 20 from it Given a value on the stack (put there by some previous operation), square it (e.g., if the value is 5, we want to do 5*5) void f1(int A) { B = 25 + A; C = B + 10; return C; }  Answer the following questions:\n Do we still need 3 variants of our mathematical operations (for example ADD, ADDd and ADDi) or can we do with less? Why? \u0026ldquo;Using the stack in this way is better than the \u0026ldquo;normal\u0026rdquo;/pseudo Assembly we\u0026rsquo;ve seen because it reduces the amount of temporary variables we need.\u0026rdquo; Is this statement true or false? Why?  Exercise 4 In most programming languages, a function can only have a single return value. If you want to return multiple values, you have to wrap them in an object/struct or use pointer input parameters (see later).\nHowever, there is no real technical reason for this\u0026hellip; a stack can perfectly be used to support multiple return values for a single function.\nCan you write a simple Assembly (pseudo)code sample that shows this concept?\nWhat this might look like in C if C allowed multiple return values:\nint A, B = f1(); int C = A + B; int,int f1(){ M = 20; N = 30; return M, N; } "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-pointers/practical-use/",
	"title": "4.2: Practical Use of Pointers",
	"tags": [],
	"description": "",
	"content": "1. Changing values around Because in C everything passed by-value, we can manipulate the values of variables in a function that has been declared outside with pointers. In Java you can also change the value of member variables in objects, but not primitives! How do you switch two numbers without giving anything back?\n#include \u0026lt;stdio.h\u0026gt;void swap(int *px, int *py) { int temp; temp = *px; *px = *py; *py = temp; } int main() { int x = 10, y = 20; swap(\u0026amp;x, \u0026amp;y); printf(\u0026#34;(%d, %d)\\n\u0026#34;, x, y); // print (20, 10) } mermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD; x[x10] y[y20] px{px} py{py} temp[temp] px --|\u0026x, address-of stack-var x| x py --|\u0026y, address-of stack-var y| y temp -.-|*px, follow pointer for the value| px  Something like that is unthinkable in Java - we need extra tricks for that, such as passing objects. Of course this implementation is also disadvantageous: is it clear to the caller that variables are being changed? Not really. High-performance algorithm implementations benefit from these possibilities. Clear domain-driven applications are not: a higher language is used for that.\n2. Not chaining values around: const To prevent further confusion, it is possible to mark pointers with the const keyword, meaning the value should not be changed. The pointer can still point to another value! As such, this is by no means a \u0026ldquo;constant\u0026rdquo;, like in many other traditional programming languages. Take the above example, and change swap\u0026rsquo;s signature to void swap(const int *px, const int *py). While compiling the code, the following errors are generated:\n test.c:5:9: error: read-only variable is not assignable *px = *py; ~~~ ^ test.c:6:9: error: read-only variable is not assignable *py = temp; ~~~ ^ 2 errors generated.  With the const keyword, we prohibit programmers from using *ptr = ... - that is, assigning another value as a dereferenced pointer. ptr = \u0026amp;temp is still possible, however. If you do not want pointers to change addresses, use const int* const px. That\u0026rsquo;s right, two times const - this is not a mistake. This reads, from right to left, as:\n px is a constant pointer to an int constant  Introducing the second const gives the following error when attempting to change the pointer itself:\n test.c:5:8: error: read-only variable is not assignable px = \u0026temp; ~~ ^ 1 error generated.  In practice, try to use as many constant variables as possible, if you want to make sure the passed values stay the same.\nPointers in combination with const can be very tricky! For instance, a const int* ptr cannot change the value of the thing it\u0026rsquo;s pointing to (*ptr = 6), but still can change the value of the address it\u0026rsquo;s pointing to (ptr = \u0026amp;otherval). For a int* const ptr\u0026mdash;where the keyword is placed in-between\u0026mdash;this is the other way around: the pointer is constant, but the value isn\u0026rsquo;t.\n 3. Arithmetics with pointers Pointers and arrays go hand-in-hand in C. Pointers can be moved around by adding and subtracting. On pointers you can also perform operations such as ++ and -- that move the pointer in the memory one place to the left or right. With char * text = \u0026quot;sup\u0026quot; the pointer refers to the first character:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A{*text} A--|begin van array|C['s'] A-.-D['u'] A-.-E['p'] A-.-F['\\0']  Creating an own version of printf(\u0026quot;%s\u0026quot;, tekst) could be implemented by looping through the pointer until nothing is left to print, like this:\nvoid print_text(char *txt) { while(*txt != \u0026#39;\\0\u0026#39;) { printf(\u0026#34;%c\u0026#34;, *txt); txt++; } } Notice txt++. We simply point to the next possible value in the memory space, which hopefully is still a character. If it is not, and it came from a string, it will be ended with \\0. Adding some value beyond the limit will result in calling upon unintended memory values, resulting in possible glitches. But C will not crash, it is very robust. You should pay extra attention while fiddling about with pointers! For instance:\nIn C, a[i] exactly the same as *(a + i)!\n #include \u0026lt;stdio.h\u0026gt;int main() { char txt[4] = \u0026#34;hey\u0026#34;; char* ptr = txt; char otherstuff[10] = \u0026#34;other\u0026#34;; for(int i = 0; i \u0026lt; 5; i++) { printf(\u0026#34;%c\u0026#34;, *ptr); ptr++; } } Depending on your compiler, the above code will print \u0026ldquo;hey ot\u0026rdquo;, meaning your ptr pointer is pointing to the next variable on the local stack after the four characters \u0026ldquo;h\u0026rdquo;, \u0026ldquo;e\u0026rdquo;, \u0026ldquo;y\u0026rdquo;, \u0026ldquo;\\0\u0026rdquo;, from the txt variable, are processed within the for loop. We will go more into detail on this in chapter 5.\nWhat happens when I change txt[4] to txt[3]?\n  Jumping to the next available address space also works with structures instead of a character array:\n#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; typedef struct Doos { int one; int two; } Doos; int main() { Doos doos1 = { 1, 2 }; Doos doos2 = { 3, 4 }; Doos* doosje = malloc(sizeof(Doos) * 2); doosje[0] = doos1; doosje[1] = doos2; for(int i = 0; i \u0026lt; 2; i++) { printf(\u0026#34;doos: one %d two %d\\n\u0026#34;, doosje-\u0026gt;one, doosje-\u0026gt;two); doosje++; } return 0; } The only problem here is that we cannot loop \u0026ldquo;until the end\u0026rdquo; using while(*doosje) { ... }. For that to work, we need linked lists (see lab 2).\nIn C, the new keyword does not exist. Creating instances is done using malloc() instead.\n 4. Linked Lists You will discover in exercise \u0026lsquo;the ancient library\u0026rsquo;, you can link different struct instances together to create a list of items, instead of simply using Arrays or other existing data structures. Internally in the C libraries, pointers are applied to connect elements of a collection. Let us try to do the same.\nThis is how the memory structure of your C code looks like without initializing any single variable:\n   Given the following structure:\nstruct node { char* name; char* value; struct node* next; }; When instantiating a node element using malloc(), we create a new variable on the heap instead of the stack:\n   However, this representation is incomplete! We create a new local variable, a pointer, and this pointer is actually also a variable on the stack. So, this code:\nvoid create_node() { node* newelement = malloc(sizeof(node)); newelement-\u0026gt;name = \u0026#34;something\u0026#34;; newelement-\u0026gt;value = \u0026#34;value\u0026#34;; newelement-\u0026gt;next = NULL; } Reserves some space on the heap, but also creates a local variable named newelement on the stack:\n   The next value is pointing to NULL (\u0026ldquo;nothing\u0026rdquo;), hence the white arrow in the right side of the Figure. Now, what if I want to create a second element, and connect both together? A second local variable reserves a second block on both the stack and the heap:\n   Now, we want to assign the second element to the next property of the first element. That\u0026rsquo;s very simple with the statement newelement-\u0026gt;next = newelement2;. Now, our memory looks like this:\n   Notice the changed arrow in the right side of the Figure. Ok, what if we want to loop over all elements by following the arrows from each element, starting with the first, what happens with our variables in the memory space? A new method creates a new chunk in the stack space, where a new variable will be created:\nvoid print(node* printer) { while(printer != NULL) { printf(\u0026#34;%s \\n\u0026#34;, printer-\u0026gt;value); printer = printer-\u0026gt;next; } } void main() { node* head = create_node(); node* tail = create_node(); head-\u0026gt;next = tail; print(head); }    In the above Figure, printer points to the first value in the heap, which is the same as the variable head. When the while() loop starts doing it\u0026rsquo;s work, the variable will point to the next value, and the next, and the next, until it points to NULL.\nThis will become clear in the exercise when you will implement these concepts yourself.\nWhen changing the value of printer, the value of head stays pointing to the first element of the heap! This is because the variable is a copy on the stack.\n What if we want to change the pointer of head using another variable? Then you will need double pointers, or node**:\nvoid change_ptr(node** ptr_to_ptr, node* new_value) { *ptr_to_ptr = new_value; } "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-pointers/",
	"title": "4: Pointers &amp; Arrays",
	"tags": [],
	"description": "",
	"content": "Chapter 4 Pointers and Arrays Chapter 4 handles the following subjects:\n Arrays in C Pointers in C: arrays don\u0026rsquo;t exist! By-reference, By-value malloc(), free() Function pointers Practical examples of pointers:  Aritmetics Linked Lists Swapping Values    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/threads/",
	"title": "6.4: Threads",
	"tags": [],
	"description": "",
	"content": "In the previous three sections we have discussed that you can run multiple processes at the same time and how this is managed by the OS. This can be useful in two ways: either a) to use multiple programs at the same time (e.g., your Web browser and music player), or b) to make efficient use of multiple hardware CPUs or processing cores.\nWe have however also seen that this is not trivial: new processes need their own memory space and associated state (the PCB) and communication between processes is either flexible but complex (shared memory) or straightforward but limited (message passing).\nLuckily, there is another way to split up processes into smaller subtasks and make more efficient use of multicore hardware without incurring as much overhead. This is accomplished using Threads.\nA thread is an independent unit of execution within a process. Put differently, it\u0026rsquo;s a sequence of code instructions that can be executed independently from and, crucially, in parallel with other threads. As such, each thread can for example be assigned to a single CPU core for parallel processing.\nWhat\u0026rsquo;s in a thread ? Up until now all the processes that were discussed contained just one thread (sometimes called the main thread). Such a process is referred to as a single-threaded process:\n  The single threaded process   Let\u0026rsquo;s examine the image above. On the top of this image the following segments are mentioned: code, heap, and (open) files, alongside the registers, program counter and the stack.\nAs we\u0026rsquo;ve seen, when multiple processes run at the same time, every individual process has separate and isolated instances of all these segments, like shown in the image below:\n  Two single threaded processes running in parallel   However, the name \u0026ldquo;single-threaded\u0026rdquo; of course also implies the existence of multi-threaded processes, which have more than one thread. Each of these threads can execute in parallel and as such also need their own copies of some of the segments: the register contents, current program counter and the full stack are now independent for each thread. You could say that each thread has its own \u0026ldquo;Thread Control Block\u0026rdquo; (in analogy with the PCB), which tracks this metadata. However, the main difference between parallel threads and parallel processes is that threads do still share many things: the copy of the code, the (open) files and the heap memory. This is one reason why threads are more lightweight than processes, as they don\u0026rsquo;t require copying these segments when new threads are created.\n  A single-threaded vs multi-threaded process    div.twocolumns { display: flex; flex-direction: row; justify-content: space-around; } div.column { width: 35%; } div.column ul { margin-top: 0px; }  The multiple threads in a multi-threaded process share:  the code segment the heap memory the list of open files   The multiple threads in a multi-threaded process have unique/independent:  register contents program counters stack memory    Why would we want multiple threads ? There are 4 major benefits to working with multiple threads:\nResponsiveness: When a single program is broken down into multiple threads, the user experience feels more responsive. Dedicated threads can be created to handle user requests and give (visual) feedback, while other threads can for example process data in the background. Resource sharing: Programs that have multiple threads typically want some sort of communication between these threads. This is done more easily between threads than between processes, as threads implicitly share memory via their heap. This is discussed in detail in the next Section. Economy: Context switching becomes cheaper when switching between threads in comparison to switching between processes. This is because less per-thread state needs to be tracked, in comparison to more per-process state. Scalability: Multiple threads can run in parallel on multi-core systems in contrast to a single threaded process.  Although there are many advantages to multi-threaded programming, it requires skilled programming to cash in on these opportunities. Not all code can be easily parallelized and communication between threads is not trivial. We will focus on these aspects in the remainder of this chapter.\n Amdahl\u0026rsquo;s law While threading seems like the ideal, lightweight solution to make your programs run faster, it might be surprising to hear that program performance typically does not scale linearly with the amount of threads and/or amount of CPU cores. This is because typically not all code in a process can be parallelized (and thus run in separate threads): there are typically parts of code that need to aggregate results from the parallel computations, which can only be done in a serial fashion. This can be seen in the following image:\n  Parallel processing can lead to serial phases in a process. Source: J. Wolf et al. - Contribution to the Fluid-Structure Interaction Analysis of Ultra-Lightweight Structures using an Embedded Approach   A theoretical model for assessing the maximal gains from multithreading a program was developed by Gene Amdahl in 1967. His formula identifies potential performance gains from adding additional computing cores to an application that has both serial and parallel components:\n\\( speedup In this formula S stands for the portion of the application that has to be run serially. N stands for the number of cores on which the parallel portion is spread. We can see that to maximize the speedup, we need to keep the divisor of the fraction as small as possible. This is done by keeping S as small as possible, and N as high as possible.\n  Source: Wikipedia   The graph above visualises Amdahl\u0026rsquo;s law. For example, as marked by the red dot, if a program has 80% of its code that can be run in parallel (and so 20% of the code has to be run serially), it can be run at maximum 2.5 times faster, using 4 cores. If only 50% of the code can be run in parallel, the amount of cores matters much less, with a maximum speedup of 2x being possible even with 16 cores.\nHowever, as stated above, it requires a skilled programmer to achieve maximal speed-up even if a large part of the program can be parallelized. If the program needs a lot of data that needs to be communicated between the serial and parallel portions this becomes even harder to achieve, as this can also cause additional slowdowns.\nUser threads vs System threads By now, you might still think that using threads is only useful if you have multiple processors, or, if you do have multiple CPU cores, that you should only use as many threads as you have cores. However, this is somewhat incorrect.\nWhile it is true that you might not get a speedup if the number of threads is larger than the number of cores, that does not mean you cannot run tasks in a seemingly parallel fashion. Take for example the simplest case where you have two threads and just a single CPU core. Like with the processes, this does not mean that thread 1 will run fully before thread 2 can start. Instead, the OS will again schedule the threads, pausing one and (re-)starting the other several times in quick succession.\nAs processor speeds are many times what the typical human would notice, this often gives the illusion of parallel execution. A good example is the \u0026ldquo;Responsiveness\u0026rdquo; benefit mentioned above, achieved by using a separate thread for User input/User interface updates. Even if the program has a thread executing heavy calculations, the UI thread will get enough CPU time to listen to User input.\nAs such, there is often not a direct one-to-one mapping between threads and actual processor cores. Internally, the OS has a concept of Kernel threads that it schedules and divides between the processors. The threads we create, sometimes called User threads can be mapped onto those kernel threads in several different ways (e.g., one-to-one, one-to-many, many-to-many) and this mapping can also change over time (e.g., transferring a thread to a different CPU). The exact details are out of scope for this course, but it\u0026rsquo;s useful to know that you typically won\u0026rsquo;t control directly how your threads run. In a multi-process setup (which is typical for most OSes) you typically can\u0026rsquo;t even guarantee that all the threads for a given process are really all running in parallel, as other threads from other processes might also require CPU time. This makes it even more difficult in practice to measure and ensure the speedup from using multiple threads.\nCreating threads Before discussing how to communicate between threads however, we first look at how we can create threads in C.\nIn the previous sections on processes, multiple processes could be created through the fork and exec functions. These function wrap the OS system-calls that are required to achieve this. Therefore, this comes intrinsically with the OS.\nThreads are of course also fully supported by the OS, but they typically require a more high-level API to easily work with. Most programming languages provide their own versions of these APIs, but they all internally call the OS-provided functionality. As we are working with Linux, we will be using the standard POSIX API library called pthreads.\nPthreads A simple example for creating a new pthread is given below:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; int counter; void* startThreadJob() { unsigned long i = 0; counter += 1; printf(\u0026#34; Job %d started\\n\u0026#34;, counter); for(i=0; i\u0026lt;(0xFFFFFFFF); i++) { // do nothing  // this is just here to keep the thread running for a while!  } printf(\u0026#34; Job %d finished\\n\u0026#34;, counter); return NULL; } int main(void) { int i = 0, err; pthread_t tid[2]; counter = 0; while(i \u0026lt; 2) { // pthread_create has 4 parameters:  // 1. pointer to pthread_t, needed to keep thread state  // 2. configuration arguments for the thread (passing NULL means we use the defaults here)  // 3. pointer to the function that will run in a separate thread  // 4. parameters to pass to the thread (no arguments for startThreadJob, so we pass NULL)  err = pthread_create( \u0026amp;(tid[i]), NULL, \u0026amp;startThreadJob, NULL ); if (err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create thread :[%s]\u0026#34;, strerror(err)); } i++; } // pthread_join pauses the current thread until the thread in the first argument is terminated  // Note that the \u0026#34;main\u0026#34; function automatically also executes in a thread, which we often call the \u0026#34;main thread\u0026#34;.  // If the joined thread was already terminated, pthread_join returns immediately  // The second argument can be used to store the return value of the thread  // Our thread currently doesn\u0026#39;t return anything, so we pass NULL  pthread_join(tid[0], NULL); pthread_join(tid[1], NULL); return 0; }  When the code above gets compiled, the pthread library has to be used. This library contains the object files that implemented the functions pthread_create() and pthread_join(). Compilation can be done because of the #include \u0026lt;pthread.h\u0026gt; line. For linking however, you have to separately tell gcc to use the pthread library as well by adding -lpthread (e.g., gcc -o program program.c -lpthread).\n   The example above is quite simple and doesn\u0026rsquo;t show a few of the more complex aspects of dealing with threads, particularly passing data into and out of the thread function (parameters and return value). The following example shows how you can pass a single parameters to the thread and how you can handle a return value. Note that we use (void*) as a type here, since the parameters can be anything and pthread_create can\u0026rsquo;t know which type they will be. As such, you as the programmer need to first cast to (void*) when creating the thread, then cast the (void*) back into what you really want it to be in the thread.\nSomething similar happens for the return value in pthread_join. Can you figure out why this needs a (void**), even though the thread function returns a (void*)? (or look it up online if you can\u0026rsquo;t?)\n #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;pthread.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt; void* threadWithParameter(void* textInput) { char* text = (char*) textInput; unsigned long i = 0; printf(\u0026#34;In thread %s: Thread started\\n\u0026#34;, text); for(i=0; i\u0026lt;(0xFFFFFFFF); i++) { // do nothing  // this is just here to keep the thread running for a while!  } printf(\u0026#34;In thread %s: Thread finished\\n\u0026#34;, text); return text; } int main(void) { int i = 0, err; pthread_t tid[2]; while(i \u0026lt; 2) { if ( i == 0 ) { err = pthread_create( \u0026amp;(tid[i]), NULL, \u0026amp;threadWithParameter, \u0026#34;nr 1\u0026#34; ); } else if( i == 1 ) { err = pthread_create( \u0026amp;(tid[i]), NULL, \u0026amp;threadWithParameter, \u0026#34;nr 2\u0026#34; ); } if (err != 0) { printf(\u0026#34;\\ncan\u0026#39;t create thread :[%s]\u0026#34;, strerror(err)); } i++; } char* returnValue = NULL; pthread_join(tid[0], (void **) \u0026amp;returnValue); printf(\u0026#34; In main thread: Thread %s returned!\\n\u0026#34;, returnValue); pthread_join(tid[1], (void **) \u0026amp;returnValue); printf(\u0026#34; In main thread: Thread %s returned!\\n\u0026#34;, returnValue); return 0; } "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/lab2_niceguy/",
	"title": "7.4: Mr. nice guy(lab)",
	"tags": [],
	"description": "",
	"content": "  image source: amazon.com    Write a program in C that fetches its own PID and priority. Display the PID, the nice value and the priority.  hint: use getpriority() in #include \u0026lt;sys/resource.h\u0026gt; and use PRIO_PROCESS      An example output    Adjust the program above (after copying it, off course) so it sleeps for 5 seconds after those operations. Repeat that process for 100 times. Compile and run that program. While that program is running, and is reporting back every 5 seconds:  change the nice value (on another terminal. Search on google for the command you need for this.) verify in the reporting that it works change the nice value so the overall priority is at its lowest change the nice value so the overall priority is at its highest      An example output    Adjust the program above (after copying it, off course) so it goes through the exact same changes in priority. Instead of changing it externally the program should change it internally, on itself.  hint: use setpriority()      An example output    Write a program in C that forks 25 new processes. Each forked process re-nices itself to one specific value (equally distributed over -20, -10, 0, 10, 19). That re-niced value won\u0026rsquo;t change anymore. After doing that, the processes count the number prime numbers lower than 10000. When they are finished, they print their count value (together with their ID and the PRIORITY).  Make sure the parent process waits until all its children are done. The printing of the count value is important \u0026hellip; Experiment what happens when you don\u0026rsquo;t do that.      An example output     Use the top command to inspect priority and nice values for running tasks on the system\n What are the PR and NI columns? How do you interpret their values? How do you see which tasks are real-time tasks? Use top while running the previous two exercises. Do you see the values changing as you\u0026rsquo;d expect? Why (not)?    Adjust the blink program\n By default, you can configure this to give all of the processes either high, normal or low priority (see the defines on top) Change the logic so that the first half of the processes (top of the screen) get highest priority, and the bottom half get lowest priority Describe the results. Is this what you expected? Why (not)? Note: since you\u0026rsquo;re running Ubuntu in a Virtual Machine, it might be that it can\u0026rsquo;t handle the default amount of processes. If necessary, lower the count and try again.    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/lab2_browsing/",
	"title": "9.4: Simply browsing through pages",
	"tags": [],
	"description": "",
	"content": "  image source: wikipedia.com    Get page size on your Linux system through a C program    An example output    Assume a system with a 32-bit logical addresses and a 4-kB (4096 bytes) page size. Write a C program that is passed a logical address (in decimal) through a command line argument. Print the page number (p) and the offset (d) for the given address.  Try with at least addresses 19986 and 3638136 Hint: lookup \u0026ldquo;bit masking\u0026rdquo;      An example output    Imagine the following system that is using a 2-level hierarchical page table:   5-bit processor (so physical memory of 32 bytes, starting at 0x00 and ending at 0x1F)\n  page size of 4 bytes (so 8 pages of 4 bytes each)\n  The outer page table has 4 entries, the inner page table has 8 entries\n  The outer page table\u0026rsquo;s values are [3,0,2,1], the inner page table\u0026rsquo;s values are [3,5,6,0,7,4,1,2]\n  Write a program that, given a logical address, returns the physical address by looking it up through the two-level page table\n  Test this with the following inputs and expected outputs:\n 21 =\u0026gt; 0x11 0 =\u0026gt; 0x04 27 = 0x1B 10 = 0x0E 3 = 0x07 28 = 0x00 19 = 0X1F 32 =\u0026gt; out-of-bounds error    Tip: it helps to first make a drawing/sketch/schematic of what this looks like before programming\n  Note: logical addresses will still be 5-bits in length! Think up-front about how the bits should be used!\n  Extra challenge (optional): add a way to have gaps in your page tables (i.e., for not-yet-allocated pages, so you can simulate page faults to find a new main memory gap and assign that to the page tables dynamically). Track whether a page is already assigned or not using an additional valid/invalid status bit.\n    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/ecosystems/",
	"title": "2.5: C Ecosystems",
	"tags": [],
	"description": "",
	"content": "Separation of concerns: functions in different C files To make the division clearer, we prepare the following C code, split into two different files:\nFile hello.c:\n// hello.c char* hello() { return \u0026#34;heykes\u0026#34;; } File main.c:\n// main.c #include \u0026lt;printf.h\u0026gt;int main() { printf(\u0026#34;%s\u0026#34;, hello()); return 0; } The main function has no knowledge of hello() because it lives in a different source file. This will be fine if we link the machine code together after compiling. compiling main.c separately gives this:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ gcc -c main.c main.c:5:18: warning: implicit declaration of function 'hello' is invalid in C99 [-Wimplicit-function-declaration] printf(\"%s\", hello()); ^ 1 warning generated.  It is a WARNING - not an ERROR - so it still compiles! Wow! That is thanks to the -c flag (compile only). The warning is easily solved with a forward function declarations before the main function: char* hello ();. This is the crucial difference between declaration and definition. However, the problems are not yet solved if we want to link this without hello.c:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ gcc main.o Undefined symbols for architecture x86_64: \"_hello\", referenced from: _main in main.o ld: symbol(s) not found for architecture x86_64 clang: error: linker command failed with exit code 1 (use -v to see invocation)  Okay, so now a blocking ERROR was generated. We also need the hello.o binaries to arrive at a successful working program. For that, we first have to execute gcc -c hello.c and then gcc main.o hello.o -o hey.\nWith the UNIX tool nm we can view the addresses that the linker needs to arrive at the hey executable. Try opening hello.o with a text editor. You then see something like this:\n cffa edfe 0700 0001 0300 0000 0100 0000 0300 0000 f001 0000 0020 0000 0000 0000 1900 0000 8801 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 7800 0000 0000 0000 1002 0000 0000 0000 7800 0000 0000 0000 0700 0000 0700 0000 0400 0000 0000 0000 5f5f 7465 7874 0000 0000 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 0000 0000 0000 0000 0d00 0000 0000 0000 1002 0000 0400 0000 8802 0000 0100 0000 0004 0080 0000 0000 0000 0000 0000 0000 5f5f 6373 7472 696e 6700 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 0d00 0000 0000 0000 0700 0000 0000 0000 1d02 0000 0000 0000 0000 0000 0000 0000 0200 0000 0000 0000 0000 0000 0000 0000 5f5f 636f 6d70 6163 745f 756e 7769 6e64 5f5f 4c44 0000 0000 0000 0000 0000 0000 1800 0000 0000 0000 2000 0000 0000 0000 2802 0000 0300 0000 9002 0000 0100 0000 0000 0002 0000 0000 0000 0000 0000 0000 5f5f 6568 5f66 7261 6d65 0000 0000 0000 5f5f 5445 5854 0000 0000 0000 0000 0000 3800 0000 0000 0000 4000 0000 0000 0000 4802 0000 0300 0000 0000 0000 0000 0000 0b00 0068 0000 0000 0000 0000 0000 0000 0200 0000 1800 0000 9802 0000 0400 0000 d802 0000 2400 0000 0b00 0000 5000 0000 0000 0000 0200 0000 0200 0000 0200 0000 0400 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 5548 89e5 488d 0500 0000 005d c368 6579 6b65 7300 0000 0000 0000 0000 0000 0000 0d00 0000 0000 0001 0000 0000 0000 0000 0000 0000 0000 0000 1400 0000 0000 0000 017a 5200 0178 1001 100c 0708 9001 0000 2400 0000 1c00 0000 a8ff ffff ffff ffff 0d00 0000 0000 0000 0041 0e10 8602 430d 0600 0000 0000 0000 0700 0000 0000 001d 0000 0000 0100 0006 1200 0000 0e02 0000 0d00 0000 0000 0000 1900 0000 0e04 0000 3800 0000 0000 0000 0100 0000 0f01 0000 0000 0000 0000 0000 0800 0000 0f04 0000 5000 0000 0000 0000 005f 6861 6c6c 6f00 5f68 616c 6c6f 2e65 6800 4c5f 2e73 7472 0045 485f 6672 616d 6530 0000  Beautiful, but not very clear. nm does help some:\n Wouters-MacBook-Air:cmake-build-debug wgroenev$ nm main.o 0000000000000060 s EH_frame0 0000000000000037 s L_.str U _hello 0000000000000000 T _main 0000000000000078 S _main.eh U _printf Wouters-MacBook-Air:cmake-build-debug wgroenev$ nm hello.o 0000000000000038 s EH_frame0 000000000000000d s L_.str 0000000000000000 T _hello 0000000000000050 S _hello.eh  You can see that in main.o the function _hello is assigned an unknown address (hence the U). This means that the left hand should assume that it is yet to come - and luckily it is correctly defined in hello.o at address 0000000000000000 (there is only 1 function).\nNote that both .o files have overlapping \u0026ldquo;address spaces\u0026rdquo;: both _hello and _main are at address 0000000000000000; this is normal. The linker will properly change/offset these addresses when creating the final program so they no longer overlap.\n This is the way the files will be coupled to each other:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD A[hello.c] B[main.c] Chello.o] Dmain.o] E{hey executable} E -- C E --|\"search main() via linker\"|D C -- A D -- B D -.-|\"search hello() via linker\"|C  Functions that have been declared in other source files must therefore be redefined (possibly with the external keyword) in your own source file where you wish to use the function. This way the compiler knows that there is a function with that signature, but \u0026ldquo;he will still come across it\u0026rdquo;. This will be further elaborated in the next labs.\nCompiling everything We use the UNIX GNU gcc compiler to compile C source files and convert them into binaries. The simplest possible way to do so is:\n gcc *.c\n Due to the lack of a target file name, the compiler creates an \u0026ldquo;a.out\u0026rdquo; file that you can execute (chmod +x has already been done for you). You can specify this with the \u0026ldquo;-o\u0026rdquo; flag. If you have something more to link, put everything in a row one after the other.\nHowever, there are still a lot of compiler options that are explained at gcc.gnu.org that you can play with.\nWhen targeting another platform, you will need a cross-compiler that compiles on your computer for another computer. That is, the instruction set might differ! (64 or 32-BIT, RISC/ARM, \u0026hellip;) Instead of using the default GCC compiler: gcc bla.c, you will download and install a custom cross-compiler and evoke it the same way: arm-eabi-none-gcc bla.c. The Game Boy Advance (GBA) or RaspberryPi for instance have an ARM chip-set and require this cross-compiler. This differs from most x86 chip-sets that leverages gcc.\n Are you still cross-compiling if you are compiling on an ARM machine yourself, using gcc, compiled for that chip-set? What if you compile code on the Raspberry for your laptop?\n  Step 1: compiling As seen in the above schematic, executing your source code requires the activation of two steps: compiling (1), and linking (2). C Preprocessor flags get parsed just before compiling. Simply calling the gcc compiler executes all steps at once. Only compiling is done using the -c statement (source input) and providing the source files as arguments, producing object files, which can be then linked into a binary.\nStep 2: linking After obtaining object files it is simply a matter of concatenating them (\u0026lsquo;linking\u0026rsquo;), to create the native executable binary file, using the -o flag and providing the object files as arguments. After linking, inspecting the disassembly (see chapter 5, debugging on how to do so in detail) shows the concatenated results.\nRepeatedly compiling (1) using a script It is annoying to have to type the same command all the time, so a simple alternative is to put your gcc command in a shell script:\n#!/bin/sh clear \u0026amp;\u0026amp; gcc -o mystuff source.c \u0026amp;\u0026amp; ./mystuff (2) Makefiles In the C world there is such a thing as a \u0026ldquo;Makefile\u0026rdquo; that defines which source files should be compiled, and in which order. This is useful for large applications where an overview must be kept.\nWith Makefiles you can describe \u0026ldquo;targets\u0026rdquo; that perform certain actions for you. For example, cleaning up binaries, compiling and linking, all as a separate step. Stringing steps together is of course also possible.\nFile Makefile:\n.DEFAULT_GOAL := all CC=gcc clean: rm -rf *.o rm -rf *.out compile: $(CC) -c main.c -o main.o link: $(CC) -o main.out main.o all: clean compile link Typically, the compiler used is set as a shell variable (CC = gcc). You can see here that compiling (gcc with the -c option does not link) and linking is split into separate make \u0026lsquo;actions\u0026rsquo;. This is not really necessary in our exercises, but shows the organizational strength of Make here.\nExecuting the above steps can be done using the make command and a default goal (all), or make compile for a specific goal (executes the \u0026lsquo;compile\u0026rsquo; steps only).\nFor more information on the correct Makefile syntax, see the GNU make documentation.\nCreate a Makefile that contains two targets: compile and run. The default target should execute both in sequence. As for what to compile, write a simple program that outputs \u0026ldquo;hello, (name)\u0026rdquo;. The name is something you ask from the user using the stdio function gets().\n  (3) CMake As you can judge for yourself from the above Makefile syntax, a typical project build file can get pretty verbose and complicated. Recent attempts to mitigate this have resulted in more modern build systems for C/C++, such as the general-purpose SCons, and CMake. CMake builds\u0026hellip; Makefiles. That means you\u0026rsquo;ll have to execute cmake, which generates a Makefile, and then make. It\u0026rsquo;s a two-step process.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD cmake[CMake] make[Makefiles] cmd[gcc \u0026 cmdline] cmake -- make make -- cmd  CMakeLists.txt contains instructions to generate a Makefile:\n cmake_minimum_required(VERSION 3.10) project(my_little_pony) set(CMAKE_CXX_STANDARD 11) add_executable(my_little_pony main.cpp biblio.cpp biblio.h animals.cpp animals.h)  A simple CMake file is much easier to read and write than a Makefile. See CMake tutorial for more information. CLion manages the add_executable arguments for you: adding new files to your project will also automatically add them to the list.\nSince CMake builds Makefiles and Makefiles use cmdline to evoke the compiler, you are essentially using high-level tools that use low-level tools. This makes it easier to repeatedly compile bigger projects, instead of calling gcc file.c yourself every single time.\n(4) IDEs Lightweights A source file consists of simply plain text. Any text editor is sufficient to write your C program. However, it may be useful to use Sublime Text or Visual Studio Code. These modern powerful editors have built-in auto-completion and build tools.\n Sublime Text 3 Build Systems en C/C++ and docs Visual Studio Code C/C++ integration  We will not stop old-school fans from using Emacs or Vi(m).\nHeavyweights CLion is the perfect cross-platform and cross-compiler candidate to do the heavy C/C++ lifting for you, and it comes with integrated debugging, stack inspection, and everything else you might expect from an IDE. If you are familiar with IntelliJ, you will love CLion: it\u0026rsquo;s built on the same platform (IDEA) - even most shortcuts are the same.\n  CLion is not free, but it is highly recommended for students (and they get a free license if you register with your university e-mail address). CLion also by default works with CMake.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch4-pointers/lab/",
	"title": "4.3: The Ancient Library",
	"tags": [],
	"description": "",
	"content": "The Ancient Library  Stuff just got interesting: you find yourself inside an extremely old library, where things clearly have not been touched for ages. A green glowing ball rests in the center of the room, giving the books an odd, but compelling color. Do you dare to touch the books? Let us go ahead and clean this place up a bit.\n   Source: dublin2019.com   1. Modeling the books The back of the first row of books is barely readable, but you can distinguish the following titles and authors:\n Cleaning the streets Beyond the Gate, by Ulder Ravengard Beyond the depths of the Underdark, by Drizzt Do\u0026rsquo;Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera  How should we proceed to model the concept of a book? Right, a struct! Keep track of the title and the author using char* properties.  Proceed with caution and whatever you do, do not directly look into the green light! Use the following blueprint:\n#define SIZE 4 Book* create_books() { // ?? } void print_books(Book* library) { // ?? } int main() { Book* library = create_books(); print_books(library); // should print the above list ([title], by [author])  return 0; } 2. Linking things together The strcuture you provided can be expanded with a third property: Book* next, pointing to the next element in the row. That way, when looping through all books, we simply need to follow the \u0026lsquo;next\u0026rsquo; pointer, until a NULL is reached, indicating the end of the library. This technique makes it possible to loop through things without knowing it\u0026rsquo;s size!\nRework your implementation by removing the #define SIZE 4 statement, and by relying on the next pointer in the print_books() method.\nTips:\n What should the value of next be when creating the book instances in create_books()? What should that function return? A handle to what? You can use while(ptr) { ... } instead of a for loop. The value within the while() statement always evaluates to true, unless a \u0026ldquo;nullpointer\u0026rdquo; (a value of 0 or NULL) is detected. That is exactly what we want. Another method to ease use of the malloc() statements might come in handy, such as Book* create_book(char* title, char* author).  3. Cleaning up the library The place looks dusty, doesn\u0026rsquo;t it?  Also, the books seem to be placed in a random order. Why don\u0026rsquo;t we take the time, now that we are here and enjoy the green glow, to order these books alphabetically by author? You can ignore the surnames, simply sort on the property author. That is, the expected output of your program should be:\n Beyond the depths of the Underdark, by Drizzt Do'Urden Ancient Aberrations and how to prevent them, by Elminster Aumar Killing people with cows: the ins and outs of Wild Magic, by Neera Cleaning the streets Beyond the Gate, by Ulder Ravengard  Create and call void sort_books(Book* library) before printing them.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; H{head} T{tail} A[book 1] B[book 2] C[book 3] D[book 4] NNULL] A --|next| B B --|next| C C --|next| D D --|next| N H -- A T -- B  Tips:\n Use the strcmp() function from \u0026lt;string.h\u0026gt; to compare two strings. Sorting a linked list is not as difficult as you might think. You will need some sort of swapping function. Take a look at the above drawing. If book 4 needs to be first, how do you swap it with book 1, while keeping the links intact? Use recursion to repeatedly call sort_books() within the same function. Which book should be placed in the beginning? What should you process next? Think in terms of head and tail.  4. Wild Surge! ASC/DESC  A sudden gust of wind enters the old library. You immediately halt what you were doing to carefully listen if imminent threats are upon you and your party. A cloaked figure swings open a side-door and starts casting a spell that sounds very unfamiliar to you. Praeses, Alia, Fero\u0026hellip; An alteration, but which one?! Quick, hide the books! But alas, it was too late\u0026hellip;  While the spell completes, you hear the figure cursing \u0026ldquo;annoying wild surges!\u0026rdquo;. The voice sounds like a woman. She rushes off, but the part of her face you were able to catch in the glimpse of an eye seems oddly familiar\u0026hellip; Suddenly, you know it. The book! The cow book! That was the author! Neera\u0026hellip;\n Neera managed to completely reverse our sorting strategy, sorting Z to A instead of A to Z. She used a function pointer to encapsulate strcmp(). The following is your main function:\nint main() { Book* library = create_books(); neera_encounter(); // changes a function pointer from asc to desc  sort_books(library); print_books(library); // prints Z -\u0026gt; A  return 0; } Create a function pointer that is initialized to the address of your ascending sorter. Neera her Wild Magic switches this to the address of a descending sorter. That pointer will be used inside sort_books() instead of strcmp().\nTips:\n Re-read chapter 4 (pointers)! You will need two extra functions, asc() and desc(), besides neera_encounter().  5. Do you rest until fully healed?  The rumble of Neera\u0026rsquo;s spell made your head rush and you fill dizzy. Maybe it would be better for you and your party to stay for the night and set up camp. During the night, you make the time to write up on the past events. It looks like it will be quite a thick book, something new to add to the old library. Now, should you add the book at the beginning of the row, or all the way at the end?\n Create a function called void add_book(Book* library, Book* book), that adds a new book to the end of the library.\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph LR; new(new book) A[book 1] B[book 2] NNULL] B -.-|next| new new --|next| N A --|next| B  That was quite simple, wasn\u0026rsquo;t it? What if we want it to add new books to the beginning of the library? You cannot change the Book* library pointer as it is copied over by-value. For this to work, we need to change the signature to a pointer of a pointer: void add_book(Book** library, Book* book).\nUse the following main function to test your code:\nint main() { Book* library = create_books(); Book* newbook = create_book(\u0026#34;My adventures\u0026#34;, \u0026#34;The Hero\u0026#34;); // add newbook to the library. But how?  print_books(library); return 0; } "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-debugging/debugging-gdb/",
	"title": "5.2: The Hard Way: GDB",
	"tags": [],
	"description": "",
	"content": "2. The hard way: Command-line debugging using GDB In order to fluently debug binary programs, they have to be compiled with the debug flag, gcc -g. This will add metadata to the binary file that gdb uses when disassembling and setting breakpoints. IDEs automatically add metadata like this when you press the \u0026ldquo;Debug\u0026rdquo; button on them, but since this is a command-line application, we need to do everything ourselves.\n2.1 With debug flags Let\u0026rsquo;s start with a heap-based application we would like to inspect:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;string.h\u0026gt; char password[] = \u0026#34;250382\u0026#34;; int main(int argc, char *argv[]) { int stackvar = 5; char *buf = (char *)malloc(100); char *secret = (char *)malloc(100); strcpy(secret, password); printf(\u0026#34;Crackme! \\n\u0026#34;); printf(\u0026#34;Password? \u0026#34;); scanf(\u0026#34;%s\u0026#34;, buf); if (!strcmp(buf, secret)) { printf(\u0026#34;Password OK :)\\n\u0026#34;); } else { printf(\u0026#34;Invalid Password! %s\\n\u0026#34;, buf); } return 0; } If the source code would not have been supplied, making an estimated guess against the password would take a (very) long time. (We naively assume here that the program has been compiled with debug information enabled).\nCompile using gcc -g hackme.c. Take a look at the filesize - without flag:\n wouter@wouter-Latitude-7490:~/Development$ gcc hackme.c -o hackme.bin \u0026\u0026 ls -la | grep hackme.bin -rwxr-xr-x 1 wouter wouter 8568 Jan 7 19:59 hackme.bin  With flag:\n wouter@wouter-Latitude-7490:~/Development$ gcc hackme.c -g -o hackme.bin \u0026\u0026 ls -la | grep hackme.bin -rwxr-xr-x 1 wouter wouter 11352 Jan 7 19:59 hackme.bin  Star the gdb debugger using gdb [binary]. It will enter the interactive gdb shell, where you can set breakpoints, step through code, and have a chance at inspecting the heap, where we might attempt to figure out what\u0026rsquo;s hidden in there.\nThings you need to know from the GDB debugger:\n r: running the program (main() method execution) c: continue after a breakpoint i: inspect (i r [regname]: inspect register name) start and next (shorthand n) or step: start stepping through the application. b *[addr]: set breakpoint at certain function/line/*address (see manual). Conditionals are possible, for instance: break func if arg == 3. delete: deletes all breakpoints disassemble [fn]: disassembles functionname (after running) x/[length][format] [address expr]: inspect dynamic memory block (see manual) print x (shorthand: p): print var, or \u0026amp;var address (Enalbe printing of addresses: show print address) info (shorthand: i) address/line (fn) or source  Bootstrap gdb and step through the whole application. As soon as the stackvar has been evaluated, try to inspect the memory value using x/d. The address expression could be hexadecimal, or \u0026amp;stackvar.  How could you evaluate a heap variable using the x command? If you have the address, how do you pry out the value on the heap?\n  More useful commands:\n Don\u0026rsquo;t remember which breakpoints you\u0026rsquo;ve set? i b. (info breakpoints) Don\u0026rsquo;t remember where you\u0026rsquo;re at now? Inspect the stack: bt (backtrace), optionally appended with full that includes local variables. Don\u0026rsquo;t know which registers to inspect? i r (see manual).  Can you spot the stack pointer and program counter? Can you see what happens to them when a function is called or an instruction is executed? Why do you think the PC doens\u0026rsquo;t simply increment with the expected four bytes when instructing gdb to execute a line of code?\n  Do not forget that the expression that is printed out is the one to be evaluated after you enter the \u0026lsquo;next\u0026rsquo; command. You can already inspect the stack variable address but it will contain junk:\n (gdb) start Temporary breakpoint 1 at 0x7d9: file hackme.c, line 11. Starting program: /home/wouter/Development/hackme.bin Temporary breakpoint 1, main (argc=1, argv=0x7fffffffdd68) at hackme.c:11 11 int stack = 5; (gdb) x/d \u0026stack 0x7fffffffdc6c: 21845 (gdb) next 12 char *buf = (char *)malloc(100); (gdb) x/d \u0026stack 0x7fffffffdc6c: 5  Address 0x7fffffffdc6c first contains 21845 - a coincidence that might have another value on your machine.\nBootstrap gdb, disassemble the main function, and set breakpoints after each malloc() call using b *[address]. You can check the return value, stored at the register eax, with i r eax.\n  How come something interesting is hidden in eax after calling malloc()?\n Because eax is the return value register, or the accumulator. You should be familiar with it due to other Hardware-oriented courses. Because malloc returns a void pointer - read the man pages carefully!  2.2 Without debug flags Now try to \u0026lsquo;hack\u0026rsquo; the password using gdb without the -g compiler flag. Imagine someone has put up a binary file on the internet and you managed to download it. No source code available, and no debug information compiled in. The gdb tool still works, disassembling still works, but method information is withheld. That means calling start and next will not reveal much-needed information about each statement, and we will have to figure it out ourselves by looking at the disassembly information.\nTry to disassemble again and look at the heap value of our secret. Notice that you will not be able to use something like x [varname] because of the lack of debug information! We will have to rely on breakpoints of address values from the disassembly.\n  Remember to always run the program first before disassembling - otherwise address values will be way too low, and thus incorrect. bt does noet help us either here: No symbol table info available.\nWhen inspecting the return value of eax, gdb returns a relative address for our current program (8 BITS), while we need an absolute one (16 BITS) when using the x command to inspect the heap. Look at the disassembly info to prepend the right bits:\n ---Type  to continue, or q  to quit--- 0x0000555555554844 : mov -0x8(%rbp),%rdx 0x0000555555554848 : mov -0x10(%rbp),%rax 0x000055555555484c : mov %rdx,%rsi ... (gdb) b *0x00005555555547ea Breakpoint 1 at 0x5555555547ea (gdb) r Starting program: /home/wouter/Development/osc-labs/solutions/debugging/a.out Breakpoint 1, 0x00005555555547ea in main () (gdb) i r eax eax 0x55756260 1433756256 (gdb) x 0x55756260 0x55756260: Cannot access memory at address 0x55756260 (gdb) x 0x0000555555756260 0x555555756260: 0x00000000  As you can see, 0x55756260 is an invalid memory address, but based on the disassembly info, we can deduce it is actually 0x0000555555756260 we need to look at.\nThere\u0026rsquo;s another way to pry out the return value of the last statement. The finish command executes until the current stack is popped off and prints the return value. Set a breakpoint to just below malloc(), call finish, and the result is:\n(gdb) finish Run till exit from #0 __GI___libc_malloc (bytes=100) at ./malloc/malloc.c:3294 0x0000aaaaaaaa08f4 in main () Value returned is $1 = (void *) 0xaaaaaaab22a0 There\u0026rsquo;s your address you can now inspect using r 0xaaaaaaab22a0. It\u0026rsquo;ll likely still be 0x00000000, so try to step and inspect until it contains the value you\u0026rsquo;re interested in.\nRegisters are platform- and architecture-specific! In other words, the return value register eax is only available on x86_64 CPUs. If you\u0026rsquo;re on a modern Mac with an ARM64, you\u0026rsquo;ll have to check info all-registers and consult the ARM Developer Documentation to find the correct register. The pc and sp registers are universal concepts.\n The (still) hard way: DDD, a UI on top of GDB Instead of invoking gdb, one can also employ ddd. This is a crude UI on top of the gdb debugger, with multiple windows where the same commands can be entered as you have learned so far. However, ddd also allows you to visualize heap/stack variables while stepping through the application. The Figure below shows a screen-shot of a debug session of our hackme app using ddd.\nThings to try out:\n Display the Source Window via the View menu. This window lets you set breakpoints and interact with the source code. Display the Machine Code Window via the View menu. This window is the equivalent of bt (backtrace) in gdb. Right-click on a line in source (compile with -g again!) -\u0026gt; Add breakpoint Start/step using the buttons or the commands in the cmdline window. Right-click in the main window -\u0026gt; \u0026ldquo;New Display\u0026rdquo; to add variables by name to watch (for instance buf and password, as shown). You can also watch references to functions - any valid gdb-style expression will do.  Take a moment to fiddle with ddd after correctly installing it. Try to inspect the same heap variable as the previous exercises, but this time visualize them in the main window. It should be (slightly) easier to accomplish.\n  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch5-debugging/",
	"title": "5: Debugging",
	"tags": [],
	"description": "",
	"content": "Chapter 5 Debugging in C Important concepts to grasp:\n Breakpoints, stepping into/over, continuing Inspecting the stack and the heap Disassembling, objdumping  Recommended Reading  The GNU Project Debugger Documentation Hackme: exploiting heap bugs Google Test Primer  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab2_threadsmgmt/",
	"title": "6.5: Threads (lab)",
	"tags": [],
	"description": "",
	"content": "   p.dinobook { color: #7E7E7E; font-size: 14px; font-weight: 300; letter-spacing: -1px; padding-top: 0px; margin-top: -20px; text-align: center; }  source: SILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G. Operating System Concepts. 9th ed. Hoboken: Wiley, 2013.\nCreating and inspecting threads   Take a closer look at the first pthreads example code from the previous section (with the startThreadJob function).\n Without executing the code, what do you think will be printed to the screen (with regards to the \u0026ldquo;Job %d started and Job %d finished\u0026rdquo; messages)? Why? Compile and run the code on your machine What is the actual output? Is it the same as what you guessed? Why (not)? Now run the second pthreads example code (with the threadWithParameter function). Does it have the same problem? Why (not)? Note: you do not have to think of a \u0026ldquo;fix\u0026rdquo; for this problem just yet: we\u0026rsquo;ll discuss some possible solutions in the next Section on inter-thread communication.    Write a C-program that has 3 threads. Each thread is passed its integer ID (1,2,3) through a parameter and announces its existence through a printf() together with its PID. Note that, unlike when spawning multiple processes, the PID is of course the same for all threads.\n Note: the second pthreads example code for passing parameters is a bit dirty, requiring an if-test for each thread to pass the correct string parameter. For this exercise, make sure you approach this in a (much) cleaner way! Now change the code so the PID isn\u0026rsquo;t printed in the thread itself, but rather passed as a return value to the main thread (via pthread_join) and printed there.      An example output of the first part of the exercise     In the previous processes lab, you had to implement a program that spawned different parallel processes, each calculating all the primes between 0 and a given number. Let\u0026rsquo;s adapt this to threads, but with a twist!\n  Write a C-program that has 4 threads (in addition to the main thread). Each thread calculates all prime numbers between a lower and larger limit. Given a maximum number N which is divisible by 4 (e.g., 100000):\n Thread 1 computes all primes between 2 and N/4 Thread 2 computes all primes between N/4 + 1 and N/2 Thread 3 computes all primes between N/2 + 1 and 3N/4 Thread 4 computes all primes between 3N/4 + 1 and N Make sure you pass these limits properly to the threads as function parameters. Think about how you can pass more than 1 parameter!    After all threads are done, the main thread prints all the resulting primes in the proper order (low to high) to the screen.\n Note: this means that the threads don\u0026rsquo;t do the printing themselves like in the previous exercises: you have to instead use heap-allocated memory to communicate back the results to the main thread!  For this exercise, it\u0026rsquo;s sufficient for each thread to store their results separately and communicate them back at the end. We will see other options later.   There are several ways of doing this and none of them are perfect (we\u0026rsquo;ll see how to make things better in the next Section). For now, just pick a method you think will work, don\u0026rsquo;t worry too much about cleanliness.    Imagine you wouldn\u0026rsquo;t have to print the results fully ordered from low to high (but still in the main thread) (You don\u0026rsquo;t actually have to implement this). Would you be able to make the program more efficient then? Why (not)?\n Put differently: does every thread take an equal amount of time to perform its work. Why (not)?      "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch2-c/ecosystems-labs/",
	"title": "2.6: Splitting up Code",
	"tags": [],
	"description": "",
	"content": "Building with Makefiles It is time to split up our Orc exercise code into different parts:\nmermaid.initialize({ startOnLoad: true, flowchart: { useMaxWidth: true } });  graph TD E{Executable} A[main.c] AAmain.o] B[orcs.c] BBorcs.o] D[orcs.h] E -- AA E -- BB AA -- A AA -- D BB -- B BB -- D  Create four different files:\n orcs.h - this is where your struct definition resides. orcs.c - this is where your methods related to orcs reside. Include the orc header file. main.c - this is where your main method resides. Include the orc header file. Makefile - builds everything. Create a compile, link, and clean target.  Compile both C files with separate gcc -c commands, merging them together with a third command, as seen in the previous section.\n  When you think you can manage using the gcc command in the commandline, automate everything by leveraging the power of Makefiles.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/interthread/",
	"title": "6.6: Inter Thread communication",
	"tags": [],
	"description": "",
	"content": "One raison d\u0026rsquo;Ãªtre for multi-threaded applications is resource sharing. In the example that was given earlier a global variable \u0026lsquo;counter\u0026rsquo; was used. No measures were taken for securing this approach and we got some unexpected results. The output of the example looks like shown below:\n  This probably came as a surprise ð  It should be clear that what we wanted to see was Job 1 started followed by Job 1 finished and that this would be repeated again for job number 2.\nThe first of both threads that gets to its function increments the counter from 0 to 1. However, before this thread has finished its execution, the second thread has started and has incremented the counter from 1 to 2. By the time the first thread finishes, the counter value is 2, which is in contrast with the intended value of 1.\nCritical Section and Race Conditions At first glance, it might seem that this problem is easily solvable by copying the global counter to a local, per-thread variable for later use:\nvoid* doSomeThing(void *arg) { unsigned long i = 0; int local_id; // new, per-thread variable  counter += 1; local_id = counter; // make a local copy of counter for this thread  printf(\u0026#34; Job %d started\\n\u0026#34;, local_id); for(i=0; i\u0026lt;(0xFFFFFFFF);i++); printf(\u0026#34; Job %d finished\\n\u0026#34;, local_id); return NULL; } If you tried to implement this, it would probably work\u0026hellip; most of the time at least. As explained in the first section on threads, threads are not necessarily run in parallel all the time, especially not if the number of threads is larger than the number of CPU cores. In fact, there are many situations where given 2 threads, the first one will get some time to execute, is then paused to give the second one some time to execute, unpaused to get some time etc.\nAs such, if two Threads executing doSomeThing() would be running on a single CPU, the following sequence of actions could occur:\n// Code that both threads will execute: counter += 1; local_id = counter; // -----------------------------------  // Thread 1 counter += 1; // after this, counter == 1  // Before Thread 1 can execute local_id = counter, it is paused by the CPU  // Thread 2  counter += 1; // after this, counter == 2  local_id = counter; // thus, in Thread 2, local_id is correctly 2 // Here, the CPU switches back to Thread 1 // Thread 1 local_id = counter; // now, local_id is still 2, instead of 1... mission failed We can see that the exact problem we were trying to prevent can still occur, but it depends on how the threads are scheduled by the OS. This is what makes multithreaded programming so difficult in practice: your program can execute without problems 99 times, but still fail the next. These bugs can be very hard to reproduce. They are often called race conditions, referring to e.g., a car/horse race where it\u0026rsquo;s not always the same contestant that wins, depending on the conditions of the racetrack. Note that this does not only occur in the case of multiple threads on the same core: similar problems can of course also happen if you have actual parallel threads as well.\nIn general, we can see that once multiple threads start accessing shared memory, things can go wrong. As such, these specific points in a multithreaded program are often referred to as the critical section: it\u0026rsquo;s of the highest importance (critical) that this section is executed by itself, without outside interference. Otherwise, race conditions can occur and we can introduce bugs.\nTo make this all a bit more tangible, we will be using an outside interactive tool, called Deadlock Empire. Instead of having you write correct code for a problem, this website challenges you to find bugs in existing code to show the many caveats of multithreaded programming. While the website uses the C# programming language and is a bit different from the C syntax we\u0026rsquo;re using, the high-level concepts are 100% the same.\nGo to the Deadlock Empire website and do:\n Tutorial: Tutorial 1: Interface Unsynchronized Code: Boolean Flags Are Enough For Everyone Unsynchronized Code: Simple Counter    Atomic operations Now that you have a feeling for race conditions and critical sections, it\u0026rsquo;s time to make things worse.\nThe example above isn\u0026rsquo;t only vulnerable through the \u0026ldquo;local_id = counter\u0026rdquo; code: the \u0026ldquo;counter += 1\u0026rdquo; code is also vulnerable to a race condition. This is because incrementing a variable (counter += 1) is not a so-called atomic operation. This means that internally, it is not implemented with a single CPU instruction, but rather composed out of a series of different operations/instructions.\nFor example, for counter += 1, the series of executed steps might look like this:\n Fetch value of counter from RAM and store it in the CPU cache memory Fetch value from CPU cache memory and store it in a CPU register for the calculation Add 1 to the register value Write the register value back to the CPU cache Write the cached value back to the RAM  Put differently: the CPU does not act on the value stored in memory directly, but rather a copy in a register. Copying from/to a register is not atomic, so bugs can occur. To make things simpler to reason about, we can boil this down to just two lines of code:\n// In simple pseudocode, counter += 1 might look like this: int temp = counter + 1; // temp is for example the CPU register here, and \u0026#34;counter\u0026#34; is the value in memory counter = temp; // the register value is written back out to the memory To the CPU, all of these steps are one or more instructions, each of which can also have a certain delay associated with them. As such, what can happen in practice is the following sequence of events:\n// Code that both threads will execute: int temp = counter + 1; counter = temp; // -----------------------------------  // Thread 1 int temp = counter + 1; // after this, Thread 1\u0026#39;s temp register contains the value 1. The \u0026#34;counter\u0026#34; value in RAM is still 0.  // Before Thread 1 can actually store this temporary register result in memory, the CPU gives control to Thread 2  // Thread 2  int temp = counter + 1; // \u0026#34;counter\u0026#34; was still 0 in RAM, so Thread 2\u0026#39;s temp register now also contains the value 1  counter = temp; // The \u0026#34;counter\u0026#34; value in RAM is now 1 // Here, the CPU switches back to Thread 1 // Thread 1 counter = temp; // The \u0026#34;counter\u0026#34; value in RAM is still 1 We can see that, even though two \u0026ldquo;counter += 1\u0026rdquo; lines of code were executed, the resulting value in memory is just 1 instead of 2. Again: you probably never saw this when testing the exercise in the lab, but theoretically it -could- happen, leading to randomly failing programmes.\nNote that this is a direct consequence of the fact that threads have separate program counters and register values, as discussed in the first Section on threads. Once individual threads are started, even if they are executing the same code, they do so in a partially separated context.\nGo to the Deadlock Empire website and do:\n Tutorial: Tutorial 2: Non-Atomic Instructions Unsynchronized Code: Confused Counter    You might now think that this can only happen if two threads execute the exact same code. However, you would be wrong, as illustrated by the following example from Wikipedia:\n// Thread 1 // ... other code b = x + 5; // ... other code  // Thread 2  // ... other code  x = 3 + z; // ... other code  Give a practical, numerical example of how, depending on CPU scheduling, b can end up with two very different values.\n  Mutexes and Locking As we\u0026rsquo;ve seen in many examples, things can go wrong really quickly when dealing with shared memory. Making matters worse, simple solutions (such as making thread-local copies of shared memory) are also doomed to fail eventually.\nAs such, it should be clear that we need a way to secure the critical sections in a thread. We need a way to make sure the OS will not pause a thread during a critical section, to make sure that all instructions are done before another thread gets execution time. Put differently: we need a way to turn groups of non-atomic operations into a single big atomic block.\nThe simplest way of doing this, is by means of a mutex. This term is a portmanteau of Mutual Exclusion. The behaviour can be seen as the lock on a toilet door. If you are using the toilet, you lock the door. Others that want to occupy the toilet have to wait until you\u0026rsquo;re finished and you unlock the door (and get out, after washing your hands). Hence, the mutex has only two states: one or zero, on or off, locked or unlocked.\nInternally, a mutex lock is typically implemented by means of a single, atomic CPU instruction. Otherwise, simply locking or unlocking the mutex would of course potentially lead to the problems it is trying to solve!\nThe concept of a mutex is implemented in the pthreads library as a new variable type: pthread_mutex_t. Locking and unlocking can be done through functions pthread_mutex_lock() and pthread_mutex_unlock(). As always, read the documentation for the exact usage of these functions.\nThe problematic example of the shared counter can be rewritten using a mutex:\nint counter; pthread_mutex_t counter_lock; /* THIS LINE HAS BEEN ADDED */ void* doSomeThing(void *arg) { unsigned long i = 0; int local_id; /* THIS LINE HAS BEEN ADDED */ pthread_mutex_lock(\u0026amp;counter_lock); /* THIS LINE HAS BEEN ADDED */ counter += 1; local_id = counter; /* THIS LINE HAS BEEN ADDED */ pthread_mutex_unlock(\u0026amp;counter_lock); /* THIS LINE HAS BEEN ADDED */ printf(\u0026#34; Job %d started\\n\u0026#34;, local_id); /* THIS LINE HAS BEEN CHANGED */ for(i=0; i\u0026lt;(0xFFFFFFFF);i++); printf(\u0026#34; Job %d finished\\n\u0026#34;, local_id); /* THIS LINE HAS BEEN CHANGED */ return NULL; } This solves the issue that was encountered above. Before the counter is accessed, the mutex is locked. This provides exclusive access to the following 2-line critical section untill the mutex is unlocked again. The counter is then incremented and copied in to variable local_id. Finally, the mutex is unlocked.\nWith this measure in place, the result is as was originally intended.\n  Note: The amount of code/instructions between locking and unlocking a mutex should of course be kept to a minimum. If you put a mutex around your entire threading function, you undo the entire possible benefit of using threads: the fact that they can run in parallel! As such, inter-thread communication via shared memory should be kept to a minimum, and only that should be protected using Mutexes/locks.\nGo to the Deadlock Empire website and do:\n Locks: Insufficient lock Locks: Deadlock    Deadlocks While locks help with many critical section problems, the Deadlock Empire examples above show that they are also not always trivial to apply correctly. This is especially the case if there is more than one shared resource/variable/memory region in play.\nFor example, a typical problem that can arise is when two threads need to obtain access to two different shared resources, but do so in opposite orders. This can for example happen in complex programs that often interact with outside peripherals like the hard disk and the network:\n// Thread 1 wants to first read from the network, then write result to the hard disk // Thread 2 wants to first read the contents of a file, then send it on the network  // Thread 1: network.lock() // CPU pauses Thread 1 and activates Thread 2  harddisk.lock() // CPU pauses Thread 2 and activates Thread 1 again harddisk.lock() // this will not complete, because Thread 2 is holding this lock already. Thread 1 has to wait  // CPU pauses Thread 1 because it has to wait and activates Thread 2 again  network.lock() // this will not complete, because Thread 1 is holding this lock already. Thread 2 has to wait  // Both threads are waiting endlessly AND also locking the network and the harddisk for any other threads that might arrive Note: this example is not entirely realistic, as the network and harddisk can generally be used by multiple threads and processes concurrently. This is because the OS (or even the hardware itself) has layers of abstraction, but also complex ways of detecting and preventing deadlocks from happening (for extra information, see this presentation (this is not course material)). Still, you can pretty easily make this mistake in your own program when accessing multiple pieces of shared memory, so watch out!\nSemaphores While mutex locks are useful, the are also relatively simple and limited in what they can convey. As such, over time more advanced thread synchronization techniques have evolved that make it a bit easier to deal with often occurring scenarios. One such more advanced technique is a semaphore. A semaphore makes it easier to track how many threads are requesting access or have already been given access to a particular resource.\nProducer-Consumer Problems This technique is used in for example producer-consumer problems. There, one or more producer threads prepare data (in our exercises: prime numbers) and put it in a limited amount of shared memory spots (say an array of size 10), ready to be used by one or more consumers (in our exercises: a function that prints the prime numbers). This type of setup prevents us from having to allocate a large amount of memory up-front to communicate thread results (as we did in the solution to the last exercise in the previous lab).\n  A high-level visualization of a multi-producer \u0026#43; multi-consumer problem with a limited amount of shared memory in between.   In this setup, we don\u0026rsquo;t just want to lock access to the shared memory so only 1 producer or consumer can use it at a given time, we want to do more: we want the producers to only write data to the shared memory if there is an open spot. If not, they have to wait. Similarly, the consumers can only read data from shared memory if there is actually something there. If not, they have to wait.\nConceptually, we could do this by keeping track of a count of the amount of items currently in the shared array (items_in_array below), and increment/decrement that count inside a mutex lock when producing/consuming (similar to the global counter solution above):\n\r// PSEUDOCODE! mutex myLock; int items_in_array = 0; int* array = malloc(sizeof(int) * 10); void* producer() { while (true) { bool itemStored = false; Item item = produceItem(); while( !itemStored ) { lock( myLock ); if( items_in_array \u0026lt; 10 ) { array[ items_in_array ] = item; items_in_array += 1; itemStored = true; } unlock( myLock ); } } } \r\r// PSEUDOCODE!  void* consumer() { while (true) { Item item = NULL; lock( myLock ); if( items_in_array \u0026gt; 0 ) { item = array[ items_in_array - 1 ]; items_in_array -= 1; } unlock( myLock ); if( item != NULL ) { consumeItem(item); } } } \r\rThis setup will work, however it is very inefficient: both the producers and the consumers are constantly trying to get a new lock on the array to check if they can produce/consume something, even if it\u0026rsquo;s only been a few milliseconds since they last checked (and found the array was full/empty and there was nothing to do). Imagine for example if the producers in the example are slow to produce new items, but the consumers are very fast: the consumers would constantly be re-locking the mutex to check if something new is available, even though there rarely is, causing the mutex to be almost immediately unlocked again. This type of behaviour is often called busy waiting or polling and is bad for performance as you\u0026rsquo;re doing a lot of unnecessary work.\nInstead, a better solution would be that a thread that wants to read/write an item from/to the array can only do so if it is actually possible. If not, the thread should be paused until it\u0026rsquo;s sure that it can make progress, instead of constantly checking. To do this, we need a new type of lock that keeps track of who\u0026rsquo;s waiting for what to happen, which is called the semaphore.\nSemaphore concepts In essence, the semaphore is nothing more than a counter (the items_in_array from above), but one that can automatically pause/resume threads that use it if the counter is zero/becomes positive (while above, we had to do this manually).\nThis is typically done using three conceptual functions on the semaphore:\n init( mySemaphore, count ) : Start the semaphore value at count (can be either 0 or a positive number). wait( mySemaphore ) : Continues if mySemaphore is \u0026gt; 0 and decrements it by 1. If not, it pauses the current thread until mySemaphore becomes positive. post( mySemaphore ) : Increments mySemaphore by 1. This then automatically causes 1 of the waiting threads (if any) to be unpaused.  As you can see, multiple threads can be waiting on the same semaphore (determined by the count passed in init()). For each time a post() is done however, only 1 of the waiting threads can of course be unpaused. The OS determines which thread is chosen from among the waiters and can do so in many different ways (for example thread that has been paused the longest, thread that has been paused the shortest, thread that was most recently started, the thread with the highest priority, etc.). More information on this is given in Chapter 7 (Scheduling).\n To illustrate this, a semaphore can be thought of as a bowl with tokens. For example, in a child daycare there can be a room with toys:\n  A photo of Cafe Boulevard (courtesy of Tripadvisor)   Only 5 children are allowed in that room. Outside, there is a bowl with bracelets (init( braceletBowl, 5 )). When a child wants to enter the room to play, they need to take a bracelet and put it on (wait( braceletBowl )). When there are no more bracelets in the bowl, a child that also wants to play in the room has to wait (also wait( braceletBowl )) until another child leaves the room and places their bracelet back in the bowl (post( braceletBowl )).\nUsing semaphores then, we can more optimally re-write the producer-consumer setup from above. Note however that we can\u0026rsquo;t just use a single semaphore, since the wait/post functions really only allow us to wait if the value is 0, not if the value is for example 10 (the maximum size of the array). As such, we need two: one to indicate how much slots are filled, and one to indicate how many are empty:\n\r// PSEUDOCODE! mutex myLock; semaphore fill_count; init(fill_count, 0); semaphore empty_count; init(empty_count, 10); int items_in_array = 0; int* array = malloc(sizeof(int) * 10); void* producer() { while (true) { Item item = produceItem(); wait( empty_count ); // THIS IS NEW!  lock( myLock ); array[ items_in_array ] = item; items_in_array += 1; unlock( myLock ); post( fill_count ); // THIS IS NEW!  } } \r\r// PSEUDOCODE!  void* consumer() { while (true) { Item item = NULL; wait( fill_count ); // THIS IS NEW!  lock( myLock ); item = array[ items_in_array - 1 ]; items_in_array -= 1; unlock( myLock ); post( empty_count ); // THIS IS NEW!  consumeItem(item); } } \r\rIt might seem strange to you that when using semaphores, we still ALSO need a mutex when actually writing to/reading from the shared array. Can you explain why this is needed? Can you explain why we\u0026rsquo;re using semaphores in the first place then, if they don\u0026rsquo;t even allow us to remove the mutex?\nAnswer:\r\rWhen using a Semaphore with an initial value higher than 1, multiple threads can still \"enter\" the semaphore at the same time (this is exactly one of the reasons to use a semaphore after all!). The amount of simultaneous threads is just limited by the Semaphore value. \rAs such, we still need to protect the critical section inside the semaphore block if it accesses shared memory, or we would again have race conditions!\r\rWe see that using semaphores is indeed not done to remove the mutex, but rather to remove the busy waiting while() loop and the custom checks if the array is full/empty. Compare the semaphore version with the pure-mutex version and you'll see we were able to actually get rid of quite some code + we're no longer constantly locking/unlocking the mutex!\r\r  Note that, if we initialize the Semaphore with a value of 1 (init( mySemaphore, 1 )), we basically get a mutex! After all, only a single thread can be active inside the semaphore at any given time then, leading to the same behaviour as a mutex. This is then typically referred to as a binary semaphore.\nIn this setup, we of course don\u0026rsquo;t need another mutex inside the semaphore block, since we\u0026rsquo;re already getting that guarantee from the binary semaphore.\nThis is sometimes used if the size of the shared memory is just a single item, often referred to as a \u0026ldquo;shelf\u0026rdquo; (see also later exercises and the threading assignment), to prevent having to keep a manual counter variable alongside a mutex (the binary semaphore is mutex and counter all-in-one!).\nMore info on the differences between a semaphore and a mutex are given here.\n Go to the Deadlock Empire website and do:\n Semaphores: Semaphores Semaphores: Producer-Consumer Semaphores: Producer-Consumer (Variant)    Semaphores in C and pthreads The pthreads library provides an API to program with semaphores in C (include semaphore.h to use). It contains the semaphore_t type and, amongst others, these functions (which are the same as discussed previously):\n sem_init(): initialises a semaphore. sem_wait(): waits until the semaphore is positive, then decrements the number inside of the semaphore. sem_post(): increments the number inside of the semaphore and unpauses one of the waiting threads (if any). (note: in other programming languages, this operation is often called signal instead of post) sem_destroy(): destroys the semaphore when it\u0026rsquo;s no longer needed.  There is also a \u0026ldquo;non-pausing\u0026rdquo; alternative to sem_wait: sem_trywait(). This variant will only decrement the semaphore if it\u0026rsquo;s positive. If it\u0026rsquo;s zero, it won\u0026rsquo;t pause the current thread. Additionally, this function returns the current value of the semaphore, which allows us to really use it as a replacement for a counter! See the manual pages and search Google for more info and the correct usage of this version!\n Note that next to mutexes and semaphores, there are many other thread synchronization utilities and concepts (such as for example \u0026ldquo;conditional variables\u0026rdquo;, \u0026ldquo;monitors\u0026rdquo; and \u0026ldquo;barriers\u0026rdquo;). Especially more modern programming languages like Java, C#, Rust, and Go typically have highly advanced threading options built-in. Some of these you can (optionally) explore and experiment with using the exercises from Deadlock Empire that we skipped.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/",
	"title": "6: Task management",
	"tags": [],
	"description": "",
	"content": "Chapter 6 Task management 6.1: Processes\n What\u0026rsquo;s in a process ? Run process !  Process Control Block Process state Open files list   Creating processes  6.2: Processes (lab)\n6.3: Inter-Process Communication (IPC)\n6.4: Threads\n What\u0026rsquo;s in a thread ? Why would we want multiple threads ?  Amdahl\u0026rsquo;s law   Creating threads  Pthreads    6.5: Threads (lab)\n6.6 Inter-Thread communication\n Critical Section Communication between threads  Locking Mutex Semaphore    6.7 Inter-Thread communication (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch6-tasks/lab3_interthreadcomm/",
	"title": "6.7: Inter-Thread communication (lab)",
	"tags": [],
	"description": "",
	"content": "  Source: G.I.   Good Morning, Sire Polite servants, impolite king Extend this incomplete C-program. As you can see, the program has one king thread and N servant threads, but they don\u0026rsquo;t really do anything yet. Implement the following using one or more semaphores:\n The king thread sleeps (using the sleep() function) for a random amount of time (a few seconds). When the king thread awakes, it signals the servant threads. All servant threads politely say Good Morning, sire (using printf()) together with their servant ID. The king thread goes to sleep again.  Take note of these things as well:\n You can find some concrete code examples of pthread semaphores in the code on the bottom of Assignment 3! Every servant needs to greet the king once (no more, no less) The king doesn\u0026rsquo;t wait for the servant greetings before he goes back to sleep You do not need to make sure that the servants always reply in the same order Repeat the wake-greet cycle at least 10 times Tip: use the rand() function to get random numbers  Polite king REVISIT, WITH A TWEAK Change the previous king-servant setup with a single change: now, the king does explicitly wait for the greetings of all his servants before he goes back to sleep.\nThink about if you need additional semaphores/mutexes for this and why.\n  An example output   Deadlock Empire: The Endgame  Go to the Deadlock Empire website and complete these two exercises:  The Final Stretch: Dragonfire The Final Stretch: Triple Danger    Producer-Consumer V1 : Single mutex In the prime-number exercises in the previous labs, we were actually implementing the basics of a producer-consumer problem, without really knowing it! In our example solution, we bypassed the shared memory issue by assigning each producer (the calculatePrimes function) a separate block of memory only they could use/access to place their results. As you know now, this is highly inefficient and a waste of memory. As such, it\u0026rsquo;s time to make our prime-number into a proper producer-consumer setup with a single shared memory array of a limited size!\nWe will do this in several steps, starting with a somewhat sub-optimal mutex-based solution and ending up with a more complex semaphore-based one!\nStart from the provided solution to the last prime-number exercise (6_5_3_primethreads.c). Change it so:\n All 4 producer threads write their results to a single shared array instead of the 4 separate results arrays (note: this single array will still be quite large, say 20000 integers). Make a new printPrimes() function that acts as the consumer. Use pthread_create in main to start a single instance of the consumer. Make sure this function prints newly produced primes as soon as possible/as soon as they\u0026rsquo;re put in the shared array (meaning: we\u0026rsquo;re no longer waiting for all the producers to be done before starting to print the results in main()).  For this, you should use a single mutex to prevent the producers/consumers from having race conditions on the single shared array.\n Tip: use a global write_index variable to manipulate the shared array  Note: in this setup, the primes will no longer be printed in-order from smallest to largest. That\u0026rsquo;s fine ;) Can you explain why though??? And can you imagine a possible solution where they -are- still printed in-order?\nProducer-Consumer V2 : Proper close One thing you might/should have noticed in the previous exercise, is that it\u0026rsquo;s no longer easy to determine when the consumer should stop printing/when it has printed all produced primes.\nIn the original solution 6_5_3_primethreads.c, we had the producers write the number 0 at the end of their result array to signal they were done (because 0 is not a prime). In the new setup however, we can\u0026rsquo;t use this anymore, because not all producers end at the same time, and so new \u0026ldquo;real\u0026rdquo; primes might be written after a 0 from the first finished producer!\nAs such, we need a new way to determine when we\u0026rsquo;re done: keep a count of producers that are done. Then, the consumer can check if this \u0026ldquo;done count\u0026rdquo; is equal to the number of producers. If so, it can safely exit without having to rely on the 0 sentinel value.\n Extend your previous code to keep a thread-safe \u0026ldquo;done count\u0026rdquo; of the number of finished producers. Make sure your consumer properly checks this \u0026ldquo;done count\u0026rdquo; (again in a thread-safe way!) and that it also properly exits/returns when all primes are printed.  Keeping the \u0026ldquo;done count\u0026rdquo; thread safe can be done in two main ways. Choose one:\n Use a count variable and a mutex Use a semaphore as a counter. (tip: here it might help to use the sem_getvalue() function. Look up online what that does and how to use it).  Producer-Consumer V3 : Multiple consumers In the previous two versions, we had only a single consumer. To get maximum performance, we of course would like to not just have multiple producers, but also multiple consumers!\nExtend your previous solution to:\n Create at least 2 printPrimes consumer threads (ideally more!), that all read from the shared array and print the produced primes. Of course you need to make sure no primes are printed twice (= thread-safe access to the shared array from all consumers)!!!  Note: this can again be done in several ways. The simplest/best way however, doesn\u0026rsquo;t even require you to define a new mutex here; you can just re-use the one from the V1 above. Tip for this: previously we just had a global write_index, now we also need another type of global index ;)\nProducer-Consumer V4 : Semaphores Up until now, you\u0026rsquo;ve used (should have been using?) mainly mutexes to access a single (huge) shared array. Because the array is so large, we didn\u0026rsquo;t have the typical mutex problems like busy waiting, because there was always room for newly produced primes to be placed in the shared memory (unless we had very high limits for the prime generation).\nTo forcibly show the usefulness of semaphores when we don\u0026rsquo;t have \u0026ldquo;unlimited\u0026rdquo; shared memory, you will now adapt your previous code to:\n Use semaphores instead of mutexes (at least for the main shared memory, you can still use a mutex for the \u0026ldquo;done count\u0026rdquo; if you did that) Use only the minimal amount of shared memory. Put differently: the shared array is now only a single integer in size instead of say 20000. All producers and all consumers can only use this single shared \u0026ldquo;memory location\u0026rdquo; to pass individual primes from one to the other. Consequently, this single \u0026ldquo;shelf\u0026rdquo; needs to be ultra-thread-safe!  Tip: These changes shouldn\u0026rsquo;t be too large (though also not tiny ;)). Nor should they be very different from the producer-consumer semaphore examples shown in the course text.\nNote: think about if you still need a mutex inside of your semaphore(s) as well. Why (not)? Look up the discussion on \u0026ldquo;binary semaphores\u0026rdquo; in the course text. Is this the same/a similar situation or not?\nNote: this exercise is a mandatory part of Assignment 3. While the assignments are individual, you CAN (and should) collaborate on this exercise! The solution will be posted only after the deadline for assignment 3 however.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch7-scheduling/",
	"title": "7: CPU scheduling",
	"tags": [],
	"description": "",
	"content": "Chapter 7 7.1: Scheduling algorithms\n (Don\u0026rsquo;t) Interrupt me !! Scheduler algorithms  FCFS SJF   Preemptive scheduling  Priority-based scheduling Round-Robin scheduling    7.2: Scheduling algorithms (lab)\n7.3: Towards real-world schedulers\n Dispatching Time slice size  The time slice is 10 ms The time slice is 100 ms What does it all mean ?   Real-world schedulers  Run queue - Hey, you there, get in line !!! Multi-level feedback queue Linux Scheduler    7.4: Towards real-world schedulers (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch8-stack/",
	"title": "8: The Stack &amp; The Heap",
	"tags": [],
	"description": "",
	"content": "Chapter 8 The Stack \u0026amp; The Heap "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/ch9-memory/",
	"title": "9: Memory Management",
	"tags": [],
	"description": "",
	"content": "Chapter 9 9.1: Memory management\n The flat earth of Arduino Let\u0026rsquo;s share Address Binding The memory-management unit Segmentation  9.2: Scheduling algorithms (lab)\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/chx-cs/",
	"title": "X: Capita Selecta",
	"tags": [],
	"description": "",
	"content": "Chapter X X.1: RTOS\n Operating Systems Real-time Operating Systems  FreeRTOS   Is RTOS Outdated?  X.2: Device drivers\n General  Where do they live?   Character device drivers Block device drivers  "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/",
	"title": "A. Assignments",
	"tags": [],
	"description": "",
	"content": "Part A Assignments This part contains obligatory assignments for students to complete and return to the teaching staff. These will be graded and make up for a portion of the total grading of this course. Every few weeks, new exercises will be made available, so be sure to check out this section now and then.\nThese are individual assignments. Working together on the same code base is not permitted, and plagiarism will be severely punished!\n Submitting your exercises Assignments can be submitted via Toledo, when assignments are available on the platform.\nFile submission format: lastname_firstname_[assignmentnr].zip. E.g. for assignment 1: groeneveld_wouter_1.zip. Do not submit single uncompressed files, always adhere to the .zip extension.\nDeadlines are also visible on the assignment page via Toledo. Usually, when instructors announce the exercise that has to be submitted, the deadline will be within a few weeks, on a day specified in the lab (and on toledo), at 5 PM (end of business day). Be sure to double check the deadline date at Toledo!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/",
	"title": "B. Appendix",
	"tags": [],
	"description": "",
	"content": "Part B Appendix Recommended reading, instructions, etc\u0026hellip;\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/reading/",
	"title": "1. Recommended reading",
	"tags": [],
	"description": "",
	"content": "Operating Systems Operating System Concepts  Another defining moment in the evolution of operating systems Small footprint operating systems, such as those driving the handheld devices that the baby dinosaurs are using on the cover, are just one of the cutting-edge applications you\u0026rsquo;ll find in Silberschatz, Galvin, and Gagne\u0026rsquo;s Operating System Concepts, Seventh Edition.\n Website: os-book.com\nProgramming in C The (Ansi) C Programming Language  This book is meant to help the reader learn how to program in C. It is the definitive reference guide, now in a second edition. Although the first edition was written in 1978, it continues to be a worldwide best-seller. This second edition brings the classic original up to date to include the ANSI standard.\n Ebook\nExpert C Programming  C programming is a craft that takes years to perfect. A reasonably sharp person can learn the basics of C quite quickly. But it takes much longer to master the nuances of the language and to write enough programs, and enough different programs, to become an expert. This book explains quirks, weird syntax, and more \u0026ldquo;deep C secrets\u0026rdquo;.\n Ebook\nTechniques for memory safety in C  In this essay, first temporal memory errors will be explained. After that the three main techniques to fully prevent these errors will be ex- plored by means of example. Furthermore for each technique the compatibility, with legacy code, and the performance will be discussed to provide a overview of the major advantages and disadvantages.\n Ebook\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/installing/",
	"title": "2. Linux Installation Instructions",
	"tags": [],
	"description": "",
	"content": "The exercises for this course are to be done on the Linux Operating System (OS). For those unfamiliar, Linux is an open source OS that comes in many different forms called distributions\u0026mdash;see chapter 1. Linux is used extensively worldwide and you will need to use it many times in your future career.\nTherefore, for this course, you\u0026rsquo;ll need to install a Linux flavor on your machine. Below are a number of options in how to do so. We would of course not ask you to remove Windows to install Linux on your personal machine: both OSes can perfectly live side-by-side. If you are using MacOS, you should still install a Linux flavor. Even though MacOS also runs on a flavor of UNIX, they are not fully compatible. For instance, you will not be able to compile and run the device driver exercises.\nThere are many types of Linux distros available. We will use either Ubuntu or Bodhi\u0026mdash;which is a more lightweight, smaller version of Ubuntu that\u0026rsquo;s easier to virtualize (see option 2 below). Ubuntu is the most newbie-friendly Linux OS.\n Download Ubuntu (choose the Ubuntu 22.04.1 LTS, filename ubuntu-22.04.1-desktop-amd64.iso) (3.6 GB image, requires 12 GB+ disk space) OR download Bodhi (choose the Standard release, filename bodhi-6.0.0-64.iso) (800MB image, requires 6 GB+ disk space)  For newcomers, we strongly recommend you to use Ubuntu if possible. Ubuntu\u0026rsquo;s default user password is\u0026hellip; ubuntu.\nStep A: Install Linux Option 0: Single Boot If you\u0026rsquo;re a hardcore software/hardware engineer, you\u0026rsquo;ll likely already have installed Linux as your only OS, and nothing else. We\u0026rsquo;re glad you\u0026rsquo;re that enthusiastic! In that case, you don\u0026rsquo;t need to do anything, except prepare the compiler: see step B.\nIf you only have Windows on your machine, choose option 1 or 2 instead.\nOption 1: Dual Boot The simplest option that will allow you to permanently enjoy Linux on your machine for many years to come, next to your existing OS. A \u0026ldquo;dual boot\u0026rdquo; denotes the presence of two bootable OSes; e.g. Windows 11 and Ubunutu, or macOS and FreeBSD.\nPlease see the official Ubuntu Install Guide if you\u0026rsquo;d like to install Linux permanently.\nA dual boot installation is not without its dangers! Make sure not to erase your existing (Windows) partition! We are not responsible and will not be able to help if something went wrong. Read the install guide carefully. If you\u0026rsquo;d rather be on the safe site, choose option 2.\n Option 2: Virtualize For this course, we provide a second option, called a \u0026ldquo;virtual machine\u0026rdquo;. This will install Linux inside of your Windows OS through an additional program called Oracle VirtualBox, which will make Linux think it\u0026rsquo;s running on normal hardware.\nIf you are using a 2020+ M1 Apple Mac architecture (ARM64 - click on \u0026ldquo;apple\u0026rdquo; - \u0026ldquo;about\u0026rdquo;: does \u0026ldquo;Chip\u0026rdquo; say \u0026ldquo;Apple M1\u0026rdquo;?), then you should try out the QEMU-based Mac-specific virtualization alternative called UTM. Follow the UTM docs on how to install Ubuntu (min 12 GB). Don\u0026rsquo;t forget to choose \u0026ldquo;virtualize\u0026rdquo; and mount the ARM64 .iso\u0026mdash;the second link, not the AMD/64-bit PC one. Bodhi Linux only works in \u0026ldquo;emulate\u0026rdquo; mode and is very slow. If that still doesn\u0026rsquo;t work, try VMWare Fusion, a personal edition is free.\nVirtualBox Install Steps   Download and install VirtualBox\n  Download a Linux .iso file (see above)\n  Prepare VirtualBox\n For this first part, we will follow an existing guide. You can also simply read along the official Ubuntu VirtualBox install guide.  Note that this is for an older version of Ubuntu, but everything is the same up until where you need to actually install Linux (the \u0026ldquo;Install Ubuntu\u0026rdquo; heading), which is where you go to step 4 below   Follow the guide until the \u0026ldquo;Install Ubuntu\u0026rdquo; Heading, taking into account these changes/options:  Memory size:  Best to choose at least 2048MB of RAM for the Virtual machine   Disk size:  For Ubuntu: choose a disk size of at least 10GB For Bodhi: choose a disk size of at least 6GB For both: you may choose the \u0026ldquo;Dynamically allocated\u0026rdquo; disk for this course if you don\u0026rsquo;t have much disk space on your machine        Install Linux\n In theory, you can run Linux without installing it (the \u0026ldquo;Try Ubuntu\u0026rdquo; option or the default state that Bodhi starts in). However, you would loose all files every time you shutdown the Virtual machine, so we recommend installing it proper for the duration of this course For Ubuntu:  Ubuntu starts into a selection screen where you can choose to either Try Ubuntu or install it. Choose install. Follow the default installation instructions in the wizard (similar to the guide we\u0026rsquo;ve been following) You can safely erase the disk: this only erases the virtual disk, not your real hard drive You do NOT need to choose additional encryption   For Bodhi:  Bodhi starts into a selection screen with several options. You need to be fast, because after a while it auto-selects the first option (= try without installing). Instead, use the arrow keys to choose \u0026ldquo;Install Bodhi\u0026rdquo; Follow the default install wizard, choosing the default options You do NOT need to choose additional encryption   After installation, reboot and login with your created account    VirtualBox Install FAQ  [WINDOWS] The screen looks very interlaced  Settings \u0026gt; Display Graphics Controller VBoxVGA -\u0026gt; VMSVGA   I want to change the keyboard layout!  See https://www.bodhilinux.com/w/configuring-your-new-bodhi-system/: Go to Menu \u0026gt; Applications \u0026gt; System Tools \u0026gt; System Settings \u0026gt; Keyboard Layout to open the Keyboard Layout dialog box. Click your language from the list and hit Apply Selected.   I want to be able to copy-and-paste text between my Host (my own computer) and the Guest (Linux ISO)!  This is not possible without the installation of an additional package: https://askubuntu.com/questions/22743/how-do-i-install-guest-additions-in-a-virtualbox-vm/22745#22745   I want to install something else such as my own favorite editor!  sudo apt install [package]. Read It\u0026rsquo;s FOSS\u0026rsquo;s Ubuntu Getting Started Guide.   I want to change the resolution of the VirtualBox display  This is sadly non-trivial with VirtualBox, where by default you can either choose the small/windowed view (default), or a fullscreen view from the menu. To choose a dynamic resolution, you need to install the \u0026ldquo;VirtualBox Guest Additions\u0026rdquo;  Boot the Virtual Machine and login In the VirtualBox menu on top, choose \u0026ldquo;Mount Guest Additions CD Image\u0026rdquo; (bottom option)   This should allow you to auto-install the necessary software. After this, reboot the VM and you should be able to choose new screen sizing options in the VirtualBox View menu  Sometimes this step requires additional software to be installed first. This typically does the trick: sudo apt install -y make perl. After this, reboot the VM and re-Mount the CD image.      If you encounter any other issues, Google is your friend. There\u0026rsquo;s a high chance others have encountered the same problem as you have, and have posted a solution.\nStep B: Install compiler \u0026amp; files Once you\u0026rsquo;re able to boot up Linux\u0026mdash;either by using virtualization or by (dual) booting directly into it\u0026mdash;you\u0026rsquo;ll need to make sure you\u0026rsquo;ve got the gcc compiler and course files at the ready.\nPreparing gcc  For many exercises in this course, we will use the \u0026ldquo;GNU Compiler Collection\u0026rdquo; (GCC) to run our C program code. We will do this from the \u0026ldquo;terminal\u0026rdquo; or \u0026ldquo;command line\u0026rdquo;, a textual interface. This comes pre-installed in Linux. Look for an icon like this one, which should open a screen that looks a bit like this one. To check if GCC is installed (it should be), type the following command in the terminal and hit enter: gcc -v  the output should be a list of details of the GCC program, and should end with a line starting with \u0026ldquo;gcc version\u0026rdquo; if this is not the case and you get an error message saying GCC is not installed, please execute the following command sudo apt update \u0026amp;\u0026amp; sudo apt install -y gcc (it will ask you for your password) and try again    Preparing the course files  For some exercises, we provide some basic code to help you get started This code is hosted in a GitHub repository. If that sounds like Gobbledygook to you, no worries: the Software Engineering course will explain all about Git and version control soon For now, you just need to execute following commands in the terminal:  Install git: sudo apt update \u0026amp;\u0026amp; sudo apt install -y git Go to your home directory: cd ~/ Download the course files: git clone https://github.com/KULeuven-Diepenbeek/osc-exercises.git course-files. This will create a new directory named \u0026ldquo;course-files\u0026rdquo; containing the necessary items. You can verify this by typing ls -la    You should now be ready to follow along with the classes!\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/appendix/cheat_sheet/",
	"title": "Cheat sheet",
	"tags": [],
	"description": "",
	"content": "This cheat sheet summarises a set of commands, so you don\u0026rsquo;t have to Google everything.\nLinux commands Below is a brief list of Linux commands. Remember that you can get more info on a command by typing \u0026ldquo;[command] -h\u0026quot;, \u0026ldquo;man [command]\u0026rdquo;, or even google the manpage on Google.\nNavigation    command goal     cd change directory   pwd \u0026lsquo;present working directory\u0026rsquo;   ls list (show content of current directory) - typical parameters: -lha (longform, human readable, all files)   history show a history of used commands. Re-execute with ![number].   alias make a shorthand command for another, longer command   clear clears all text from terminal window    CLI tools    command goal     man show text manual for another command (exit by pressing q)   mkdir make (create) a directory   touch create a file   rm remove a file/directory - typical params: -rf (recursive, force)   cat print raw file contents to terminal output   head print the first few lines of raw file contents to the terminal output - typical parameters: -n 15 (number of lines is 15)   tail print the last few lines of raw file contents to the terminal output - typical parameters: -n 15 (number of lines is 15)   grep search for content in a file   find search for content in a file   awk scripting language to filter strings/file contents   vi powerful editor with a steep learning curve (FYI: exiting is through: ESCAPE then \u0026ldquo;:q!\u0026quot;)   nano slightly simpler editor which is easier to control   chmod change modifiers of a file (r/w/x/d)   time record time it takes to run another command or give resource usage    Compiling    command goal     gcc GNU C Compiler - params: -c, -o, \u0026hellip;   make Execute a Makefile - params: [target]   ./[filepath] execute a compiled file/program    System administration    command goal     lsusb list USB devices   ps report a snaptshot of the current processes - typical parameters: -aux   pstree display a tree of processes   kill send a signal to a process - typical parameters: -9 PID (kill signal, process id (get from ps))   bg send process to the background   fg retrieve process to the background   jobs list processes in the background   ssh secure shell - remote access to another computer via command line    "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/exercises/none-yet/",
	"title": "None yet!",
	"tags": [],
	"description": "",
	"content": "Check back soon.\n"
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/",
	"title": "Operating Systems and C",
	"tags": [],
	"description": "",
	"content": "Operating systems and C   source: xkcd     source: xkcd   "
},
{
	"uri": "https://kuleuven-diepenbeek.github.io/osc-course/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]